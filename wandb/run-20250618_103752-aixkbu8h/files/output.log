Epoch 1/160: 84it [00:10,  7.66it/s]                        
2025-06-18 10:38:05.419 | INFO     | model:train:239 - [epoch 1]: epoch loss = 2.435101,   acc = 0.159565
[epoch 1]: epoch loss = 2.435101,   acc = 0.159565
Epoch 2/160: 84it [00:10,  8.19it/s]                        
2025-06-18 10:38:15.736 | INFO     | model:train:239 - [epoch 2]: epoch loss = 2.356635,   acc = 0.167936
[epoch 2]: epoch loss = 2.356635,   acc = 0.167936
Epoch 3/160: 84it [00:10,  8.27it/s]                        
2025-06-18 10:38:25.955 | INFO     | model:train:239 - [epoch 3]: epoch loss = 2.315885,   acc = 0.173504
[epoch 3]: epoch loss = 2.315885,   acc = 0.173504
Epoch 4/160: 84it [00:10,  8.24it/s]                        
2025-06-18 10:38:36.220 | INFO     | model:train:239 - [epoch 4]: epoch loss = 2.269906,   acc = 0.191328
[epoch 4]: epoch loss = 2.269906,   acc = 0.191328
Epoch 5/160: 84it [00:10,  8.35it/s]                        
2025-06-18 10:38:46.349 | INFO     | model:train:239 - [epoch 5]: epoch loss = 2.198157,   acc = 0.188562
[epoch 5]: epoch loss = 2.198157,   acc = 0.188562
Epoch 6/160: 84it [00:09,  8.42it/s]                        
2025-06-18 10:38:56.390 | INFO     | model:train:239 - [epoch 6]: epoch loss = 2.098932,   acc = 0.191640
[epoch 6]: epoch loss = 2.098932,   acc = 0.191640
Epoch 7/160: 84it [00:10,  8.12it/s]                        
2025-06-18 10:39:06.807 | INFO     | model:train:239 - [epoch 7]: epoch loss = 1.913530,   acc = 0.293710
[epoch 7]: epoch loss = 1.913530,   acc = 0.293710
Epoch 8/160: 84it [00:10,  8.00it/s]                        
2025-06-18 10:39:17.392 | INFO     | model:train:239 - [epoch 8]: epoch loss = 1.763247,   acc = 0.398443
[epoch 8]: epoch loss = 1.763247,   acc = 0.398443
Epoch 9/160: 84it [00:10,  8.02it/s]                        
2025-06-18 10:39:27.934 | INFO     | model:train:239 - [epoch 9]: epoch loss = 1.632374,   acc = 0.484473
[epoch 9]: epoch loss = 1.632374,   acc = 0.484473
Epoch 10/160: 84it [00:10,  8.00it/s]                        
2025-06-18 10:39:38.506 | INFO     | model:train:239 - [epoch 10]: epoch loss = 1.537523,   acc = 0.538366
[epoch 10]: epoch loss = 1.537523,   acc = 0.538366
Epoch 11/160: 84it [00:10,  7.97it/s]                        
2025-06-18 10:39:49.123 | INFO     | model:train:239 - [epoch 11]: epoch loss = 1.495700,   acc = 0.544157
[epoch 11]: epoch loss = 1.495700,   acc = 0.544157
Epoch 12/160: 84it [00:10,  7.86it/s]                        
2025-06-18 10:39:59.880 | INFO     | model:train:239 - [epoch 12]: epoch loss = 1.396243,   acc = 0.582491
[epoch 12]: epoch loss = 1.396243,   acc = 0.582491
Epoch 13/160: 84it [00:10,  7.94it/s]                        
2025-06-18 10:40:10.524 | INFO     | model:train:239 - [epoch 13]: epoch loss = 1.333397,   acc = 0.615903
[epoch 13]: epoch loss = 1.333397,   acc = 0.615903
Epoch 14/160: 84it [00:10,  7.95it/s]                        
2025-06-18 10:40:21.153 | INFO     | model:train:239 - [epoch 14]: epoch loss = 1.285078,   acc = 0.632033
[epoch 14]: epoch loss = 1.285078,   acc = 0.632033
Epoch 15/160: 84it [00:10,  7.79it/s]                        
2025-06-18 10:40:32.004 | INFO     | model:train:239 - [epoch 15]: epoch loss = 1.224321,   acc = 0.662275
[epoch 15]: epoch loss = 1.224321,   acc = 0.662275
Epoch 16/160: 84it [00:10,  7.75it/s]                        
2025-06-18 10:40:42.928 | INFO     | model:train:239 - [epoch 16]: epoch loss = 1.171056,   acc = 0.678379
[epoch 16]: epoch loss = 1.171056,   acc = 0.678379
Epoch 17/160: 84it [00:10,  8.19it/s]                        
2025-06-18 10:40:53.253 | INFO     | model:train:239 - [epoch 17]: epoch loss = 1.130020,   acc = 0.695715
[epoch 17]: epoch loss = 1.130020,   acc = 0.695715
Epoch 18/160: 84it [00:10,  7.80it/s]                        
2025-06-18 10:41:04.111 | INFO     | model:train:239 - [epoch 18]: epoch loss = 1.110489,   acc = 0.701751
[epoch 18]: epoch loss = 1.110489,   acc = 0.701751
Epoch 19/160: 84it [00:10,  7.97it/s]                        
2025-06-18 10:41:14.721 | INFO     | model:train:239 - [epoch 19]: epoch loss = 1.060970,   acc = 0.711907
[epoch 19]: epoch loss = 1.060970,   acc = 0.711907
Epoch 20/160: 84it [00:10,  7.87it/s]                        
2025-06-18 10:41:25.486 | INFO     | model:train:239 - [epoch 20]: epoch loss = 0.999478,   acc = 0.735484
[epoch 20]: epoch loss = 0.999478,   acc = 0.735484
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 6365,  5166,  4270,  5506,  9952, 15137,  6740,  8204,   224,
        1104,   751,  3254]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5732,  5479,  5002,  8100, 11645, 10194,  5049,  6585,  1571,
        1590,  2551,  3175]))
2025-06-18 10:41:25.817 | INFO     | model:train:255 - [epoch 20]: val loss = 2.300826,   val acc = 0.551693,   val balanced acc = 0.458792
[epoch 20]: val loss = 2.300826,   val acc = 0.551693,   val balanced acc = 0.458792
Epoch 21/160:  73%|███████▎  | 60/82 [00:07<00:02,  7.44it/s]wandb: WARNING Tried to log to step 20 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 21/160: 84it [00:10,  7.73it/s]                        
2025-06-18 10:41:36.948 | INFO     | model:train:239 - [epoch 21]: epoch loss = 0.961120,   acc = 0.750240
[epoch 21]: epoch loss = 0.961120,   acc = 0.750240
Epoch 22/160: 84it [00:10,  7.83it/s]                        
2025-06-18 10:41:47.760 | INFO     | model:train:239 - [epoch 22]: epoch loss = 0.966634,   acc = 0.749505
[epoch 22]: epoch loss = 0.966634,   acc = 0.749505
Epoch 23/160: 84it [00:10,  7.77it/s]                        
2025-06-18 10:41:58.646 | INFO     | model:train:239 - [epoch 23]: epoch loss = 0.967852,   acc = 0.744831
[epoch 23]: epoch loss = 0.967852,   acc = 0.744831
Epoch 24/160: 84it [00:10,  7.92it/s]                        
2025-06-18 10:42:09.337 | INFO     | model:train:239 - [epoch 24]: epoch loss = 1.027966,   acc = 0.717049
[epoch 24]: epoch loss = 1.027966,   acc = 0.717049
Epoch 25/160: 84it [00:10,  8.00it/s]                        
2025-06-18 10:42:19.917 | INFO     | model:train:239 - [epoch 25]: epoch loss = 0.903853,   acc = 0.767533
[epoch 25]: epoch loss = 0.903853,   acc = 0.767533
Epoch 26/160: 84it [00:10,  7.92it/s]                        
2025-06-18 10:42:30.606 | INFO     | model:train:239 - [epoch 26]: epoch loss = 0.856677,   acc = 0.782218
[epoch 26]: epoch loss = 0.856677,   acc = 0.782218
Epoch 27/160: 84it [00:10,  7.77it/s]                        
2025-06-18 10:42:41.503 | INFO     | model:train:239 - [epoch 27]: epoch loss = 0.841703,   acc = 0.785598
[epoch 27]: epoch loss = 0.841703,   acc = 0.785598
Epoch 28/160: 84it [00:10,  7.85it/s]                        
2025-06-18 10:42:52.280 | INFO     | model:train:239 - [epoch 28]: epoch loss = 0.808710,   acc = 0.798882
[epoch 28]: epoch loss = 0.808710,   acc = 0.798882
Epoch 29/160: 84it [00:10,  7.72it/s]                        
2025-06-18 10:43:03.423 | INFO     | model:train:239 - [epoch 29]: epoch loss = 0.800039,   acc = 0.801569
[epoch 29]: epoch loss = 0.800039,   acc = 0.801569
Epoch 30/160: 84it [00:10,  7.92it/s]                        
2025-06-18 10:43:14.093 | INFO     | model:train:239 - [epoch 30]: epoch loss = 0.773339,   acc = 0.807432
[epoch 30]: epoch loss = 0.773339,   acc = 0.807432
Epoch 31/160: 84it [00:10,  7.88it/s]                        
2025-06-18 10:43:24.822 | INFO     | model:train:239 - [epoch 31]: epoch loss = 0.746235,   acc = 0.821143
[epoch 31]: epoch loss = 0.746235,   acc = 0.821143
Epoch 32/160: 84it [00:10,  8.19it/s]                        
2025-06-18 10:43:35.159 | INFO     | model:train:239 - [epoch 32]: epoch loss = 0.724723,   acc = 0.829624
[epoch 32]: epoch loss = 0.724723,   acc = 0.829624
Epoch 33/160: 84it [00:10,  7.89it/s]                        
2025-06-18 10:43:45.879 | INFO     | model:train:239 - [epoch 33]: epoch loss = 0.702150,   acc = 0.835177
[epoch 33]: epoch loss = 0.702150,   acc = 0.835177
Epoch 34/160: 84it [00:10,  7.80it/s]                        
2025-06-18 10:43:56.714 | INFO     | model:train:239 - [epoch 34]: epoch loss = 0.692303,   acc = 0.839473
[epoch 34]: epoch loss = 0.692303,   acc = 0.839473
Epoch 35/160: 84it [00:10,  7.99it/s]                        
2025-06-18 10:44:07.320 | INFO     | model:train:239 - [epoch 35]: epoch loss = 0.675964,   acc = 0.843317
[epoch 35]: epoch loss = 0.675964,   acc = 0.843317
Epoch 36/160: 84it [00:10,  7.92it/s]                        
2025-06-18 10:44:18.023 | INFO     | model:train:239 - [epoch 36]: epoch loss = 0.683011,   acc = 0.840976
[epoch 36]: epoch loss = 0.683011,   acc = 0.840976
Epoch 37/160: 84it [00:10,  7.87it/s]                        
2025-06-18 10:44:28.805 | INFO     | model:train:239 - [epoch 37]: epoch loss = 0.677667,   acc = 0.841082
[epoch 37]: epoch loss = 0.677667,   acc = 0.841082
Epoch 38/160: 84it [00:10,  7.89it/s]                        
2025-06-18 10:44:39.590 | INFO     | model:train:239 - [epoch 38]: epoch loss = 0.639147,   acc = 0.853074
[epoch 38]: epoch loss = 0.639147,   acc = 0.853074
Epoch 39/160: 84it [00:10,  7.78it/s]                        
2025-06-18 10:44:50.495 | INFO     | model:train:239 - [epoch 39]: epoch loss = 0.635739,   acc = 0.853414
[epoch 39]: epoch loss = 0.635739,   acc = 0.853414
Epoch 40/160: 84it [00:10,  7.96it/s]                        
2025-06-18 10:45:01.169 | INFO     | model:train:239 - [epoch 40]: epoch loss = 0.652626,   acc = 0.850312
[epoch 40]: epoch loss = 0.652626,   acc = 0.850312
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5571,  3838,  4331,  6273, 10990, 11086,  7937, 10811,   465,
        1068,   787,  3516]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5732,  5479,  5002,  8100, 11645, 10194,  5049,  6585,  1571,
        1590,  2551,  3175]))
2025-06-18 10:45:01.475 | INFO     | model:train:255 - [epoch 40]: val loss = 2.802893,   val acc = 0.550673,   val balanced acc = 0.477252
[epoch 40]: val loss = 2.802893,   val acc = 0.550673,   val balanced acc = 0.477252
Epoch 41/160:  20%|█▉        | 16/82 [00:01<00:07,  8.61it/s]wandb: WARNING Tried to log to step 40 that is less than the current step 41. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 41/160: 84it [00:10,  7.99it/s]                        
2025-06-18 10:45:12.344 | INFO     | model:train:239 - [epoch 41]: epoch loss = 0.638937,   acc = 0.853757
[epoch 41]: epoch loss = 0.638937,   acc = 0.853757
Epoch 42/160: 84it [00:10,  7.89it/s]                        
2025-06-18 10:45:23.125 | INFO     | model:train:239 - [epoch 42]: epoch loss = 0.622718,   acc = 0.856675
[epoch 42]: epoch loss = 0.622718,   acc = 0.856675
Epoch 43/160: 84it [00:10,  7.89it/s]                        
2025-06-18 10:45:33.882 | INFO     | model:train:239 - [epoch 43]: epoch loss = 0.588127,   acc = 0.868600
[epoch 43]: epoch loss = 0.588127,   acc = 0.868600
Epoch 44/160: 84it [00:10,  8.10it/s]                        
2025-06-18 10:45:44.397 | INFO     | model:train:239 - [epoch 44]: epoch loss = 0.614077,   acc = 0.861871
[epoch 44]: epoch loss = 0.614077,   acc = 0.861871
Epoch 45/160: 84it [00:10,  7.85it/s]                        
2025-06-18 10:45:55.210 | INFO     | model:train:239 - [epoch 45]: epoch loss = 0.573437,   acc = 0.873603
[epoch 45]: epoch loss = 0.573437,   acc = 0.873603
Epoch 46/160: 84it [00:10,  8.00it/s]                        
2025-06-18 10:46:05.819 | INFO     | model:train:239 - [epoch 46]: epoch loss = 0.546518,   acc = 0.883781
[epoch 46]: epoch loss = 0.546518,   acc = 0.883781
Epoch 47/160: 84it [00:10,  7.77it/s]                        
2025-06-18 10:46:16.738 | INFO     | model:train:239 - [epoch 47]: epoch loss = 0.524056,   acc = 0.890544
[epoch 47]: epoch loss = 0.524056,   acc = 0.890544
Epoch 48/160: 84it [00:10,  7.90it/s]                        
2025-06-18 10:46:27.517 | INFO     | model:train:239 - [epoch 48]: epoch loss = 0.509627,   acc = 0.896099
[epoch 48]: epoch loss = 0.509627,   acc = 0.896099
Epoch 49/160: 84it [00:10,  7.97it/s]                        
2025-06-18 10:46:38.194 | INFO     | model:train:239 - [epoch 49]: epoch loss = 0.494966,   acc = 0.899722
[epoch 49]: epoch loss = 0.494966,   acc = 0.899722
Epoch 50/160: 84it [00:10,  8.09it/s]                        
2025-06-18 10:46:48.686 | INFO     | model:train:239 - [epoch 50]: epoch loss = 0.492955,   acc = 0.899316
[epoch 50]: epoch loss = 0.492955,   acc = 0.899316
Epoch 51/160: 84it [00:10,  7.96it/s]                        
2025-06-18 10:46:59.423 | INFO     | model:train:239 - [epoch 51]: epoch loss = 0.487370,   acc = 0.901178
[epoch 51]: epoch loss = 0.487370,   acc = 0.901178
Epoch 52/160: 84it [00:10,  8.33it/s]                        
2025-06-18 10:47:09.646 | INFO     | model:train:239 - [epoch 52]: epoch loss = 0.482638,   acc = 0.904602
[epoch 52]: epoch loss = 0.482638,   acc = 0.904602
Epoch 53/160: 84it [00:10,  7.80it/s]                        
2025-06-18 10:47:20.483 | INFO     | model:train:239 - [epoch 53]: epoch loss = 0.464069,   acc = 0.908740
[epoch 53]: epoch loss = 0.464069,   acc = 0.908740
Epoch 54/160: 84it [00:10,  8.09it/s]                        
2025-06-18 10:47:30.949 | INFO     | model:train:239 - [epoch 54]: epoch loss = 0.461513,   acc = 0.908715
[epoch 54]: epoch loss = 0.461513,   acc = 0.908715
Epoch 55/160: 84it [00:10,  7.83it/s]                        
2025-06-18 10:47:41.745 | INFO     | model:train:239 - [epoch 55]: epoch loss = 0.458010,   acc = 0.910292
[epoch 55]: epoch loss = 0.458010,   acc = 0.910292
Epoch 56/160: 84it [00:10,  7.84it/s]                        
2025-06-18 10:47:52.579 | INFO     | model:train:239 - [epoch 56]: epoch loss = 0.457818,   acc = 0.907366
[epoch 56]: epoch loss = 0.457818,   acc = 0.907366
Epoch 57/160: 84it [00:10,  7.87it/s]                        
2025-06-18 10:48:03.316 | INFO     | model:train:239 - [epoch 57]: epoch loss = 0.458168,   acc = 0.908018
[epoch 57]: epoch loss = 0.458168,   acc = 0.908018
Epoch 58/160: 84it [00:10,  7.73it/s]                        
2025-06-18 10:48:14.253 | INFO     | model:train:239 - [epoch 58]: epoch loss = 0.439793,   acc = 0.912832
[epoch 58]: epoch loss = 0.439793,   acc = 0.912832
Epoch 59/160: 84it [00:10,  7.78it/s]                        
2025-06-18 10:48:25.116 | INFO     | model:train:239 - [epoch 59]: epoch loss = 0.436834,   acc = 0.915456
[epoch 59]: epoch loss = 0.436834,   acc = 0.915456
Epoch 60/160: 84it [00:10,  7.69it/s]                        
2025-06-18 10:48:36.167 | INFO     | model:train:239 - [epoch 60]: epoch loss = 0.426583,   acc = 0.918022
[epoch 60]: epoch loss = 0.426583,   acc = 0.918022
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 8221,  3710,  5252,  7059, 10583, 10247,  6839,  7136,  1542,
        1612,  1147,  3325]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5732,  5479,  5002,  8100, 11645, 10194,  5049,  6585,  1571,
        1590,  2551,  3175]))
2025-06-18 10:48:36.499 | INFO     | model:train:255 - [epoch 60]: val loss = 2.587422,   val acc = 0.574670,   val balanced acc = 0.528772
[epoch 60]: val loss = 2.587422,   val acc = 0.574670,   val balanced acc = 0.528772
Epoch 61/160:  68%|██████▊   | 56/82 [00:07<00:03,  7.90it/s]wandb: WARNING Tried to log to step 60 that is less than the current step 61. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 61/160: 84it [00:10,  7.78it/s]                        
2025-06-18 10:48:47.472 | INFO     | model:train:239 - [epoch 61]: epoch loss = 0.424311,   acc = 0.918071
[epoch 61]: epoch loss = 0.424311,   acc = 0.918071
Epoch 62/160: 84it [00:10,  7.91it/s]                        
2025-06-18 10:48:58.180 | INFO     | model:train:239 - [epoch 62]: epoch loss = 0.417918,   acc = 0.919201
[epoch 62]: epoch loss = 0.417918,   acc = 0.919201
Epoch 63/160: 84it [00:10,  8.00it/s]                        
2025-06-18 10:49:08.822 | INFO     | model:train:239 - [epoch 63]: epoch loss = 0.415273,   acc = 0.921080
[epoch 63]: epoch loss = 0.415273,   acc = 0.921080
Epoch 64/160: 84it [00:10,  8.19it/s]                        
2025-06-18 10:49:19.194 | INFO     | model:train:239 - [epoch 64]: epoch loss = 0.406502,   acc = 0.924694
[epoch 64]: epoch loss = 0.406502,   acc = 0.924694
Epoch 65/160: 84it [00:10,  7.91it/s]                        
2025-06-18 10:49:29.928 | INFO     | model:train:239 - [epoch 65]: epoch loss = 0.403618,   acc = 0.924025
[epoch 65]: epoch loss = 0.403618,   acc = 0.924025
Epoch 66/160: 84it [00:10,  7.81it/s]                        
2025-06-18 10:49:40.934 | INFO     | model:train:239 - [epoch 66]: epoch loss = 0.399853,   acc = 0.924128
[epoch 66]: epoch loss = 0.399853,   acc = 0.924128
Epoch 67/160: 84it [00:10,  7.82it/s]                        
2025-06-18 10:49:51.806 | INFO     | model:train:239 - [epoch 67]: epoch loss = 0.392708,   acc = 0.926479
[epoch 67]: epoch loss = 0.392708,   acc = 0.926479
Epoch 68/160: 84it [00:10,  7.81it/s]                        
2025-06-18 10:50:02.689 | INFO     | model:train:239 - [epoch 68]: epoch loss = 0.385524,   acc = 0.928963
[epoch 68]: epoch loss = 0.385524,   acc = 0.928963
Epoch 69/160: 84it [00:10,  7.84it/s]                        
2025-06-18 10:50:13.507 | INFO     | model:train:239 - [epoch 69]: epoch loss = 0.380842,   acc = 0.929723
[epoch 69]: epoch loss = 0.380842,   acc = 0.929723
Epoch 70/160: 84it [00:10,  8.36it/s]                        
2025-06-18 10:50:23.787 | INFO     | model:train:239 - [epoch 70]: epoch loss = 0.386510,   acc = 0.929748
[epoch 70]: epoch loss = 0.386510,   acc = 0.929748
Epoch 71/160: 84it [00:10,  8.00it/s]                        
2025-06-18 10:50:34.539 | INFO     | model:train:239 - [epoch 71]: epoch loss = 0.388649,   acc = 0.927017
[epoch 71]: epoch loss = 0.388649,   acc = 0.927017
Epoch 72/160: 84it [00:10,  7.77it/s]                        
2025-06-18 10:50:45.489 | INFO     | model:train:239 - [epoch 72]: epoch loss = 0.381268,   acc = 0.927850
[epoch 72]: epoch loss = 0.381268,   acc = 0.927850
Epoch 73/160: 84it [00:10,  7.89it/s]                        
2025-06-18 10:50:56.243 | INFO     | model:train:239 - [epoch 73]: epoch loss = 0.371655,   acc = 0.931211
[epoch 73]: epoch loss = 0.371655,   acc = 0.931211
Epoch 74/160: 84it [00:10,  7.92it/s]                        
2025-06-18 10:51:06.953 | INFO     | model:train:239 - [epoch 74]: epoch loss = 0.360052,   acc = 0.933494
[epoch 74]: epoch loss = 0.360052,   acc = 0.933494
Epoch 75/160: 84it [00:10,  8.16it/s]                        
2025-06-18 10:51:17.324 | INFO     | model:train:239 - [epoch 75]: epoch loss = 0.358391,   acc = 0.936284
[epoch 75]: epoch loss = 0.358391,   acc = 0.936284
Epoch 76/160: 84it [00:10,  7.90it/s]                        
2025-06-18 10:51:28.550 | INFO     | model:train:239 - [epoch 76]: epoch loss = 0.352964,   acc = 0.936771
[epoch 76]: epoch loss = 0.352964,   acc = 0.936771
Epoch 77/160: 84it [00:10,  7.96it/s]                        
2025-06-18 10:51:39.220 | INFO     | model:train:239 - [epoch 77]: epoch loss = 0.348941,   acc = 0.937828
[epoch 77]: epoch loss = 0.348941,   acc = 0.937828
Epoch 78/160: 84it [00:10,  8.19it/s]                        
2025-06-18 10:51:49.625 | INFO     | model:train:239 - [epoch 78]: epoch loss = 0.349659,   acc = 0.937954
[epoch 78]: epoch loss = 0.349659,   acc = 0.937954
Epoch 79/160: 84it [00:10,  7.93it/s]                        
2025-06-18 10:52:00.341 | INFO     | model:train:239 - [epoch 79]: epoch loss = 0.340705,   acc = 0.939923
[epoch 79]: epoch loss = 0.340705,   acc = 0.939923
Epoch 80/160: 84it [00:10,  7.96it/s]                        
2025-06-18 10:52:10.989 | INFO     | model:train:239 - [epoch 80]: epoch loss = 0.339796,   acc = 0.940308
[epoch 80]: epoch loss = 0.339796,   acc = 0.940308
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 8314,  4001,  5307,  7006, 11202,  9230,  8473,  6870,   838,
        1439,   790,  3203]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5732,  5479,  5002,  8100, 11645, 10194,  5049,  6585,  1571,
        1590,  2551,  3175]))
2025-06-18 10:52:11.294 | INFO     | model:train:255 - [epoch 80]: val loss = 3.150009,   val acc = 0.557317,   val balanced acc = 0.513346
[epoch 80]: val loss = 3.150009,   val acc = 0.557317,   val balanced acc = 0.513346
2025-06-18 10:52:11.382 | INFO     | model:train:275 - EarlyStopping counter: 1 out of 3
EarlyStopping counter: 1 out of 3
Epoch 81/160:  20%|█▉        | 16/82 [00:02<00:11,  5.76it/s]wandb: WARNING Tried to log to step 80 that is less than the current step 81. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 81/160: 84it [00:10,  7.72it/s]                        
2025-06-18 10:52:22.372 | INFO     | model:train:239 - [epoch 81]: epoch loss = 0.328639,   acc = 0.942826
[epoch 81]: epoch loss = 0.328639,   acc = 0.942826
Epoch 82/160: 84it [00:10,  7.93it/s]                        
2025-06-18 10:52:33.102 | INFO     | model:train:239 - [epoch 82]: epoch loss = 0.332925,   acc = 0.942658
[epoch 82]: epoch loss = 0.332925,   acc = 0.942658
Epoch 83/160: 84it [00:10,  7.83it/s]                        
2025-06-18 10:52:43.911 | INFO     | model:train:239 - [epoch 83]: epoch loss = 0.332220,   acc = 0.942362
[epoch 83]: epoch loss = 0.332220,   acc = 0.942362
Epoch 84/160: 84it [00:10,  8.09it/s]                        
2025-06-18 10:52:54.399 | INFO     | model:train:239 - [epoch 84]: epoch loss = 0.332510,   acc = 0.942610
[epoch 84]: epoch loss = 0.332510,   acc = 0.942610
Epoch 85/160: 84it [00:11,  7.57it/s]                        
2025-06-18 10:53:05.571 | INFO     | model:train:239 - [epoch 85]: epoch loss = 0.325323,   acc = 0.942928
[epoch 85]: epoch loss = 0.325323,   acc = 0.942928
Epoch 86/160: 84it [00:10,  7.65it/s]                        
2025-06-18 10:53:16.659 | INFO     | model:train:239 - [epoch 86]: epoch loss = 0.322743,   acc = 0.943534
[epoch 86]: epoch loss = 0.322743,   acc = 0.943534
Epoch 87/160: 84it [00:10,  7.70it/s]                        
2025-06-18 10:53:27.648 | INFO     | model:train:239 - [epoch 87]: epoch loss = 0.321630,   acc = 0.943058
[epoch 87]: epoch loss = 0.321630,   acc = 0.943058
Epoch 88/160: 84it [00:10,  7.87it/s]                        
2025-06-18 10:53:38.449 | INFO     | model:train:239 - [epoch 88]: epoch loss = 0.322419,   acc = 0.943291
[epoch 88]: epoch loss = 0.322419,   acc = 0.943291
Epoch 89/160: 84it [00:10,  7.86it/s]                        
2025-06-18 10:53:49.216 | INFO     | model:train:239 - [epoch 89]: epoch loss = 0.319932,   acc = 0.943592
[epoch 89]: epoch loss = 0.319932,   acc = 0.943592
Epoch 90/160: 84it [00:10,  7.65it/s]                        
2025-06-18 10:54:00.301 | INFO     | model:train:239 - [epoch 90]: epoch loss = 0.311520,   acc = 0.946263
[epoch 90]: epoch loss = 0.311520,   acc = 0.946263
Epoch 91/160: 84it [00:10,  7.74it/s]                        
2025-06-18 10:54:11.273 | INFO     | model:train:239 - [epoch 91]: epoch loss = 0.311822,   acc = 0.946877
[epoch 91]: epoch loss = 0.311822,   acc = 0.946877
Epoch 92/160: 84it [00:10,  7.77it/s]                        
2025-06-18 10:54:22.197 | INFO     | model:train:239 - [epoch 92]: epoch loss = 0.308708,   acc = 0.947503
[epoch 92]: epoch loss = 0.308708,   acc = 0.947503
Epoch 93/160: 84it [00:10,  7.76it/s]                        
2025-06-18 10:54:33.098 | INFO     | model:train:239 - [epoch 93]: epoch loss = 0.307415,   acc = 0.947268
[epoch 93]: epoch loss = 0.307415,   acc = 0.947268
Epoch 94/160: 84it [00:10,  7.71it/s]                        
2025-06-18 10:54:44.111 | INFO     | model:train:239 - [epoch 94]: epoch loss = 0.305256,   acc = 0.947704
[epoch 94]: epoch loss = 0.305256,   acc = 0.947704
Epoch 95/160: 84it [00:10,  7.82it/s]                        
2025-06-18 10:54:54.928 | INFO     | model:train:239 - [epoch 95]: epoch loss = 0.304529,   acc = 0.948651
[epoch 95]: epoch loss = 0.304529,   acc = 0.948651
Epoch 96/160: 84it [00:10,  7.89it/s]                        
2025-06-18 10:55:05.695 | INFO     | model:train:239 - [epoch 96]: epoch loss = 0.301800,   acc = 0.949763
[epoch 96]: epoch loss = 0.301800,   acc = 0.949763
Epoch 97/160: 84it [00:10,  7.82it/s]                        
2025-06-18 10:55:16.559 | INFO     | model:train:239 - [epoch 97]: epoch loss = 0.298428,   acc = 0.949299
[epoch 97]: epoch loss = 0.298428,   acc = 0.949299
Epoch 98/160: 84it [00:10,  7.95it/s]                        
2025-06-18 10:55:27.251 | INFO     | model:train:239 - [epoch 98]: epoch loss = 0.298951,   acc = 0.948742
[epoch 98]: epoch loss = 0.298951,   acc = 0.948742
Epoch 99/160: 84it [00:10,  7.97it/s]                        
2025-06-18 10:55:37.870 | INFO     | model:train:239 - [epoch 99]: epoch loss = 0.296920,   acc = 0.950472
[epoch 99]: epoch loss = 0.296920,   acc = 0.950472
Epoch 100/160: 84it [00:10,  8.14it/s]                        
2025-06-18 10:55:48.307 | INFO     | model:train:239 - [epoch 100]: epoch loss = 0.290504,   acc = 0.951848
[epoch 100]: epoch loss = 0.290504,   acc = 0.951848
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 8609,  3807,  5158,  6809, 11907,  9962,  6736,  7305,   771,
        1589,   763,  3257]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5732,  5479,  5002,  8100, 11645, 10194,  5049,  6585,  1571,
        1590,  2551,  3175]))
2025-06-18 10:55:48.617 | INFO     | model:train:255 - [epoch 100]: val loss = 3.050847,   val acc = 0.564846,   val balanced acc = 0.516853
[epoch 100]: val loss = 3.050847,   val acc = 0.564846,   val balanced acc = 0.516853
2025-06-18 10:55:48.698 | INFO     | model:train:275 - EarlyStopping counter: 2 out of 3
EarlyStopping counter: 2 out of 3
Epoch 101/160:  44%|████▍     | 36/82 [00:05<00:05,  7.87it/s]wandb: WARNING Tried to log to step 100 that is less than the current step 101. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 101/160: 84it [00:10,  7.97it/s]                        
2025-06-18 10:55:59.325 | INFO     | model:train:239 - [epoch 101]: epoch loss = 0.292290,   acc = 0.951469
[epoch 101]: epoch loss = 0.292290,   acc = 0.951469
Epoch 102/160: 84it [00:10,  7.96it/s]                        
2025-06-18 10:56:09.995 | INFO     | model:train:239 - [epoch 102]: epoch loss = 0.288955,   acc = 0.952180
[epoch 102]: epoch loss = 0.288955,   acc = 0.952180
Epoch 103/160: 84it [00:10,  7.96it/s]                        
2025-06-18 10:56:20.629 | INFO     | model:train:239 - [epoch 103]: epoch loss = 0.287153,   acc = 0.952766
[epoch 103]: epoch loss = 0.287153,   acc = 0.952766
Epoch 104/160: 84it [00:10,  7.86it/s]                        
2025-06-18 10:56:31.435 | INFO     | model:train:239 - [epoch 104]: epoch loss = 0.284405,   acc = 0.953110
[epoch 104]: epoch loss = 0.284405,   acc = 0.953110
Epoch 105/160: 84it [00:10,  7.81it/s]                        
2025-06-18 10:56:42.264 | INFO     | model:train:239 - [epoch 105]: epoch loss = 0.284400,   acc = 0.953682
[epoch 105]: epoch loss = 0.284400,   acc = 0.953682
Epoch 106/160: 84it [00:10,  7.80it/s]                        
2025-06-18 10:56:53.145 | INFO     | model:train:239 - [epoch 106]: epoch loss = 0.282588,   acc = 0.953956
[epoch 106]: epoch loss = 0.282588,   acc = 0.953956
Epoch 107/160: 84it [00:10,  7.66it/s]                        
2025-06-18 10:57:04.194 | INFO     | model:train:239 - [epoch 107]: epoch loss = 0.281921,   acc = 0.953803
[epoch 107]: epoch loss = 0.281921,   acc = 0.953803
Epoch 108/160: 84it [00:10,  7.90it/s]                        
2025-06-18 10:57:14.976 | INFO     | model:train:239 - [epoch 108]: epoch loss = 0.281882,   acc = 0.953765
[epoch 108]: epoch loss = 0.281882,   acc = 0.953765
Epoch 109/160: 84it [00:10,  8.10it/s]                        
2025-06-18 10:57:25.435 | INFO     | model:train:239 - [epoch 109]: epoch loss = 0.281186,   acc = 0.953603
[epoch 109]: epoch loss = 0.281186,   acc = 0.953603
Epoch 110/160: 84it [00:10,  8.29it/s]                        
2025-06-18 10:57:35.717 | INFO     | model:train:239 - [epoch 110]: epoch loss = 0.284121,   acc = 0.953595
[epoch 110]: epoch loss = 0.284121,   acc = 0.953595
Epoch 111/160: 84it [00:10,  7.95it/s]                        
2025-06-18 10:57:46.365 | INFO     | model:train:239 - [epoch 111]: epoch loss = 0.275449,   acc = 0.955111
[epoch 111]: epoch loss = 0.275449,   acc = 0.955111
Epoch 112/160: 84it [00:10,  7.94it/s]                        
2025-06-18 10:57:57.072 | INFO     | model:train:239 - [epoch 112]: epoch loss = 0.277118,   acc = 0.955486
[epoch 112]: epoch loss = 0.277118,   acc = 0.955486
Epoch 113/160: 84it [00:10,  7.76it/s]                        
2025-06-18 10:58:07.977 | INFO     | model:train:239 - [epoch 113]: epoch loss = 0.275289,   acc = 0.955222
[epoch 113]: epoch loss = 0.275289,   acc = 0.955222
Epoch 114/160: 84it [00:10,  7.91it/s]                        
2025-06-18 10:58:18.747 | INFO     | model:train:239 - [epoch 114]: epoch loss = 0.272734,   acc = 0.955761
[epoch 114]: epoch loss = 0.272734,   acc = 0.955761
Epoch 115/160: 84it [00:10,  7.70it/s]                        
2025-06-18 10:58:29.739 | INFO     | model:train:239 - [epoch 115]: epoch loss = 0.273172,   acc = 0.955105
[epoch 115]: epoch loss = 0.273172,   acc = 0.955105
Epoch 116/160: 84it [00:10,  7.81it/s]                        
2025-06-18 10:58:40.592 | INFO     | model:train:239 - [epoch 116]: epoch loss = 0.271063,   acc = 0.956023
[epoch 116]: epoch loss = 0.271063,   acc = 0.956023
Epoch 117/160: 84it [00:10,  7.97it/s]                        
2025-06-18 10:58:51.213 | INFO     | model:train:239 - [epoch 117]: epoch loss = 0.270477,   acc = 0.956977
[epoch 117]: epoch loss = 0.270477,   acc = 0.956977
Epoch 118/160: 84it [00:10,  7.81it/s]                        
2025-06-18 10:59:02.109 | INFO     | model:train:239 - [epoch 118]: epoch loss = 0.271482,   acc = 0.956565
[epoch 118]: epoch loss = 0.271482,   acc = 0.956565
Epoch 119/160: 84it [00:10,  7.79it/s]                        
2025-06-18 10:59:13.010 | INFO     | model:train:239 - [epoch 119]: epoch loss = 0.269973,   acc = 0.957140
[epoch 119]: epoch loss = 0.269973,   acc = 0.957140
Epoch 120/160: 84it [00:10,  8.14it/s]                        
2025-06-18 10:59:23.455 | INFO     | model:train:239 - [epoch 120]: epoch loss = 0.270392,   acc = 0.956928
[epoch 120]: epoch loss = 0.270392,   acc = 0.956928
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 8447,  3998,  5119,  6037, 11513, 10993,  7518,  6815,   838,
        1527,   699,  3169]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5732,  5479,  5002,  8100, 11645, 10194,  5049,  6585,  1571,
        1590,  2551,  3175]))
2025-06-18 10:59:23.772 | INFO     | model:train:255 - [epoch 120]: val loss = 3.338592,   val acc = 0.556042,   val balanced acc = 0.510613
[epoch 120]: val loss = 3.338592,   val acc = 0.556042,   val balanced acc = 0.510613
2025-06-18 10:59:23.825 | INFO     | model:train:275 - EarlyStopping counter: 3 out of 3
EarlyStopping counter: 3 out of 3
2025-06-18 10:59:23.825 | INFO     | model:train:277 - Early stopping triggered.
Early stopping triggered.
