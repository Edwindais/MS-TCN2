Epoch 1/160: 100%|██████████| 82/82 [00:08<00:00,  9.17it/s]
2025-07-02 22:44:18.947 | INFO     | model:train:251 - [epoch 1]: epoch loss = 9.316348,   acc = 0.171569
[epoch 1]: epoch loss = 9.316348,   acc = 0.171569
Epoch 2/160: 100%|██████████| 82/82 [00:08<00:00,  9.66it/s]
2025-07-02 22:44:27.474 | INFO     | model:train:251 - [epoch 2]: epoch loss = 8.433046,   acc = 0.208705
[epoch 2]: epoch loss = 8.433046,   acc = 0.208705
Epoch 3/160: 100%|██████████| 82/82 [00:08<00:00,  9.81it/s]
2025-07-02 22:44:35.913 | INFO     | model:train:251 - [epoch 3]: epoch loss = 7.177304,   acc = 0.336675
[epoch 3]: epoch loss = 7.177304,   acc = 0.336675
Epoch 4/160: 100%|██████████| 82/82 [00:08<00:00,  9.66it/s]
2025-07-02 22:44:44.483 | INFO     | model:train:251 - [epoch 4]: epoch loss = 6.011511,   acc = 0.520446
[epoch 4]: epoch loss = 6.011511,   acc = 0.520446
Epoch 5/160: 100%|██████████| 82/82 [00:08<00:00,  9.52it/s]
2025-07-02 22:44:53.193 | INFO     | model:train:251 - [epoch 5]: epoch loss = 5.210329,   acc = 0.606432
[epoch 5]: epoch loss = 5.210329,   acc = 0.606432
Epoch 6/160: 100%|██████████| 82/82 [00:08<00:00,  9.63it/s]
2025-07-02 22:45:01.789 | INFO     | model:train:251 - [epoch 6]: epoch loss = 4.838584,   acc = 0.641378
[epoch 6]: epoch loss = 4.838584,   acc = 0.641378
Epoch 7/160: 100%|██████████| 82/82 [00:08<00:00,  9.84it/s]
2025-07-02 22:45:10.202 | INFO     | model:train:251 - [epoch 7]: epoch loss = 4.545273,   acc = 0.665113
[epoch 7]: epoch loss = 4.545273,   acc = 0.665113
Epoch 8/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 22:45:18.606 | INFO     | model:train:251 - [epoch 8]: epoch loss = 4.210839,   acc = 0.696238
[epoch 8]: epoch loss = 4.210839,   acc = 0.696238
Epoch 9/160: 100%|██████████| 82/82 [00:08<00:00,  9.95it/s]
2025-07-02 22:45:26.905 | INFO     | model:train:251 - [epoch 9]: epoch loss = 3.985741,   acc = 0.713188
[epoch 9]: epoch loss = 3.985741,   acc = 0.713188
Epoch 10/160: 100%|██████████| 82/82 [00:08<00:00,  9.48it/s]
2025-07-02 22:45:35.622 | INFO     | model:train:251 - [epoch 10]: epoch loss = 3.651240,   acc = 0.743957
[epoch 10]: epoch loss = 3.651240,   acc = 0.743957
Epoch 11/160: 100%|██████████| 82/82 [00:08<00:00,  9.68it/s]
2025-07-02 22:45:44.156 | INFO     | model:train:251 - [epoch 11]: epoch loss = 3.452688,   acc = 0.761429
[epoch 11]: epoch loss = 3.452688,   acc = 0.761429
Epoch 12/160: 100%|██████████| 82/82 [00:08<00:00,  9.55it/s]
2025-07-02 22:45:52.812 | INFO     | model:train:251 - [epoch 12]: epoch loss = 3.338679,   acc = 0.772053
[epoch 12]: epoch loss = 3.338679,   acc = 0.772053
Epoch 13/160: 100%|██████████| 82/82 [00:08<00:00,  9.81it/s]
2025-07-02 22:46:01.236 | INFO     | model:train:251 - [epoch 13]: epoch loss = 3.616594,   acc = 0.752988
[epoch 13]: epoch loss = 3.616594,   acc = 0.752988
Epoch 14/160: 100%|██████████| 82/82 [00:08<00:00,  9.65it/s]
2025-07-02 22:46:09.806 | INFO     | model:train:251 - [epoch 14]: epoch loss = 3.147047,   acc = 0.789066
[epoch 14]: epoch loss = 3.147047,   acc = 0.789066
Epoch 15/160: 100%|██████████| 82/82 [00:08<00:00,  9.63it/s]
2025-07-02 22:46:18.380 | INFO     | model:train:251 - [epoch 15]: epoch loss = 2.882535,   acc = 0.816672
[epoch 15]: epoch loss = 2.882535,   acc = 0.816672
Epoch 16/160: 100%|██████████| 82/82 [00:08<00:00,  9.63it/s]
2025-07-02 22:46:26.958 | INFO     | model:train:251 - [epoch 16]: epoch loss = 2.726809,   acc = 0.829120
[epoch 16]: epoch loss = 2.726809,   acc = 0.829120
Epoch 17/160: 100%|██████████| 82/82 [00:08<00:00,  9.60it/s]
2025-07-02 22:46:35.580 | INFO     | model:train:251 - [epoch 17]: epoch loss = 2.636214,   acc = 0.835164
[epoch 17]: epoch loss = 2.636214,   acc = 0.835164
Epoch 18/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 22:46:44.049 | INFO     | model:train:251 - [epoch 18]: epoch loss = 2.475467,   acc = 0.848508
[epoch 18]: epoch loss = 2.475467,   acc = 0.848508
Epoch 19/160: 100%|██████████| 82/82 [00:08<00:00,  9.60it/s]
2025-07-02 22:46:52.674 | INFO     | model:train:251 - [epoch 19]: epoch loss = 2.405540,   acc = 0.854612
[epoch 19]: epoch loss = 2.405540,   acc = 0.854612
Epoch 20/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 22:47:01.163 | INFO     | model:train:251 - [epoch 20]: epoch loss = 2.636960,   acc = 0.829808
[epoch 20]: epoch loss = 2.636960,   acc = 0.829808
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5911,  4707,  4274,  8414, 14085, 14128,  5762,  7822,  1008,
        1613,  1650,  1796]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 8380,  6077,  4321,  5525, 13939, 13696,  5106,  6929,   112,
        1780,  1352,  3953]))
2025-07-02 22:47:01.491 | INFO     | model:train:267 - [epoch 20]: val loss = 5.504005,   val acc = 0.601686,   val balanced acc = 0.520592
[epoch 20]: val loss = 5.504005,   val acc = 0.601686,   val balanced acc = 0.520592
Epoch 21/160:  91%|█████████▏| 75/82 [00:07<00:00, 10.43it/s]wandb: WARNING Tried to log to step 20 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 21/160: 100%|██████████| 82/82 [00:08<00:00,  9.86it/s]
2025-07-02 22:47:09.931 | INFO     | model:train:251 - [epoch 21]: epoch loss = 2.381737,   acc = 0.854290
[epoch 21]: epoch loss = 2.381737,   acc = 0.854290
Epoch 22/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 22:47:18.391 | INFO     | model:train:251 - [epoch 22]: epoch loss = 2.105538,   acc = 0.878697
[epoch 22]: epoch loss = 2.105538,   acc = 0.878697
Epoch 23/160: 100%|██████████| 82/82 [00:08<00:00,  9.94it/s]
2025-07-02 22:47:26.848 | INFO     | model:train:251 - [epoch 23]: epoch loss = 2.096670,   acc = 0.878739
[epoch 23]: epoch loss = 2.096670,   acc = 0.878739
Epoch 24/160: 100%|██████████| 82/82 [00:08<00:00,  9.53it/s]
2025-07-02 22:47:35.514 | INFO     | model:train:251 - [epoch 24]: epoch loss = 2.029062,   acc = 0.882876
[epoch 24]: epoch loss = 2.029062,   acc = 0.882876
Epoch 25/160: 100%|██████████| 82/82 [00:08<00:00,  9.96it/s]
2025-07-02 22:47:43.832 | INFO     | model:train:251 - [epoch 25]: epoch loss = 2.003696,   acc = 0.885128
[epoch 25]: epoch loss = 2.003696,   acc = 0.885128
Epoch 26/160: 100%|██████████| 82/82 [00:08<00:00,  9.79it/s]
2025-07-02 22:47:52.355 | INFO     | model:train:251 - [epoch 26]: epoch loss = 2.246542,   acc = 0.860263
[epoch 26]: epoch loss = 2.246542,   acc = 0.860263
Epoch 27/160: 100%|██████████| 82/82 [00:08<00:00,  9.93it/s]
2025-07-02 22:48:00.705 | INFO     | model:train:251 - [epoch 27]: epoch loss = 2.008747,   acc = 0.882502
[epoch 27]: epoch loss = 2.008747,   acc = 0.882502
Epoch 28/160: 100%|██████████| 82/82 [00:08<00:00, 10.06it/s]
2025-07-02 22:48:08.942 | INFO     | model:train:251 - [epoch 28]: epoch loss = 1.787045,   acc = 0.903450
[epoch 28]: epoch loss = 1.787045,   acc = 0.903450
Epoch 29/160: 100%|██████████| 82/82 [00:08<00:00,  9.86it/s]
2025-07-02 22:48:17.405 | INFO     | model:train:251 - [epoch 29]: epoch loss = 1.697877,   acc = 0.908375
[epoch 29]: epoch loss = 1.697877,   acc = 0.908375
Epoch 30/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 22:48:25.902 | INFO     | model:train:251 - [epoch 30]: epoch loss = 1.609925,   acc = 0.916657
[epoch 30]: epoch loss = 1.609925,   acc = 0.916657
Epoch 31/160: 100%|██████████| 82/82 [00:08<00:00,  9.95it/s]
2025-07-02 22:48:34.222 | INFO     | model:train:251 - [epoch 31]: epoch loss = 1.574571,   acc = 0.918460
[epoch 31]: epoch loss = 1.574571,   acc = 0.918460
Epoch 32/160: 100%|██████████| 82/82 [00:08<00:00, 10.03it/s]
2025-07-02 22:48:42.478 | INFO     | model:train:251 - [epoch 32]: epoch loss = 1.563228,   acc = 0.917891
[epoch 32]: epoch loss = 1.563228,   acc = 0.917891
Epoch 33/160: 100%|██████████| 82/82 [00:08<00:00,  9.98it/s]
2025-07-02 22:48:50.791 | INFO     | model:train:251 - [epoch 33]: epoch loss = 1.515201,   acc = 0.921109
[epoch 33]: epoch loss = 1.515201,   acc = 0.921109
Epoch 34/160: 100%|██████████| 82/82 [00:08<00:00,  9.95it/s]
2025-07-02 22:48:59.307 | INFO     | model:train:251 - [epoch 34]: epoch loss = 1.452866,   acc = 0.926072
[epoch 34]: epoch loss = 1.452866,   acc = 0.926072
Epoch 35/160: 100%|██████████| 82/82 [00:08<00:00, 10.15it/s]
2025-07-02 22:49:07.465 | INFO     | model:train:251 - [epoch 35]: epoch loss = 1.412239,   acc = 0.929158
[epoch 35]: epoch loss = 1.412239,   acc = 0.929158
Epoch 36/160: 100%|██████████| 82/82 [00:08<00:00, 10.12it/s]
2025-07-02 22:49:15.639 | INFO     | model:train:251 - [epoch 36]: epoch loss = 1.419170,   acc = 0.925995
[epoch 36]: epoch loss = 1.419170,   acc = 0.925995
Epoch 37/160: 100%|██████████| 82/82 [00:08<00:00,  9.73it/s]
2025-07-02 22:49:24.144 | INFO     | model:train:251 - [epoch 37]: epoch loss = 1.619430,   acc = 0.908111
[epoch 37]: epoch loss = 1.619430,   acc = 0.908111
Epoch 38/160: 100%|██████████| 82/82 [00:08<00:00,  9.67it/s]
2025-07-02 22:49:32.766 | INFO     | model:train:251 - [epoch 38]: epoch loss = 1.719027,   acc = 0.899962
[epoch 38]: epoch loss = 1.719027,   acc = 0.899962
Epoch 39/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 22:49:41.255 | INFO     | model:train:251 - [epoch 39]: epoch loss = 1.452768,   acc = 0.923609
[epoch 39]: epoch loss = 1.452768,   acc = 0.923609
Epoch 40/160: 100%|██████████| 82/82 [00:08<00:00,  9.92it/s]
2025-07-02 22:49:49.600 | INFO     | model:train:251 - [epoch 40]: epoch loss = 1.296284,   acc = 0.935871
[epoch 40]: epoch loss = 1.296284,   acc = 0.935871
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 9025,  5229,  5090,  7254, 11719, 15707,  4350,  5508,   303,
        1925,  2399,  2661]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 8380,  6077,  4321,  5525, 13939, 13696,  5106,  6929,   112,
        1780,  1352,  3953]))
2025-07-02 22:49:49.891 | INFO     | model:train:267 - [epoch 40]: val loss = 5.813084,   val acc = 0.639202,   val balanced acc = 0.574280
[epoch 40]: val loss = 5.813084,   val acc = 0.639202,   val balanced acc = 0.574280
Epoch 41/160: 100%|██████████| 82/82 [00:08<00:00,  9.69it/s]
2025-07-02 22:49:58.484 | INFO     | model:train:251 - [epoch 41]: epoch loss = 1.240956,   acc = 0.939696
[epoch 41]: epoch loss = 1.240956,   acc = 0.939696
Epoch 42/160:  10%|▉         | 8/82 [00:00<00:07,  9.86it/s]wandb: WARNING Tried to log to step 40 that is less than the current step 41. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 42/160: 100%|██████████| 82/82 [00:08<00:00,  9.87it/s]
2025-07-02 22:50:06.870 | INFO     | model:train:251 - [epoch 42]: epoch loss = 1.206578,   acc = 0.942624
[epoch 42]: epoch loss = 1.206578,   acc = 0.942624
Epoch 43/160: 100%|██████████| 82/82 [00:08<00:00,  9.84it/s]
2025-07-02 22:50:15.285 | INFO     | model:train:251 - [epoch 43]: epoch loss = 1.160723,   acc = 0.946082
[epoch 43]: epoch loss = 1.160723,   acc = 0.946082
Epoch 44/160: 100%|██████████| 82/82 [00:08<00:00,  9.71it/s]
2025-07-02 22:50:23.811 | INFO     | model:train:251 - [epoch 44]: epoch loss = 1.194358,   acc = 0.941794
[epoch 44]: epoch loss = 1.194358,   acc = 0.941794
Epoch 45/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 22:50:32.285 | INFO     | model:train:251 - [epoch 45]: epoch loss = 1.122737,   acc = 0.947960
[epoch 45]: epoch loss = 1.122737,   acc = 0.947960
Epoch 46/160: 100%|██████████| 82/82 [00:08<00:00,  9.78it/s]
2025-07-02 22:50:40.753 | INFO     | model:train:251 - [epoch 46]: epoch loss = 1.096544,   acc = 0.948974
[epoch 46]: epoch loss = 1.096544,   acc = 0.948974
Epoch 47/160: 100%|██████████| 82/82 [00:08<00:00,  9.84it/s]
2025-07-02 22:50:49.148 | INFO     | model:train:251 - [epoch 47]: epoch loss = 1.177018,   acc = 0.940193
[epoch 47]: epoch loss = 1.177018,   acc = 0.940193
Epoch 48/160: 100%|██████████| 82/82 [00:08<00:00,  9.72it/s]
2025-07-02 22:50:57.970 | INFO     | model:train:251 - [epoch 48]: epoch loss = 1.121792,   acc = 0.945662
[epoch 48]: epoch loss = 1.121792,   acc = 0.945662
Epoch 49/160: 100%|██████████| 82/82 [00:08<00:00, 10.05it/s]
2025-07-02 22:51:06.188 | INFO     | model:train:251 - [epoch 49]: epoch loss = 1.039929,   acc = 0.952852
[epoch 49]: epoch loss = 1.039929,   acc = 0.952852
Epoch 50/160: 100%|██████████| 82/82 [00:08<00:00, 10.05it/s]
2025-07-02 22:51:14.413 | INFO     | model:train:251 - [epoch 50]: epoch loss = 1.031656,   acc = 0.951836
[epoch 50]: epoch loss = 1.031656,   acc = 0.951836
Epoch 51/160: 100%|██████████| 82/82 [00:08<00:00,  9.88it/s]
2025-07-02 22:51:22.782 | INFO     | model:train:251 - [epoch 51]: epoch loss = 1.002532,   acc = 0.953633
[epoch 51]: epoch loss = 1.002532,   acc = 0.953633
Epoch 52/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 22:51:31.270 | INFO     | model:train:251 - [epoch 52]: epoch loss = 0.978597,   acc = 0.955941
[epoch 52]: epoch loss = 0.978597,   acc = 0.955941
Epoch 53/160: 100%|██████████| 82/82 [00:08<00:00,  9.90it/s]
2025-07-02 22:51:39.606 | INFO     | model:train:251 - [epoch 53]: epoch loss = 1.001883,   acc = 0.952990
[epoch 53]: epoch loss = 1.001883,   acc = 0.952990
Epoch 54/160: 100%|██████████| 82/82 [00:08<00:00,  9.92it/s]
2025-07-02 22:51:47.948 | INFO     | model:train:251 - [epoch 54]: epoch loss = 1.069480,   acc = 0.946280
[epoch 54]: epoch loss = 1.069480,   acc = 0.946280
Epoch 55/160: 100%|██████████| 82/82 [00:08<00:00, 10.02it/s]
2025-07-02 22:51:56.188 | INFO     | model:train:251 - [epoch 55]: epoch loss = 0.974694,   acc = 0.954285
[epoch 55]: epoch loss = 0.974694,   acc = 0.954285
Epoch 56/160: 100%|██████████| 82/82 [00:08<00:00,  9.92it/s]
2025-07-02 22:52:04.635 | INFO     | model:train:251 - [epoch 56]: epoch loss = 0.937284,   acc = 0.957255
[epoch 56]: epoch loss = 0.937284,   acc = 0.957255
Epoch 57/160: 100%|██████████| 82/82 [00:08<00:00,  9.84it/s]
2025-07-02 22:52:13.028 | INFO     | model:train:251 - [epoch 57]: epoch loss = 0.912625,   acc = 0.958958
[epoch 57]: epoch loss = 0.912625,   acc = 0.958958
Epoch 58/160: 100%|██████████| 82/82 [00:08<00:00, 10.01it/s]
2025-07-02 22:52:21.282 | INFO     | model:train:251 - [epoch 58]: epoch loss = 0.880706,   acc = 0.961332
[epoch 58]: epoch loss = 0.880706,   acc = 0.961332
Epoch 59/160: 100%|██████████| 82/82 [00:08<00:00,  9.60it/s]
2025-07-02 22:52:29.883 | INFO     | model:train:251 - [epoch 59]: epoch loss = 0.921562,   acc = 0.957636
[epoch 59]: epoch loss = 0.921562,   acc = 0.957636
Epoch 60/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 22:52:38.240 | INFO     | model:train:251 - [epoch 60]: epoch loss = 0.946776,   acc = 0.954559
[epoch 60]: epoch loss = 0.946776,   acc = 0.954559
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 9556,  5326,  4857,  6725, 11037, 12687,  3904,  7828,   274,
        3009,  2761,  3206]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 8380,  6077,  4321,  5525, 13939, 13696,  5106,  6929,   112,
        1780,  1352,  3953]))
2025-07-02 22:52:38.500 | INFO     | model:train:267 - [epoch 60]: val loss = 6.138503,   val acc = 0.638935,   val balanced acc = 0.596170
[epoch 60]: val loss = 6.138503,   val acc = 0.638935,   val balanced acc = 0.596170
Epoch 61/160:  10%|▉         | 8/82 [00:00<00:08,  9.19it/s]wandb: WARNING Tried to log to step 60 that is less than the current step 61. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 61/160: 100%|██████████| 82/82 [00:08<00:00, 10.03it/s]
2025-07-02 22:52:46.776 | INFO     | model:train:251 - [epoch 61]: epoch loss = 0.896963,   acc = 0.958203
[epoch 61]: epoch loss = 0.896963,   acc = 0.958203
Epoch 62/160: 100%|██████████| 82/82 [00:08<00:00, 10.04it/s]
2025-07-02 22:52:55.006 | INFO     | model:train:251 - [epoch 62]: epoch loss = 0.886967,   acc = 0.958250
[epoch 62]: epoch loss = 0.886967,   acc = 0.958250
Epoch 63/160: 100%|██████████| 82/82 [00:08<00:00,  9.92it/s]
2025-07-02 22:53:03.331 | INFO     | model:train:251 - [epoch 63]: epoch loss = 0.827615,   acc = 0.963686
[epoch 63]: epoch loss = 0.827615,   acc = 0.963686
Epoch 64/160: 100%|██████████| 82/82 [00:08<00:00, 10.05it/s]
2025-07-02 22:53:11.560 | INFO     | model:train:251 - [epoch 64]: epoch loss = 0.799629,   acc = 0.966163
[epoch 64]: epoch loss = 0.799629,   acc = 0.966163
Epoch 65/160: 100%|██████████| 82/82 [00:08<00:00,  9.93it/s]
2025-07-02 22:53:19.892 | INFO     | model:train:251 - [epoch 65]: epoch loss = 0.800174,   acc = 0.965361
[epoch 65]: epoch loss = 0.800174,   acc = 0.965361
Epoch 66/160: 100%|██████████| 82/82 [00:08<00:00, 10.07it/s]
2025-07-02 22:53:28.104 | INFO     | model:train:251 - [epoch 66]: epoch loss = 0.820268,   acc = 0.962648
[epoch 66]: epoch loss = 0.820268,   acc = 0.962648
Epoch 67/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 22:53:36.552 | INFO     | model:train:251 - [epoch 67]: epoch loss = 1.003310,   acc = 0.947243
[epoch 67]: epoch loss = 1.003310,   acc = 0.947243
Epoch 68/160: 100%|██████████| 82/82 [00:08<00:00,  9.73it/s]
2025-07-02 22:53:45.746 | INFO     | model:train:251 - [epoch 68]: epoch loss = 1.164417,   acc = 0.935322
[epoch 68]: epoch loss = 1.164417,   acc = 0.935322
Epoch 69/160: 100%|██████████| 82/82 [00:08<00:00,  9.80it/s]
2025-07-02 22:53:54.171 | INFO     | model:train:251 - [epoch 69]: epoch loss = 0.892031,   acc = 0.957364
[epoch 69]: epoch loss = 0.892031,   acc = 0.957364
Epoch 70/160: 100%|██████████| 82/82 [00:08<00:00,  9.92it/s]
2025-07-02 22:54:02.509 | INFO     | model:train:251 - [epoch 70]: epoch loss = 0.784768,   acc = 0.966519
[epoch 70]: epoch loss = 0.784768,   acc = 0.966519
Epoch 71/160: 100%|██████████| 82/82 [00:08<00:00,  9.97it/s]
2025-07-02 22:54:11.057 | INFO     | model:train:251 - [epoch 71]: epoch loss = 0.752794,   acc = 0.968620
[epoch 71]: epoch loss = 0.752794,   acc = 0.968620
Epoch 72/160: 100%|██████████| 82/82 [00:08<00:00, 10.03it/s]
2025-07-02 22:54:19.295 | INFO     | model:train:251 - [epoch 72]: epoch loss = 0.735161,   acc = 0.969746
[epoch 72]: epoch loss = 0.735161,   acc = 0.969746
Epoch 73/160: 100%|██████████| 82/82 [00:08<00:00,  9.98it/s]
2025-07-02 22:54:27.615 | INFO     | model:train:251 - [epoch 73]: epoch loss = 0.720013,   acc = 0.970557
[epoch 73]: epoch loss = 0.720013,   acc = 0.970557
Epoch 74/160: 100%|██████████| 82/82 [00:08<00:00,  9.78it/s]
2025-07-02 22:54:36.814 | INFO     | model:train:251 - [epoch 74]: epoch loss = 0.706235,   acc = 0.971257
[epoch 74]: epoch loss = 0.706235,   acc = 0.971257
Epoch 75/160: 100%|██████████| 82/82 [00:08<00:00, 10.06it/s]
2025-07-02 22:54:45.029 | INFO     | model:train:251 - [epoch 75]: epoch loss = 0.701527,   acc = 0.971147
[epoch 75]: epoch loss = 0.701527,   acc = 0.971147
Epoch 76/160: 100%|██████████| 82/82 [00:08<00:00, 10.04it/s]
2025-07-02 22:54:53.260 | INFO     | model:train:251 - [epoch 76]: epoch loss = 0.711780,   acc = 0.969628
[epoch 76]: epoch loss = 0.711780,   acc = 0.969628
Epoch 77/160: 100%|██████████| 82/82 [00:08<00:00, 10.04it/s]
2025-07-02 22:55:01.506 | INFO     | model:train:251 - [epoch 77]: epoch loss = 0.694990,   acc = 0.970608
[epoch 77]: epoch loss = 0.694990,   acc = 0.970608
Epoch 78/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 22:55:09.907 | INFO     | model:train:251 - [epoch 78]: epoch loss = 0.684785,   acc = 0.971477
[epoch 78]: epoch loss = 0.684785,   acc = 0.971477
Epoch 79/160: 100%|██████████| 82/82 [00:08<00:00, 10.22it/s]
2025-07-02 22:55:18.005 | INFO     | model:train:251 - [epoch 79]: epoch loss = 0.667701,   acc = 0.972943
[epoch 79]: epoch loss = 0.667701,   acc = 0.972943
Epoch 80/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 22:55:26.466 | INFO     | model:train:251 - [epoch 80]: epoch loss = 0.659840,   acc = 0.973254
[epoch 80]: epoch loss = 0.659840,   acc = 0.973254
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 8000,  5569,  4799,  6871, 12641, 15605,  3990,  6540,   226,
        1459,  2535,  2935]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 8380,  6077,  4321,  5525, 13939, 13696,  5106,  6929,   112,
        1780,  1352,  3953]))
2025-07-02 22:55:26.724 | INFO     | model:train:267 - [epoch 80]: val loss = 6.643008,   val acc = 0.643993,   val balanced acc = 0.567438
[epoch 80]: val loss = 6.643008,   val acc = 0.643993,   val balanced acc = 0.567438
2025-07-02 22:55:26.762 | INFO     | model:train:287 - EarlyStopping counter: 1 out of 3
EarlyStopping counter: 1 out of 3
Epoch 81/160:  33%|███▎      | 27/82 [00:02<00:05, 10.94it/s]wandb: WARNING Tried to log to step 80 that is less than the current step 81. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 81/160: 100%|██████████| 82/82 [00:08<00:00,  9.99it/s]
2025-07-02 22:55:35.036 | INFO     | model:train:251 - [epoch 81]: epoch loss = 0.658530,   acc = 0.972830
[epoch 81]: epoch loss = 0.658530,   acc = 0.972830
Epoch 82/160: 100%|██████████| 82/82 [00:08<00:00, 10.00it/s]
2025-07-02 22:55:43.308 | INFO     | model:train:251 - [epoch 82]: epoch loss = 0.656715,   acc = 0.972645
[epoch 82]: epoch loss = 0.656715,   acc = 0.972645
Epoch 83/160: 100%|██████████| 82/82 [00:08<00:00,  9.90it/s]
2025-07-02 22:55:51.651 | INFO     | model:train:251 - [epoch 83]: epoch loss = 0.644045,   acc = 0.973765
[epoch 83]: epoch loss = 0.644045,   acc = 0.973765
Epoch 84/160: 100%|██████████| 82/82 [00:08<00:00, 10.01it/s]
2025-07-02 22:55:59.905 | INFO     | model:train:251 - [epoch 84]: epoch loss = 0.646317,   acc = 0.972897
[epoch 84]: epoch loss = 0.646317,   acc = 0.972897
Epoch 85/160: 100%|██████████| 82/82 [00:08<00:00, 10.04it/s]
2025-07-02 22:56:08.150 | INFO     | model:train:251 - [epoch 85]: epoch loss = 0.683394,   acc = 0.969514
[epoch 85]: epoch loss = 0.683394,   acc = 0.969514
Epoch 86/160: 100%|██████████| 82/82 [00:08<00:00, 10.02it/s]
2025-07-02 22:56:16.393 | INFO     | model:train:251 - [epoch 86]: epoch loss = 0.650097,   acc = 0.972305
[epoch 86]: epoch loss = 0.650097,   acc = 0.972305
Epoch 87/160: 100%|██████████| 82/82 [00:08<00:00, 10.06it/s]
2025-07-02 22:56:24.617 | INFO     | model:train:251 - [epoch 87]: epoch loss = 0.625360,   acc = 0.974463
[epoch 87]: epoch loss = 0.625360,   acc = 0.974463
Epoch 88/160: 100%|██████████| 82/82 [00:08<00:00, 10.11it/s]
2025-07-02 22:56:32.794 | INFO     | model:train:251 - [epoch 88]: epoch loss = 0.613884,   acc = 0.975071
[epoch 88]: epoch loss = 0.613884,   acc = 0.975071
Epoch 89/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 22:56:41.146 | INFO     | model:train:251 - [epoch 89]: epoch loss = 0.609722,   acc = 0.975423
[epoch 89]: epoch loss = 0.609722,   acc = 0.975423
Epoch 90/160: 100%|██████████| 82/82 [00:08<00:00, 10.21it/s]
2025-07-02 22:56:49.239 | INFO     | model:train:251 - [epoch 90]: epoch loss = 0.609300,   acc = 0.974749
[epoch 90]: epoch loss = 0.609300,   acc = 0.974749
Epoch 91/160: 100%|██████████| 82/82 [00:08<00:00,  9.95it/s]
2025-07-02 22:56:57.543 | INFO     | model:train:251 - [epoch 91]: epoch loss = 0.603850,   acc = 0.975075
[epoch 91]: epoch loss = 0.603850,   acc = 0.975075
Epoch 92/160: 100%|██████████| 82/82 [00:08<00:00, 10.08it/s]
2025-07-02 22:57:05.742 | INFO     | model:train:251 - [epoch 92]: epoch loss = 0.589414,   acc = 0.976166
[epoch 92]: epoch loss = 0.589414,   acc = 0.976166
Epoch 93/160: 100%|██████████| 82/82 [00:08<00:00, 10.02it/s]
2025-07-02 22:57:13.988 | INFO     | model:train:251 - [epoch 93]: epoch loss = 0.590051,   acc = 0.975931
[epoch 93]: epoch loss = 0.590051,   acc = 0.975931
Epoch 94/160: 100%|██████████| 82/82 [00:07<00:00, 10.25it/s]
2025-07-02 22:57:22.048 | INFO     | model:train:251 - [epoch 94]: epoch loss = 0.588507,   acc = 0.976250
[epoch 94]: epoch loss = 0.588507,   acc = 0.976250
Epoch 95/160: 100%|██████████| 82/82 [00:08<00:00,  9.68it/s]
2025-07-02 22:57:30.591 | INFO     | model:train:251 - [epoch 95]: epoch loss = 0.582607,   acc = 0.976378
[epoch 95]: epoch loss = 0.582607,   acc = 0.976378
Epoch 96/160: 100%|██████████| 82/82 [00:08<00:00,  9.81it/s]
2025-07-02 22:57:39.078 | INFO     | model:train:251 - [epoch 96]: epoch loss = 0.569782,   acc = 0.977236
[epoch 96]: epoch loss = 0.569782,   acc = 0.977236
Epoch 97/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 22:57:47.553 | INFO     | model:train:251 - [epoch 97]: epoch loss = 0.562851,   acc = 0.977955
[epoch 97]: epoch loss = 0.562851,   acc = 0.977955
Epoch 98/160: 100%|██████████| 82/82 [00:08<00:00,  9.66it/s]
2025-07-02 22:57:56.130 | INFO     | model:train:251 - [epoch 98]: epoch loss = 0.563176,   acc = 0.977368
[epoch 98]: epoch loss = 0.563176,   acc = 0.977368
Epoch 99/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 22:58:04.618 | INFO     | model:train:251 - [epoch 99]: epoch loss = 0.557685,   acc = 0.977850
[epoch 99]: epoch loss = 0.557685,   acc = 0.977850
Epoch 100/160: 100%|██████████| 82/82 [00:08<00:00, 10.12it/s]
2025-07-02 22:58:12.808 | INFO     | model:train:251 - [epoch 100]: epoch loss = 0.557161,   acc = 0.977435
[epoch 100]: epoch loss = 0.557161,   acc = 0.977435
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 9416,  5540,  5130,  6911, 11950, 14851,  4216,  6196,   111,
        1889,  2570,  2390]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 8380,  6077,  4321,  5525, 13939, 13696,  5106,  6929,   112,
        1780,  1352,  3953]))
2025-07-02 22:58:13.057 | INFO     | model:train:267 - [epoch 100]: val loss = 7.211073,   val acc = 0.633118,   val balanced acc = 0.567356
[epoch 100]: val loss = 7.211073,   val acc = 0.633118,   val balanced acc = 0.567356
2025-07-02 22:58:13.092 | INFO     | model:train:287 - EarlyStopping counter: 2 out of 3
EarlyStopping counter: 2 out of 3
Epoch 101/160:  76%|███████▌  | 62/82 [00:06<00:01, 11.40it/s]wandb: WARNING Tried to log to step 100 that is less than the current step 101. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 101/160: 100%|██████████| 82/82 [00:08<00:00,  9.98it/s]
2025-07-02 22:58:21.417 | INFO     | model:train:251 - [epoch 101]: epoch loss = 0.545215,   acc = 0.978445
[epoch 101]: epoch loss = 0.545215,   acc = 0.978445
Epoch 102/160: 100%|██████████| 82/82 [00:08<00:00, 10.12it/s]
2025-07-02 22:58:29.609 | INFO     | model:train:251 - [epoch 102]: epoch loss = 0.540306,   acc = 0.979026
[epoch 102]: epoch loss = 0.540306,   acc = 0.979026
Epoch 103/160: 100%|██████████| 82/82 [00:08<00:00, 10.16it/s]
2025-07-02 22:58:37.790 | INFO     | model:train:251 - [epoch 103]: epoch loss = 0.541522,   acc = 0.978384
[epoch 103]: epoch loss = 0.541522,   acc = 0.978384
Epoch 104/160: 100%|██████████| 82/82 [00:08<00:00, 10.07it/s]
2025-07-02 22:58:45.997 | INFO     | model:train:251 - [epoch 104]: epoch loss = 0.537986,   acc = 0.978486
[epoch 104]: epoch loss = 0.537986,   acc = 0.978486
Epoch 105/160: 100%|██████████| 82/82 [00:08<00:00,  9.94it/s]
2025-07-02 22:58:54.321 | INFO     | model:train:251 - [epoch 105]: epoch loss = 0.531099,   acc = 0.979048
[epoch 105]: epoch loss = 0.531099,   acc = 0.979048
Epoch 106/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 22:59:02.725 | INFO     | model:train:251 - [epoch 106]: epoch loss = 0.529023,   acc = 0.978984
[epoch 106]: epoch loss = 0.529023,   acc = 0.978984
Epoch 107/160: 100%|██████████| 82/82 [00:08<00:00, 10.00it/s]
2025-07-02 22:59:11.009 | INFO     | model:train:251 - [epoch 107]: epoch loss = 0.519469,   acc = 0.979844
[epoch 107]: epoch loss = 0.519469,   acc = 0.979844
Epoch 108/160: 100%|██████████| 82/82 [00:08<00:00,  9.81it/s]
2025-07-02 22:59:19.444 | INFO     | model:train:251 - [epoch 108]: epoch loss = 0.515723,   acc = 0.980159
[epoch 108]: epoch loss = 0.515723,   acc = 0.980159
Epoch 109/160: 100%|██████████| 82/82 [00:08<00:00,  9.82it/s]
2025-07-02 22:59:27.861 | INFO     | model:train:251 - [epoch 109]: epoch loss = 0.511592,   acc = 0.980214
[epoch 109]: epoch loss = 0.511592,   acc = 0.980214
Epoch 110/160: 100%|██████████| 82/82 [00:08<00:00,  9.86it/s]
2025-07-02 22:59:36.258 | INFO     | model:train:251 - [epoch 110]: epoch loss = 0.511266,   acc = 0.980295
[epoch 110]: epoch loss = 0.511266,   acc = 0.980295
Epoch 111/160: 100%|██████████| 82/82 [00:08<00:00,  9.62it/s]
2025-07-02 22:59:44.866 | INFO     | model:train:251 - [epoch 111]: epoch loss = 0.504140,   acc = 0.980837
[epoch 111]: epoch loss = 0.504140,   acc = 0.980837
Epoch 112/160: 100%|██████████| 82/82 [00:08<00:00,  9.57it/s]
2025-07-02 22:59:53.493 | INFO     | model:train:251 - [epoch 112]: epoch loss = 0.509310,   acc = 0.980057
[epoch 112]: epoch loss = 0.509310,   acc = 0.980057
Epoch 113/160: 100%|██████████| 82/82 [00:08<00:00,  9.78it/s]
2025-07-02 23:00:01.958 | INFO     | model:train:251 - [epoch 113]: epoch loss = 0.503437,   acc = 0.980204
[epoch 113]: epoch loss = 0.503437,   acc = 0.980204
Epoch 114/160: 100%|██████████| 82/82 [00:08<00:00,  9.67it/s]
2025-07-02 23:00:10.499 | INFO     | model:train:251 - [epoch 114]: epoch loss = 0.498235,   acc = 0.980732
[epoch 114]: epoch loss = 0.498235,   acc = 0.980732
Epoch 115/160: 100%|██████████| 82/82 [00:08<00:00,  9.81it/s]
2025-07-02 23:00:18.945 | INFO     | model:train:251 - [epoch 115]: epoch loss = 0.493813,   acc = 0.980987
[epoch 115]: epoch loss = 0.493813,   acc = 0.980987
Epoch 116/160: 100%|██████████| 82/82 [00:08<00:00,  9.66it/s]
2025-07-02 23:00:27.505 | INFO     | model:train:251 - [epoch 116]: epoch loss = 0.486731,   acc = 0.981772
[epoch 116]: epoch loss = 0.486731,   acc = 0.981772
Epoch 117/160: 100%|██████████| 82/82 [00:08<00:00,  9.87it/s]
2025-07-02 23:00:35.885 | INFO     | model:train:251 - [epoch 117]: epoch loss = 0.486278,   acc = 0.981638
[epoch 117]: epoch loss = 0.486278,   acc = 0.981638
Epoch 118/160: 100%|██████████| 82/82 [00:08<00:00,  9.95it/s]
2025-07-02 23:00:44.386 | INFO     | model:train:251 - [epoch 118]: epoch loss = 0.486740,   acc = 0.981451
[epoch 118]: epoch loss = 0.486740,   acc = 0.981451
Epoch 119/160: 100%|██████████| 82/82 [00:08<00:00,  9.95it/s]
2025-07-02 23:00:52.698 | INFO     | model:train:251 - [epoch 119]: epoch loss = 0.478907,   acc = 0.982044
[epoch 119]: epoch loss = 0.478907,   acc = 0.982044
Epoch 120/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 23:01:01.176 | INFO     | model:train:251 - [epoch 120]: epoch loss = 0.479001,   acc = 0.981942
[epoch 120]: epoch loss = 0.479001,   acc = 0.981942
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 8945,  5460,  4620,  7154, 12566, 14992,  4227,  6563,   110,
        1665,  2158,  2710]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 8380,  6077,  4321,  5525, 13939, 13696,  5106,  6929,   112,
        1780,  1352,  3953]))
2025-07-02 23:01:01.479 | INFO     | model:train:267 - [epoch 120]: val loss = 7.358857,   val acc = 0.637769,   val balanced acc = 0.563762
[epoch 120]: val loss = 7.358857,   val acc = 0.637769,   val balanced acc = 0.563762
2025-07-02 23:01:01.515 | INFO     | model:train:287 - EarlyStopping counter: 3 out of 3
EarlyStopping counter: 3 out of 3
2025-07-02 23:01:01.516 | INFO     | model:train:289 - Early stopping triggered.
Early stopping triggered.
