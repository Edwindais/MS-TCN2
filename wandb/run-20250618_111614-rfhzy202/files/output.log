Epoch 1/160: 84it [00:11,  7.59it/s]                        
2025-06-18 11:16:27.210 | INFO     | model:train:239 - [epoch 1]: epoch loss = 2.452914,   acc = 0.153040
[epoch 1]: epoch loss = 2.452914,   acc = 0.153040
Epoch 2/160: 84it [00:10,  8.05it/s]                        
2025-06-18 11:16:37.717 | INFO     | model:train:239 - [epoch 2]: epoch loss = 2.378839,   acc = 0.159264
[epoch 2]: epoch loss = 2.378839,   acc = 0.159264
Epoch 3/160: 84it [00:10,  8.16it/s]                        
2025-06-18 11:16:48.150 | INFO     | model:train:239 - [epoch 3]: epoch loss = 2.326214,   acc = 0.174145
[epoch 3]: epoch loss = 2.326214,   acc = 0.174145
Epoch 4/160: 84it [00:10,  8.19it/s]                        
2025-06-18 11:16:58.548 | INFO     | model:train:239 - [epoch 4]: epoch loss = 2.275128,   acc = 0.188101
[epoch 4]: epoch loss = 2.275128,   acc = 0.188101
Epoch 5/160: 84it [00:10,  8.31it/s]                        
2025-06-18 11:17:08.781 | INFO     | model:train:239 - [epoch 5]: epoch loss = 2.200812,   acc = 0.181679
[epoch 5]: epoch loss = 2.200812,   acc = 0.181679
Epoch 6/160: 84it [00:10,  8.23it/s]                        
2025-06-18 11:17:19.081 | INFO     | model:train:239 - [epoch 6]: epoch loss = 2.096908,   acc = 0.186506
[epoch 6]: epoch loss = 2.096908,   acc = 0.186506
Epoch 7/160: 84it [00:10,  7.99it/s]                        
2025-06-18 11:17:29.687 | INFO     | model:train:239 - [epoch 7]: epoch loss = 1.939176,   acc = 0.283843
[epoch 7]: epoch loss = 1.939176,   acc = 0.283843
Epoch 8/160: 84it [00:10,  7.90it/s]                        
2025-06-18 11:17:40.408 | INFO     | model:train:239 - [epoch 8]: epoch loss = 1.744074,   acc = 0.407388
[epoch 8]: epoch loss = 1.744074,   acc = 0.407388
Epoch 9/160: 84it [00:10,  7.89it/s]                        
2025-06-18 11:17:51.139 | INFO     | model:train:239 - [epoch 9]: epoch loss = 1.614268,   acc = 0.484824
[epoch 9]: epoch loss = 1.614268,   acc = 0.484824
Epoch 10/160: 84it [00:10,  7.87it/s]                        
2025-06-18 11:18:01.894 | INFO     | model:train:239 - [epoch 10]: epoch loss = 1.670613,   acc = 0.476115
[epoch 10]: epoch loss = 1.670613,   acc = 0.476115
Epoch 11/160: 84it [00:10,  7.96it/s]                        
2025-06-18 11:18:12.522 | INFO     | model:train:239 - [epoch 11]: epoch loss = 1.488075,   acc = 0.555820
[epoch 11]: epoch loss = 1.488075,   acc = 0.555820
Epoch 12/160: 84it [00:10,  8.03it/s]                        
2025-06-18 11:18:23.075 | INFO     | model:train:239 - [epoch 12]: epoch loss = 1.401717,   acc = 0.586886
[epoch 12]: epoch loss = 1.401717,   acc = 0.586886
Epoch 13/160: 84it [00:10,  7.87it/s]                        
2025-06-18 11:18:33.825 | INFO     | model:train:239 - [epoch 13]: epoch loss = 1.341079,   acc = 0.606543
[epoch 13]: epoch loss = 1.341079,   acc = 0.606543
Epoch 14/160: 84it [00:10,  8.07it/s]                        
2025-06-18 11:18:44.383 | INFO     | model:train:239 - [epoch 14]: epoch loss = 1.263841,   acc = 0.637844
[epoch 14]: epoch loss = 1.263841,   acc = 0.637844
Epoch 15/160: 84it [00:10,  7.89it/s]                        
2025-06-18 11:18:55.109 | INFO     | model:train:239 - [epoch 15]: epoch loss = 1.241553,   acc = 0.649237
[epoch 15]: epoch loss = 1.241553,   acc = 0.649237
Epoch 16/160: 84it [00:10,  8.08it/s]                        
2025-06-18 11:19:05.590 | INFO     | model:train:239 - [epoch 16]: epoch loss = 1.199743,   acc = 0.664932
[epoch 16]: epoch loss = 1.199743,   acc = 0.664932
Epoch 17/160: 84it [00:09,  8.58it/s]                        
2025-06-18 11:19:15.463 | INFO     | model:train:239 - [epoch 17]: epoch loss = 1.152920,   acc = 0.684295
[epoch 17]: epoch loss = 1.152920,   acc = 0.684295
Epoch 18/160: 84it [00:10,  8.26it/s]                        
2025-06-18 11:19:25.729 | INFO     | model:train:239 - [epoch 18]: epoch loss = 1.125577,   acc = 0.692514
[epoch 18]: epoch loss = 1.125577,   acc = 0.692514
Epoch 19/160: 84it [00:10,  8.34it/s]                        
2025-06-18 11:19:35.880 | INFO     | model:train:239 - [epoch 19]: epoch loss = 1.051361,   acc = 0.716363
[epoch 19]: epoch loss = 1.051361,   acc = 0.716363
Epoch 20/160: 84it [00:10,  8.27it/s]                        
2025-06-18 11:19:46.121 | INFO     | model:train:239 - [epoch 20]: epoch loss = 1.014020,   acc = 0.730159
[epoch 20]: epoch loss = 1.014020,   acc = 0.730159
/home/djh/my_envs/dai_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2776: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 7786,  5411,  4982,  6362, 13712, 12726, 10511,  7955,   453,
        1746,   561,  3958]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([ 9488,  6643,  5408,  5975, 14715, 15247,  6790,  6138,  1236,
        1309,  3214]))
2025-06-18 11:19:46.445 | INFO     | model:train:255 - [epoch 20]: val loss = 2.410226,   val acc = 0.579770,   val balanced acc = 0.545885
[epoch 20]: val loss = 2.410226,   val acc = 0.579770,   val balanced acc = 0.545885
Epoch 21/160:  88%|████████▊ | 72/82 [00:08<00:01,  7.79it/s]wandb: WARNING Tried to log to step 20 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 21/160: 84it [00:10,  8.15it/s]                        
2025-06-18 11:19:56.873 | INFO     | model:train:239 - [epoch 21]: epoch loss = 0.978643,   acc = 0.745355
[epoch 21]: epoch loss = 0.978643,   acc = 0.745355
Epoch 22/160: 84it [00:10,  8.19it/s]                        
2025-06-18 11:20:07.215 | INFO     | model:train:239 - [epoch 22]: epoch loss = 0.982902,   acc = 0.737662
[epoch 22]: epoch loss = 0.982902,   acc = 0.737662
Epoch 23/160: 84it [00:10,  8.33it/s]                        
2025-06-18 11:20:17.377 | INFO     | model:train:239 - [epoch 23]: epoch loss = 0.959970,   acc = 0.754553
[epoch 23]: epoch loss = 0.959970,   acc = 0.754553
Epoch 24/160: 84it [00:10,  8.15it/s]                        
2025-06-18 11:20:27.754 | INFO     | model:train:239 - [epoch 24]: epoch loss = 1.049776,   acc = 0.716587
[epoch 24]: epoch loss = 1.049776,   acc = 0.716587
Epoch 25/160: 84it [00:10,  8.28it/s]                        
2025-06-18 11:20:37.957 | INFO     | model:train:239 - [epoch 25]: epoch loss = 0.958334,   acc = 0.752077
[epoch 25]: epoch loss = 0.958334,   acc = 0.752077
Epoch 26/160: 84it [00:10,  8.24it/s]                        
2025-06-18 11:20:48.415 | INFO     | model:train:239 - [epoch 26]: epoch loss = 0.900521,   acc = 0.768875
[epoch 26]: epoch loss = 0.900521,   acc = 0.768875
Epoch 27/160: 84it [00:10,  8.04it/s]                        
2025-06-18 11:20:58.923 | INFO     | model:train:239 - [epoch 27]: epoch loss = 0.888368,   acc = 0.773605
[epoch 27]: epoch loss = 0.888368,   acc = 0.773605
Epoch 28/160: 84it [00:10,  7.89it/s]                        
2025-06-18 11:21:09.634 | INFO     | model:train:239 - [epoch 28]: epoch loss = 0.851790,   acc = 0.785978
[epoch 28]: epoch loss = 0.851790,   acc = 0.785978
Epoch 29/160: 84it [00:10,  8.01it/s]                        
2025-06-18 11:21:20.215 | INFO     | model:train:239 - [epoch 29]: epoch loss = 0.929034,   acc = 0.757144
[epoch 29]: epoch loss = 0.929034,   acc = 0.757144
Epoch 30/160: 84it [00:10,  7.87it/s]                        
2025-06-18 11:21:30.979 | INFO     | model:train:239 - [epoch 30]: epoch loss = 0.845485,   acc = 0.780534
[epoch 30]: epoch loss = 0.845485,   acc = 0.780534
Epoch 31/160: 84it [00:10,  8.36it/s]                        
2025-06-18 11:21:41.120 | INFO     | model:train:239 - [epoch 31]: epoch loss = 0.788874,   acc = 0.805236
[epoch 31]: epoch loss = 0.788874,   acc = 0.805236
Epoch 32/160: 84it [00:10,  8.38it/s]                        
2025-06-18 11:21:51.201 | INFO     | model:train:239 - [epoch 32]: epoch loss = 0.744801,   acc = 0.819576
[epoch 32]: epoch loss = 0.744801,   acc = 0.819576
Epoch 33/160: 84it [00:10,  8.23it/s]                        
2025-06-18 11:22:01.509 | INFO     | model:train:239 - [epoch 33]: epoch loss = 0.723521,   acc = 0.831410
[epoch 33]: epoch loss = 0.723521,   acc = 0.831410
Epoch 34/160: 84it [00:10,  8.26it/s]                        
2025-06-18 11:22:11.785 | INFO     | model:train:239 - [epoch 34]: epoch loss = 0.698961,   acc = 0.832610
[epoch 34]: epoch loss = 0.698961,   acc = 0.832610
Epoch 35/160: 84it [00:10,  8.29it/s]                        
2025-06-18 11:22:22.024 | INFO     | model:train:239 - [epoch 35]: epoch loss = 0.698429,   acc = 0.837850
[epoch 35]: epoch loss = 0.698429,   acc = 0.837850
Epoch 36/160: 84it [00:10,  8.03it/s]                        
2025-06-18 11:22:32.587 | INFO     | model:train:239 - [epoch 36]: epoch loss = 0.701988,   acc = 0.836962
[epoch 36]: epoch loss = 0.701988,   acc = 0.836962
Epoch 37/160: 84it [00:10,  8.18it/s]                        
2025-06-18 11:22:42.995 | INFO     | model:train:239 - [epoch 37]: epoch loss = 0.754888,   acc = 0.814314
[epoch 37]: epoch loss = 0.754888,   acc = 0.814314
Epoch 38/160: 84it [00:10,  8.25it/s]                        
2025-06-18 11:22:53.272 | INFO     | model:train:239 - [epoch 38]: epoch loss = 0.697652,   acc = 0.834018
[epoch 38]: epoch loss = 0.697652,   acc = 0.834018
Epoch 39/160: 84it [00:10,  8.03it/s]                        
2025-06-18 11:23:03.803 | INFO     | model:train:239 - [epoch 39]: epoch loss = 0.711186,   acc = 0.830213
[epoch 39]: epoch loss = 0.711186,   acc = 0.830213
Epoch 40/160: 84it [00:10,  8.19it/s]                        
2025-06-18 11:23:14.147 | INFO     | model:train:239 - [epoch 40]: epoch loss = 0.681682,   acc = 0.840673
[epoch 40]: epoch loss = 0.681682,   acc = 0.840673
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([ 8765,  7417,  4946,  5969, 11972, 18288,  5119,  8080,   952,
         732,  3923]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([ 9488,  6643,  5408,  5975, 14715, 15247,  6790,  6138,  1236,
        1309,  3214]))
2025-06-18 11:23:14.494 | INFO     | model:train:255 - [epoch 40]: val loss = 2.364198,   val acc = 0.586440,   val balanced acc = 0.544824
[epoch 40]: val loss = 2.364198,   val acc = 0.586440,   val balanced acc = 0.544824
2025-06-18 11:23:14.532 | INFO     | model:train:275 - EarlyStopping counter: 1 out of 3
EarlyStopping counter: 1 out of 3
Epoch 41/160:  10%|▉         | 8/82 [00:00<00:07, 10.10it/s]wandb: WARNING Tried to log to step 40 that is less than the current step 41. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 41/160: 84it [00:10,  8.08it/s]                        
2025-06-18 11:23:24.996 | INFO     | model:train:239 - [epoch 41]: epoch loss = 0.619296,   acc = 0.861750
[epoch 41]: epoch loss = 0.619296,   acc = 0.861750
Epoch 42/160: 84it [00:10,  7.97it/s]                        
2025-06-18 11:23:35.630 | INFO     | model:train:239 - [epoch 42]: epoch loss = 0.597758,   acc = 0.867547
[epoch 42]: epoch loss = 0.597758,   acc = 0.867547
Epoch 43/160: 84it [00:10,  8.04it/s]                        
2025-06-18 11:23:46.191 | INFO     | model:train:239 - [epoch 43]: epoch loss = 0.581559,   acc = 0.872988
[epoch 43]: epoch loss = 0.581559,   acc = 0.872988
Epoch 44/160: 84it [00:10,  8.17it/s]                        
2025-06-18 11:23:57.027 | INFO     | model:train:239 - [epoch 44]: epoch loss = 0.570133,   acc = 0.877650
[epoch 44]: epoch loss = 0.570133,   acc = 0.877650
Epoch 45/160: 84it [00:10,  8.25it/s]                        
2025-06-18 11:24:07.293 | INFO     | model:train:239 - [epoch 45]: epoch loss = 0.567724,   acc = 0.879509
[epoch 45]: epoch loss = 0.567724,   acc = 0.879509
Epoch 46/160: 84it [00:10,  8.11it/s]                        
2025-06-18 11:24:17.744 | INFO     | model:train:239 - [epoch 46]: epoch loss = 0.558868,   acc = 0.882064
[epoch 46]: epoch loss = 0.558868,   acc = 0.882064
Epoch 47/160: 84it [00:10,  7.79it/s]                        
2025-06-18 11:24:28.593 | INFO     | model:train:239 - [epoch 47]: epoch loss = 0.536276,   acc = 0.887011
[epoch 47]: epoch loss = 0.536276,   acc = 0.887011
Epoch 48/160: 84it [00:10,  8.13it/s]                        
2025-06-18 11:24:39.021 | INFO     | model:train:239 - [epoch 48]: epoch loss = 0.531982,   acc = 0.889206
[epoch 48]: epoch loss = 0.531982,   acc = 0.889206
Epoch 49/160: 84it [00:10,  8.22it/s]                        
2025-06-18 11:24:49.329 | INFO     | model:train:239 - [epoch 49]: epoch loss = 0.521567,   acc = 0.891632
[epoch 49]: epoch loss = 0.521567,   acc = 0.891632
Epoch 50/160: 84it [00:10,  8.17it/s]                        
2025-06-18 11:24:59.678 | INFO     | model:train:239 - [epoch 50]: epoch loss = 0.506500,   acc = 0.896709
[epoch 50]: epoch loss = 0.506500,   acc = 0.896709
Epoch 51/160: 84it [00:10,  8.01it/s]                        
2025-06-18 11:25:10.246 | INFO     | model:train:239 - [epoch 51]: epoch loss = 0.498479,   acc = 0.899362
[epoch 51]: epoch loss = 0.498479,   acc = 0.899362
Epoch 52/160: 84it [00:10,  8.32it/s]                        
2025-06-18 11:25:20.448 | INFO     | model:train:239 - [epoch 52]: epoch loss = 0.494814,   acc = 0.900807
[epoch 52]: epoch loss = 0.494814,   acc = 0.900807
Epoch 53/160: 84it [00:10,  8.16it/s]                        
2025-06-18 11:25:30.801 | INFO     | model:train:239 - [epoch 53]: epoch loss = 0.488518,   acc = 0.901846
[epoch 53]: epoch loss = 0.488518,   acc = 0.901846
Epoch 54/160: 84it [00:10,  8.13it/s]                        
2025-06-18 11:25:41.203 | INFO     | model:train:239 - [epoch 54]: epoch loss = 0.480646,   acc = 0.903813
[epoch 54]: epoch loss = 0.480646,   acc = 0.903813
Epoch 55/160: 84it [00:10,  8.08it/s]                        
2025-06-18 11:25:51.679 | INFO     | model:train:239 - [epoch 55]: epoch loss = 0.474666,   acc = 0.904845
[epoch 55]: epoch loss = 0.474666,   acc = 0.904845
Epoch 56/160: 84it [00:10,  8.15it/s]                        
2025-06-18 11:26:02.079 | INFO     | model:train:239 - [epoch 56]: epoch loss = 0.471456,   acc = 0.906993
[epoch 56]: epoch loss = 0.471456,   acc = 0.906993
Epoch 57/160: 84it [00:10,  8.08it/s]                        
2025-06-18 11:26:12.636 | INFO     | model:train:239 - [epoch 57]: epoch loss = 0.459815,   acc = 0.910396
[epoch 57]: epoch loss = 0.459815,   acc = 0.910396
Epoch 58/160: 84it [00:10,  8.12it/s]                        
2025-06-18 11:26:23.603 | INFO     | model:train:239 - [epoch 58]: epoch loss = 0.450229,   acc = 0.911513
[epoch 58]: epoch loss = 0.450229,   acc = 0.911513
Epoch 59/160: 84it [00:10,  8.36it/s]                        
2025-06-18 11:26:33.730 | INFO     | model:train:239 - [epoch 59]: epoch loss = 0.450107,   acc = 0.912740
[epoch 59]: epoch loss = 0.450107,   acc = 0.912740
Epoch 60/160: 84it [00:10,  8.03it/s]                        
2025-06-18 11:26:44.993 | INFO     | model:train:239 - [epoch 60]: epoch loss = 0.445859,   acc = 0.912962
[epoch 60]: epoch loss = 0.445859,   acc = 0.912962
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([11607,  6905,  4663,  6144, 12643, 17085,  4856,  6572,  1045,
         751,  3892]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([ 9488,  6643,  5408,  5975, 14715, 15247,  6790,  6138,  1236,
        1309,  3214]))
2025-06-18 11:26:45.353 | INFO     | model:train:255 - [epoch 60]: val loss = 2.627297,   val acc = 0.603298,   val balanced acc = 0.559395
[epoch 60]: val loss = 2.627297,   val acc = 0.603298,   val balanced acc = 0.559395
Epoch 61/160:   0%|          | 0/82 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 60 that is less than the current step 61. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 61/160: 84it [00:10,  7.97it/s]                        
2025-06-18 11:26:56.004 | INFO     | model:train:239 - [epoch 61]: epoch loss = 0.433774,   acc = 0.916989
[epoch 61]: epoch loss = 0.433774,   acc = 0.916989
Epoch 62/160: 84it [00:10,  7.90it/s]                        
2025-06-18 11:27:06.711 | INFO     | model:train:239 - [epoch 62]: epoch loss = 0.427379,   acc = 0.917613
[epoch 62]: epoch loss = 0.427379,   acc = 0.917613
Epoch 63/160: 84it [00:10,  7.85it/s]                        
2025-06-18 11:27:17.483 | INFO     | model:train:239 - [epoch 63]: epoch loss = 0.416821,   acc = 0.920247
[epoch 63]: epoch loss = 0.416821,   acc = 0.920247
Epoch 64/160: 84it [00:10,  7.90it/s]                        
2025-06-18 11:27:28.180 | INFO     | model:train:239 - [epoch 64]: epoch loss = 0.428922,   acc = 0.918025
[epoch 64]: epoch loss = 0.428922,   acc = 0.918025
Epoch 65/160: 84it [00:10,  8.06it/s]                        
2025-06-18 11:27:38.676 | INFO     | model:train:239 - [epoch 65]: epoch loss = 0.416674,   acc = 0.920681
[epoch 65]: epoch loss = 0.416674,   acc = 0.920681
Epoch 66/160: 84it [00:10,  7.91it/s]                        
2025-06-18 11:27:49.387 | INFO     | model:train:239 - [epoch 66]: epoch loss = 0.406475,   acc = 0.921915
[epoch 66]: epoch loss = 0.406475,   acc = 0.921915
Epoch 67/160: 84it [00:10,  8.11it/s]                        
2025-06-18 11:27:59.891 | INFO     | model:train:239 - [epoch 67]: epoch loss = 0.406375,   acc = 0.923313
[epoch 67]: epoch loss = 0.406375,   acc = 0.923313
Epoch 68/160: 84it [00:10,  8.14it/s]                        
2025-06-18 11:28:10.273 | INFO     | model:train:239 - [epoch 68]: epoch loss = 0.408374,   acc = 0.924057
[epoch 68]: epoch loss = 0.408374,   acc = 0.924057
Epoch 69/160: 84it [00:10,  8.25it/s]                        
2025-06-18 11:28:20.523 | INFO     | model:train:239 - [epoch 69]: epoch loss = 0.406414,   acc = 0.923017
[epoch 69]: epoch loss = 0.406414,   acc = 0.923017
Epoch 70/160: 84it [00:09,  8.43it/s]                        
2025-06-18 11:28:30.551 | INFO     | model:train:239 - [epoch 70]: epoch loss = 0.403124,   acc = 0.924997
[epoch 70]: epoch loss = 0.403124,   acc = 0.924997
Epoch 71/160: 84it [00:10,  7.98it/s]                        
2025-06-18 11:28:41.142 | INFO     | model:train:239 - [epoch 71]: epoch loss = 0.392193,   acc = 0.925137
[epoch 71]: epoch loss = 0.392193,   acc = 0.925137
Epoch 72/160: 84it [00:10,  8.24it/s]                        
2025-06-18 11:28:51.400 | INFO     | model:train:239 - [epoch 72]: epoch loss = 0.383156,   acc = 0.927131
[epoch 72]: epoch loss = 0.383156,   acc = 0.927131
Epoch 73/160: 84it [00:10,  8.15it/s]                        
2025-06-18 11:29:01.796 | INFO     | model:train:239 - [epoch 73]: epoch loss = 0.376798,   acc = 0.932178
[epoch 73]: epoch loss = 0.376798,   acc = 0.932178
Epoch 74/160: 84it [00:10,  8.01it/s]                        
2025-06-18 11:29:12.360 | INFO     | model:train:239 - [epoch 74]: epoch loss = 0.370390,   acc = 0.932450
[epoch 74]: epoch loss = 0.370390,   acc = 0.932450
Epoch 75/160: 84it [00:10,  8.08it/s]                        
2025-06-18 11:29:22.840 | INFO     | model:train:239 - [epoch 75]: epoch loss = 0.366616,   acc = 0.933331
[epoch 75]: epoch loss = 0.366616,   acc = 0.933331
Epoch 76/160: 84it [00:10,  8.30it/s]                        
2025-06-18 11:29:33.325 | INFO     | model:train:239 - [epoch 76]: epoch loss = 0.364937,   acc = 0.935390
[epoch 76]: epoch loss = 0.364937,   acc = 0.935390
Epoch 77/160: 84it [00:10,  8.06it/s]                        
2025-06-18 11:29:43.816 | INFO     | model:train:239 - [epoch 77]: epoch loss = 0.359267,   acc = 0.936610
[epoch 77]: epoch loss = 0.359267,   acc = 0.936610
Epoch 78/160: 84it [00:10,  8.27it/s]                        
2025-06-18 11:29:54.031 | INFO     | model:train:239 - [epoch 78]: epoch loss = 0.361638,   acc = 0.936869
[epoch 78]: epoch loss = 0.361638,   acc = 0.936869
Epoch 79/160: 84it [00:10,  8.04it/s]                        
2025-06-18 11:30:04.551 | INFO     | model:train:239 - [epoch 79]: epoch loss = 0.354820,   acc = 0.935753
[epoch 79]: epoch loss = 0.354820,   acc = 0.935753
Epoch 80/160: 84it [00:10,  8.14it/s]                        
2025-06-18 11:30:14.936 | INFO     | model:train:239 - [epoch 80]: epoch loss = 0.353152,   acc = 0.936781
[epoch 80]: epoch loss = 0.353152,   acc = 0.936781
/home/djh/my_envs/dai_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2776: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([11139,  6663,  5081,  6579, 12539, 15999,  5682,  7529,     5,
        1171,   589,  3187]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([ 9488,  6643,  5408,  5975, 14715, 15247,  6790,  6138,  1236,
        1309,  3214]))
2025-06-18 11:30:15.277 | INFO     | model:train:255 - [epoch 80]: val loss = 2.504417,   val acc = 0.595814,   val balanced acc = 0.553689
[epoch 80]: val loss = 2.504417,   val acc = 0.595814,   val balanced acc = 0.553689
2025-06-18 11:30:15.333 | INFO     | model:train:275 - EarlyStopping counter: 1 out of 3
EarlyStopping counter: 1 out of 3
Epoch 81/160:   0%|          | 0/82 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 80 that is less than the current step 81. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 81/160: 84it [00:10,  7.85it/s]                        
2025-06-18 11:30:26.090 | INFO     | model:train:239 - [epoch 81]: epoch loss = 0.344240,   acc = 0.939717
[epoch 81]: epoch loss = 0.344240,   acc = 0.939717
Epoch 82/160: 84it [00:10,  8.04it/s]                        
2025-06-18 11:30:36.609 | INFO     | model:train:239 - [epoch 82]: epoch loss = 0.337437,   acc = 0.941256
[epoch 82]: epoch loss = 0.337437,   acc = 0.941256
Epoch 83/160: 84it [00:10,  7.98it/s]                        
2025-06-18 11:30:47.191 | INFO     | model:train:239 - [epoch 83]: epoch loss = 0.336540,   acc = 0.941806
[epoch 83]: epoch loss = 0.336540,   acc = 0.941806
Epoch 84/160: 84it [00:10,  8.31it/s]                        
2025-06-18 11:30:57.388 | INFO     | model:train:239 - [epoch 84]: epoch loss = 0.336923,   acc = 0.941940
[epoch 84]: epoch loss = 0.336923,   acc = 0.941940
Epoch 85/160: 84it [00:10,  8.04it/s]                        
2025-06-18 11:31:07.931 | INFO     | model:train:239 - [epoch 85]: epoch loss = 0.334599,   acc = 0.943014
[epoch 85]: epoch loss = 0.334599,   acc = 0.943014
Epoch 86/160: 84it [00:10,  8.06it/s]                        
2025-06-18 11:31:18.456 | INFO     | model:train:239 - [epoch 86]: epoch loss = 0.334269,   acc = 0.941299
[epoch 86]: epoch loss = 0.334269,   acc = 0.941299
Epoch 87/160: 84it [00:10,  8.19it/s]                        
2025-06-18 11:31:28.785 | INFO     | model:train:239 - [epoch 87]: epoch loss = 0.333481,   acc = 0.942058
[epoch 87]: epoch loss = 0.333481,   acc = 0.942058
Epoch 88/160: 84it [00:10,  8.03it/s]                        
2025-06-18 11:31:39.323 | INFO     | model:train:239 - [epoch 88]: epoch loss = 0.326814,   acc = 0.941606
[epoch 88]: epoch loss = 0.326814,   acc = 0.941606
Epoch 89/160: 84it [00:10,  8.29it/s]                        
2025-06-18 11:31:49.555 | INFO     | model:train:239 - [epoch 89]: epoch loss = 0.330515,   acc = 0.941959
[epoch 89]: epoch loss = 0.330515,   acc = 0.941959
Epoch 90/160: 84it [00:10,  8.21it/s]                        
2025-06-18 11:31:59.851 | INFO     | model:train:239 - [epoch 90]: epoch loss = 0.325939,   acc = 0.943341
[epoch 90]: epoch loss = 0.325939,   acc = 0.943341
Epoch 91/160: 84it [00:10,  8.04it/s]                        
2025-06-18 11:32:10.420 | INFO     | model:train:239 - [epoch 91]: epoch loss = 0.316281,   acc = 0.945247
[epoch 91]: epoch loss = 0.316281,   acc = 0.945247
Epoch 92/160: 84it [00:10,  8.21it/s]                        
2025-06-18 11:32:20.737 | INFO     | model:train:239 - [epoch 92]: epoch loss = 0.316986,   acc = 0.945671
[epoch 92]: epoch loss = 0.316986,   acc = 0.945671
Epoch 93/160: 84it [00:10,  7.87it/s]                        
2025-06-18 11:32:31.509 | INFO     | model:train:239 - [epoch 93]: epoch loss = 0.312810,   acc = 0.947087
[epoch 93]: epoch loss = 0.312810,   acc = 0.947087
Epoch 94/160: 84it [00:10,  7.98it/s]                        
2025-06-18 11:32:42.135 | INFO     | model:train:239 - [epoch 94]: epoch loss = 0.314266,   acc = 0.947258
[epoch 94]: epoch loss = 0.314266,   acc = 0.947258
Epoch 95/160: 84it [00:10,  7.99it/s]                        
2025-06-18 11:32:52.756 | INFO     | model:train:239 - [epoch 95]: epoch loss = 0.312039,   acc = 0.946854
[epoch 95]: epoch loss = 0.312039,   acc = 0.946854
Epoch 96/160: 84it [00:10,  7.97it/s]                        
2025-06-18 11:33:03.367 | INFO     | model:train:239 - [epoch 96]: epoch loss = 0.310464,   acc = 0.947999
[epoch 96]: epoch loss = 0.310464,   acc = 0.947999
Epoch 97/160: 84it [00:10,  7.98it/s]                        
2025-06-18 11:33:13.961 | INFO     | model:train:239 - [epoch 97]: epoch loss = 0.312492,   acc = 0.946980
[epoch 97]: epoch loss = 0.312492,   acc = 0.946980
Epoch 98/160: 84it [00:10,  8.28it/s]                        
2025-06-18 11:33:24.179 | INFO     | model:train:239 - [epoch 98]: epoch loss = 0.307899,   acc = 0.948587
[epoch 98]: epoch loss = 0.307899,   acc = 0.948587
Epoch 99/160: 84it [00:10,  8.09it/s]                        
2025-06-18 11:33:34.651 | INFO     | model:train:239 - [epoch 99]: epoch loss = 0.304352,   acc = 0.949361
[epoch 99]: epoch loss = 0.304352,   acc = 0.949361
Epoch 100/160: 84it [00:10,  8.06it/s]                        
2025-06-18 11:33:45.139 | INFO     | model:train:239 - [epoch 100]: epoch loss = 0.297674,   acc = 0.950208
[epoch 100]: epoch loss = 0.297674,   acc = 0.950208
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([11950,  6791,  4615,  6050, 12219, 17845,  5025,  6566,   873,
         725,  3504]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([ 9488,  6643,  5408,  5975, 14715, 15247,  6790,  6138,  1236,
        1309,  3214]))
2025-06-18 11:33:45.482 | INFO     | model:train:255 - [epoch 100]: val loss = 3.008744,   val acc = 0.596129,   val balanced acc = 0.554574
[epoch 100]: val loss = 3.008744,   val acc = 0.596129,   val balanced acc = 0.554574
2025-06-18 11:33:45.538 | INFO     | model:train:275 - EarlyStopping counter: 2 out of 3
EarlyStopping counter: 2 out of 3
wandb: WARNING Tried to log to step 100 that is less than the current step 101. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 101/160: 84it [00:10,  8.05it/s]                        
2025-06-18 11:33:56.087 | INFO     | model:train:239 - [epoch 101]: epoch loss = 0.301091,   acc = 0.950524
[epoch 101]: epoch loss = 0.301091,   acc = 0.950524
Epoch 102/160: 84it [00:10,  7.94it/s]                        
2025-06-18 11:34:06.751 | INFO     | model:train:239 - [epoch 102]: epoch loss = 0.294621,   acc = 0.950904
[epoch 102]: epoch loss = 0.294621,   acc = 0.950904
Epoch 103/160: 84it [00:10,  8.29it/s]                        
2025-06-18 11:34:16.968 | INFO     | model:train:239 - [epoch 103]: epoch loss = 0.302668,   acc = 0.951190
[epoch 103]: epoch loss = 0.302668,   acc = 0.951190
Epoch 104/160: 84it [00:10,  8.04it/s]                        
2025-06-18 11:34:27.500 | INFO     | model:train:239 - [epoch 104]: epoch loss = 0.296070,   acc = 0.951073
[epoch 104]: epoch loss = 0.296070,   acc = 0.951073
Epoch 105/160: 84it [00:10,  8.01it/s]                        
2025-06-18 11:34:38.065 | INFO     | model:train:239 - [epoch 105]: epoch loss = 0.293147,   acc = 0.952057
[epoch 105]: epoch loss = 0.293147,   acc = 0.952057
Epoch 106/160: 84it [00:10,  8.18it/s]                        
2025-06-18 11:34:48.405 | INFO     | model:train:239 - [epoch 106]: epoch loss = 0.294031,   acc = 0.952750
[epoch 106]: epoch loss = 0.294031,   acc = 0.952750
Epoch 107/160: 84it [00:10,  7.96it/s]                        
2025-06-18 11:34:59.055 | INFO     | model:train:239 - [epoch 107]: epoch loss = 0.286598,   acc = 0.952523
[epoch 107]: epoch loss = 0.286598,   acc = 0.952523
Epoch 108/160: 84it [00:10,  7.94it/s]                        
2025-06-18 11:35:09.711 | INFO     | model:train:239 - [epoch 108]: epoch loss = 0.287213,   acc = 0.953083
[epoch 108]: epoch loss = 0.287213,   acc = 0.953083
Epoch 109/160: 84it [00:10,  8.06it/s]                        
2025-06-18 11:35:20.205 | INFO     | model:train:239 - [epoch 109]: epoch loss = 0.288806,   acc = 0.953503
[epoch 109]: epoch loss = 0.288806,   acc = 0.953503
Epoch 110/160: 84it [00:09,  8.48it/s]                        
2025-06-18 11:35:30.186 | INFO     | model:train:239 - [epoch 110]: epoch loss = 0.292544,   acc = 0.954046
[epoch 110]: epoch loss = 0.292544,   acc = 0.954046
Epoch 111/160: 84it [00:10,  8.10it/s]                        
2025-06-18 11:35:40.627 | INFO     | model:train:239 - [epoch 111]: epoch loss = 0.283623,   acc = 0.953577
[epoch 111]: epoch loss = 0.283623,   acc = 0.953577
Epoch 112/160: 84it [00:10,  8.10it/s]                        
2025-06-18 11:35:51.073 | INFO     | model:train:239 - [epoch 112]: epoch loss = 0.283226,   acc = 0.954474
[epoch 112]: epoch loss = 0.283226,   acc = 0.954474
Epoch 113/160: 84it [00:10,  8.09it/s]                        
2025-06-18 11:36:01.519 | INFO     | model:train:239 - [epoch 113]: epoch loss = 0.283493,   acc = 0.954093
[epoch 113]: epoch loss = 0.283493,   acc = 0.954093
Epoch 114/160: 84it [00:10,  8.10it/s]                        
2025-06-18 11:36:12.153 | INFO     | model:train:239 - [epoch 114]: epoch loss = 0.279740,   acc = 0.954334
[epoch 114]: epoch loss = 0.279740,   acc = 0.954334
Epoch 115/160: 84it [00:10,  8.17it/s]                        
2025-06-18 11:36:22.504 | INFO     | model:train:239 - [epoch 115]: epoch loss = 0.281011,   acc = 0.955314
[epoch 115]: epoch loss = 0.281011,   acc = 0.955314
Epoch 116/160: 84it [00:10,  8.21it/s]                        
2025-06-18 11:36:32.830 | INFO     | model:train:239 - [epoch 116]: epoch loss = 0.277766,   acc = 0.955262
[epoch 116]: epoch loss = 0.277766,   acc = 0.955262
Epoch 117/160: 84it [00:10,  8.16it/s]                        
2025-06-18 11:36:43.245 | INFO     | model:train:239 - [epoch 117]: epoch loss = 0.275449,   acc = 0.956837
[epoch 117]: epoch loss = 0.275449,   acc = 0.956837
Epoch 118/160: 84it [00:10,  8.15it/s]                        
2025-06-18 11:36:53.645 | INFO     | model:train:239 - [epoch 118]: epoch loss = 0.274844,   acc = 0.956651
[epoch 118]: epoch loss = 0.274844,   acc = 0.956651
Epoch 119/160: 84it [00:10,  8.20it/s]                        
2025-06-18 11:37:03.964 | INFO     | model:train:239 - [epoch 119]: epoch loss = 0.274749,   acc = 0.956076
[epoch 119]: epoch loss = 0.274749,   acc = 0.956076
Epoch 120/160: 84it [00:10,  8.14it/s]                        
2025-06-18 11:37:14.475 | INFO     | model:train:239 - [epoch 120]: epoch loss = 0.272727,   acc = 0.956813
[epoch 120]: epoch loss = 0.272727,   acc = 0.956813
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([11700,  6793,  4893,  6585, 13410, 15208,  4790,  7588,   990,
         666,  3540]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([ 9488,  6643,  5408,  5975, 14715, 15247,  6790,  6138,  1236,
        1309,  3214]))
2025-06-18 11:37:14.815 | INFO     | model:train:255 - [epoch 120]: val loss = 2.877807,   val acc = 0.580518,   val balanced acc = 0.551560
[epoch 120]: val loss = 2.877807,   val acc = 0.580518,   val balanced acc = 0.551560
2025-06-18 11:37:14.893 | INFO     | model:train:275 - EarlyStopping counter: 3 out of 3
EarlyStopping counter: 3 out of 3
2025-06-18 11:37:14.893 | INFO     | model:train:277 - Early stopping triggered.
Early stopping triggered.
