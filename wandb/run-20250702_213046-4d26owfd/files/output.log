Epoch 1/160: 100%|██████████| 82/82 [00:30<00:00,  2.66it/s]
2025-07-02 21:31:18.546 | INFO     | model:train:251 - [epoch 1]: epoch loss = 9.307054,   acc = 0.168968
[epoch 1]: epoch loss = 9.307054,   acc = 0.168968
Epoch 2/160: 100%|██████████| 82/82 [00:08<00:00,  9.64it/s]
2025-07-02 21:31:27.090 | INFO     | model:train:251 - [epoch 2]: epoch loss = 8.357656,   acc = 0.210776
[epoch 2]: epoch loss = 8.357656,   acc = 0.210776
Epoch 3/160: 100%|██████████| 82/82 [00:08<00:00,  9.70it/s]
2025-07-02 21:31:35.603 | INFO     | model:train:251 - [epoch 3]: epoch loss = 6.991138,   acc = 0.380342
[epoch 3]: epoch loss = 6.991138,   acc = 0.380342
Epoch 4/160: 100%|██████████| 82/82 [00:08<00:00,  9.71it/s]
2025-07-02 21:31:44.384 | INFO     | model:train:251 - [epoch 4]: epoch loss = 5.871901,   acc = 0.530971
[epoch 4]: epoch loss = 5.871901,   acc = 0.530971
Epoch 5/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 21:31:52.840 | INFO     | model:train:251 - [epoch 5]: epoch loss = 5.318020,   acc = 0.588879
[epoch 5]: epoch loss = 5.318020,   acc = 0.588879
Epoch 6/160: 100%|██████████| 82/82 [00:08<00:00,  9.72it/s]
2025-07-02 21:32:01.351 | INFO     | model:train:251 - [epoch 6]: epoch loss = 4.875350,   acc = 0.634863
[epoch 6]: epoch loss = 4.875350,   acc = 0.634863
Epoch 7/160: 100%|██████████| 82/82 [00:08<00:00,  9.72it/s]
2025-07-02 21:32:09.848 | INFO     | model:train:251 - [epoch 7]: epoch loss = 4.508679,   acc = 0.666207
[epoch 7]: epoch loss = 4.508679,   acc = 0.666207
Epoch 8/160: 100%|██████████| 82/82 [00:08<00:00,  9.72it/s]
2025-07-02 21:32:18.368 | INFO     | model:train:251 - [epoch 8]: epoch loss = 4.247842,   acc = 0.691294
[epoch 8]: epoch loss = 4.247842,   acc = 0.691294
Epoch 9/160: 100%|██████████| 82/82 [00:08<00:00,  9.62it/s]
2025-07-02 21:32:26.951 | INFO     | model:train:251 - [epoch 9]: epoch loss = 3.979647,   acc = 0.715455
[epoch 9]: epoch loss = 3.979647,   acc = 0.715455
Epoch 10/160: 100%|██████████| 82/82 [00:08<00:00,  9.56it/s]
2025-07-02 21:32:35.590 | INFO     | model:train:251 - [epoch 10]: epoch loss = 3.832259,   acc = 0.724308
[epoch 10]: epoch loss = 3.832259,   acc = 0.724308
Epoch 11/160: 100%|██████████| 82/82 [00:08<00:00,  9.58it/s]
2025-07-02 21:32:44.210 | INFO     | model:train:251 - [epoch 11]: epoch loss = 3.544668,   acc = 0.751434
[epoch 11]: epoch loss = 3.544668,   acc = 0.751434
Epoch 12/160: 100%|██████████| 82/82 [00:08<00:00,  9.66it/s]
2025-07-02 21:32:52.758 | INFO     | model:train:251 - [epoch 12]: epoch loss = 3.474395,   acc = 0.761655
[epoch 12]: epoch loss = 3.474395,   acc = 0.761655
Epoch 13/160: 100%|██████████| 82/82 [00:08<00:00,  9.72it/s]
2025-07-02 21:33:01.257 | INFO     | model:train:251 - [epoch 13]: epoch loss = 3.485752,   acc = 0.758460
[epoch 13]: epoch loss = 3.485752,   acc = 0.758460
Epoch 14/160: 100%|██████████| 82/82 [00:08<00:00,  9.65it/s]
2025-07-02 21:33:09.821 | INFO     | model:train:251 - [epoch 14]: epoch loss = 3.052609,   acc = 0.795520
[epoch 14]: epoch loss = 3.052609,   acc = 0.795520
Epoch 15/160: 100%|██████████| 82/82 [00:08<00:00,  9.64it/s]
2025-07-02 21:33:18.389 | INFO     | model:train:251 - [epoch 15]: epoch loss = 2.861248,   acc = 0.816273
[epoch 15]: epoch loss = 2.861248,   acc = 0.816273
Epoch 16/160: 100%|██████████| 82/82 [00:08<00:00,  9.62it/s]
2025-07-02 21:33:26.993 | INFO     | model:train:251 - [epoch 16]: epoch loss = 2.808632,   acc = 0.817812
[epoch 16]: epoch loss = 2.808632,   acc = 0.817812
Epoch 17/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 21:33:35.676 | INFO     | model:train:251 - [epoch 17]: epoch loss = 2.711055,   acc = 0.827025
[epoch 17]: epoch loss = 2.711055,   acc = 0.827025
Epoch 18/160: 100%|██████████| 82/82 [00:08<00:00,  9.67it/s]
2025-07-02 21:33:44.222 | INFO     | model:train:251 - [epoch 18]: epoch loss = 2.628273,   acc = 0.832935
[epoch 18]: epoch loss = 2.628273,   acc = 0.832935
Epoch 19/160: 100%|██████████| 82/82 [00:08<00:00,  9.62it/s]
2025-07-02 21:33:52.811 | INFO     | model:train:251 - [epoch 19]: epoch loss = 2.870046,   acc = 0.804029
[epoch 19]: epoch loss = 2.870046,   acc = 0.804029
Epoch 20/160: 100%|██████████| 82/82 [00:08<00:00,  9.68it/s]
2025-07-02 21:34:01.348 | INFO     | model:train:251 - [epoch 20]: epoch loss = 2.515001,   acc = 0.844223
[epoch 20]: epoch loss = 2.515001,   acc = 0.844223
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([11887,  6789,  4899,  6559, 11018, 14658,  6409,  8982,   240,
        1848,  1422,  1881]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([10612,  6355,  5192,  6562, 11372, 14001,  7157,  6462,   929,
        2086,  1181,  4683]))
2025-07-02 21:34:03.844 | INFO     | model:train:267 - [epoch 20]: val loss = 4.846797,   val acc = 0.636659,   val balanced acc = 0.553971
[epoch 20]: val loss = 4.846797,   val acc = 0.636659,   val balanced acc = 0.553971
Epoch 21/160:  40%|████      | 33/82 [00:03<00:04, 10.35it/s]wandb: WARNING Tried to log to step 20 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 21/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 21:34:12.384 | INFO     | model:train:251 - [epoch 21]: epoch loss = 2.281767,   acc = 0.863785
[epoch 21]: epoch loss = 2.281767,   acc = 0.863785
Epoch 22/160: 100%|██████████| 82/82 [00:08<00:00,  9.90it/s]
2025-07-02 21:34:20.740 | INFO     | model:train:251 - [epoch 22]: epoch loss = 2.134830,   acc = 0.876072
[epoch 22]: epoch loss = 2.134830,   acc = 0.876072
Epoch 23/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 21:34:29.236 | INFO     | model:train:251 - [epoch 23]: epoch loss = 2.101871,   acc = 0.880168
[epoch 23]: epoch loss = 2.101871,   acc = 0.880168
Epoch 24/160: 100%|██████████| 82/82 [00:08<00:00,  9.79it/s]
2025-07-02 21:34:37.995 | INFO     | model:train:251 - [epoch 24]: epoch loss = 1.957106,   acc = 0.891096
[epoch 24]: epoch loss = 1.957106,   acc = 0.891096
Epoch 25/160: 100%|██████████| 82/82 [00:08<00:00,  9.95it/s]
2025-07-02 21:34:46.302 | INFO     | model:train:251 - [epoch 25]: epoch loss = 1.992003,   acc = 0.886828
[epoch 25]: epoch loss = 1.992003,   acc = 0.886828
Epoch 26/160: 100%|██████████| 82/82 [00:08<00:00, 10.17it/s]
2025-07-02 21:34:54.448 | INFO     | model:train:251 - [epoch 26]: epoch loss = 1.956903,   acc = 0.887281
[epoch 26]: epoch loss = 1.956903,   acc = 0.887281
Epoch 27/160: 100%|██████████| 82/82 [00:08<00:00, 10.12it/s]
2025-07-02 21:35:02.638 | INFO     | model:train:251 - [epoch 27]: epoch loss = 1.821951,   acc = 0.900924
[epoch 27]: epoch loss = 1.821951,   acc = 0.900924
Epoch 28/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 21:35:11.125 | INFO     | model:train:251 - [epoch 28]: epoch loss = 1.795914,   acc = 0.902189
[epoch 28]: epoch loss = 1.795914,   acc = 0.902189
Epoch 29/160: 100%|██████████| 82/82 [00:08<00:00,  9.87it/s]
2025-07-02 21:35:19.521 | INFO     | model:train:251 - [epoch 29]: epoch loss = 2.455332,   acc = 0.841809
[epoch 29]: epoch loss = 2.455332,   acc = 0.841809
Epoch 30/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 21:35:28.570 | INFO     | model:train:251 - [epoch 30]: epoch loss = 1.922167,   acc = 0.888771
[epoch 30]: epoch loss = 1.922167,   acc = 0.888771
Epoch 31/160: 100%|██████████| 82/82 [00:08<00:00,  9.87it/s]
2025-07-02 21:35:36.970 | INFO     | model:train:251 - [epoch 31]: epoch loss = 1.669203,   acc = 0.911545
[epoch 31]: epoch loss = 1.669203,   acc = 0.911545
Epoch 32/160: 100%|██████████| 82/82 [00:08<00:00, 10.07it/s]
2025-07-02 21:35:45.199 | INFO     | model:train:251 - [epoch 32]: epoch loss = 1.586128,   acc = 0.916709
[epoch 32]: epoch loss = 1.586128,   acc = 0.916709
Epoch 33/160: 100%|██████████| 82/82 [00:08<00:00,  9.82it/s]
2025-07-02 21:35:53.618 | INFO     | model:train:251 - [epoch 33]: epoch loss = 1.530140,   acc = 0.920886
[epoch 33]: epoch loss = 1.530140,   acc = 0.920886
Epoch 34/160: 100%|██████████| 82/82 [00:08<00:00,  9.93it/s]
2025-07-02 21:36:01.956 | INFO     | model:train:251 - [epoch 34]: epoch loss = 1.470806,   acc = 0.925491
[epoch 34]: epoch loss = 1.470806,   acc = 0.925491
Epoch 35/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 21:36:10.370 | INFO     | model:train:251 - [epoch 35]: epoch loss = 1.442472,   acc = 0.927063
[epoch 35]: epoch loss = 1.442472,   acc = 0.927063
Epoch 36/160: 100%|██████████| 82/82 [00:08<00:00, 10.02it/s]
2025-07-02 21:36:18.826 | INFO     | model:train:251 - [epoch 36]: epoch loss = 1.427304,   acc = 0.928291
[epoch 36]: epoch loss = 1.427304,   acc = 0.928291
Epoch 37/160: 100%|██████████| 82/82 [00:08<00:00,  9.98it/s]
2025-07-02 21:36:27.421 | INFO     | model:train:251 - [epoch 37]: epoch loss = 1.419910,   acc = 0.925868
[epoch 37]: epoch loss = 1.419910,   acc = 0.925868
Epoch 38/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 21:36:35.862 | INFO     | model:train:251 - [epoch 38]: epoch loss = 1.336681,   acc = 0.934420
[epoch 38]: epoch loss = 1.336681,   acc = 0.934420
Epoch 39/160: 100%|██████████| 82/82 [00:08<00:00,  9.71it/s]
2025-07-02 21:36:44.562 | INFO     | model:train:251 - [epoch 39]: epoch loss = 1.302544,   acc = 0.935960
[epoch 39]: epoch loss = 1.302544,   acc = 0.935960
Epoch 40/160: 100%|██████████| 82/82 [00:08<00:00,  9.82it/s]
2025-07-02 21:36:52.992 | INFO     | model:train:251 - [epoch 40]: epoch loss = 1.286833,   acc = 0.937596
[epoch 40]: epoch loss = 1.286833,   acc = 0.937596
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([15625,  5835,  5434,  6743, 11020, 12444,  5473,  7764,   393,
        2185,   935,  2741]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([10612,  6355,  5192,  6562, 11372, 14001,  7157,  6462,   929,
        2086,  1181,  4683]))
2025-07-02 21:36:53.323 | INFO     | model:train:267 - [epoch 40]: val loss = 5.864714,   val acc = 0.630196,   val balanced acc = 0.555853
[epoch 40]: val loss = 5.864714,   val acc = 0.630196,   val balanced acc = 0.555853
Epoch 41/160:  43%|████▎     | 35/82 [00:03<00:04,  9.59it/s]wandb: WARNING Tried to log to step 40 that is less than the current step 41. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 41/160: 100%|██████████| 82/82 [00:08<00:00,  9.78it/s]
2025-07-02 21:37:01.848 | INFO     | model:train:251 - [epoch 41]: epoch loss = 1.266171,   acc = 0.937820
[epoch 41]: epoch loss = 1.266171,   acc = 0.937820
Epoch 42/160: 100%|██████████| 82/82 [00:08<00:00,  9.90it/s]
2025-07-02 21:37:10.372 | INFO     | model:train:251 - [epoch 42]: epoch loss = 1.232530,   acc = 0.939134
[epoch 42]: epoch loss = 1.232530,   acc = 0.939134
Epoch 43/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 21:37:18.880 | INFO     | model:train:251 - [epoch 43]: epoch loss = 1.206912,   acc = 0.941595
[epoch 43]: epoch loss = 1.206912,   acc = 0.941595
Epoch 44/160: 100%|██████████| 82/82 [00:08<00:00,  9.91it/s]
2025-07-02 21:37:27.237 | INFO     | model:train:251 - [epoch 44]: epoch loss = 1.316874,   acc = 0.931244
[epoch 44]: epoch loss = 1.316874,   acc = 0.931244
Epoch 45/160: 100%|██████████| 82/82 [00:08<00:00, 10.18it/s]
2025-07-02 21:37:36.432 | INFO     | model:train:251 - [epoch 45]: epoch loss = 1.273247,   acc = 0.934849
[epoch 45]: epoch loss = 1.273247,   acc = 0.934849
Epoch 46/160: 100%|██████████| 82/82 [00:08<00:00, 10.00it/s]
2025-07-02 21:37:44.703 | INFO     | model:train:251 - [epoch 46]: epoch loss = 2.073094,   acc = 0.867330
[epoch 46]: epoch loss = 2.073094,   acc = 0.867330
Epoch 47/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 21:37:53.162 | INFO     | model:train:251 - [epoch 47]: epoch loss = 1.719434,   acc = 0.897427
[epoch 47]: epoch loss = 1.719434,   acc = 0.897427
Epoch 48/160: 100%|██████████| 82/82 [00:08<00:00,  9.98it/s]
2025-07-02 21:38:01.414 | INFO     | model:train:251 - [epoch 48]: epoch loss = 1.255170,   acc = 0.938378
[epoch 48]: epoch loss = 1.255170,   acc = 0.938378
Epoch 49/160: 100%|██████████| 82/82 [00:08<00:00, 10.05it/s]
2025-07-02 21:38:09.763 | INFO     | model:train:251 - [epoch 49]: epoch loss = 1.123966,   acc = 0.948885
[epoch 49]: epoch loss = 1.123966,   acc = 0.948885
Epoch 50/160: 100%|██████████| 82/82 [00:08<00:00, 10.01it/s]
2025-07-02 21:38:18.024 | INFO     | model:train:251 - [epoch 50]: epoch loss = 1.073125,   acc = 0.952188
[epoch 50]: epoch loss = 1.073125,   acc = 0.952188
Epoch 51/160: 100%|██████████| 82/82 [00:08<00:00, 10.06it/s]
2025-07-02 21:38:27.103 | INFO     | model:train:251 - [epoch 51]: epoch loss = 1.048193,   acc = 0.953067
[epoch 51]: epoch loss = 1.048193,   acc = 0.953067
Epoch 52/160: 100%|██████████| 82/82 [00:08<00:00,  9.90it/s]
2025-07-02 21:38:35.684 | INFO     | model:train:251 - [epoch 52]: epoch loss = 1.026302,   acc = 0.954130
[epoch 52]: epoch loss = 1.026302,   acc = 0.954130
Epoch 53/160: 100%|██████████| 82/82 [00:08<00:00,  9.85it/s]
2025-07-02 21:38:44.071 | INFO     | model:train:251 - [epoch 53]: epoch loss = 0.997893,   acc = 0.956681
[epoch 53]: epoch loss = 0.997893,   acc = 0.956681
Epoch 54/160: 100%|██████████| 82/82 [00:08<00:00,  9.75it/s]
2025-07-02 21:38:52.562 | INFO     | model:train:251 - [epoch 54]: epoch loss = 0.966337,   acc = 0.958558
[epoch 54]: epoch loss = 0.966337,   acc = 0.958558
Epoch 55/160: 100%|██████████| 82/82 [00:08<00:00,  9.85it/s]
2025-07-02 21:39:01.090 | INFO     | model:train:251 - [epoch 55]: epoch loss = 0.949732,   acc = 0.959185
[epoch 55]: epoch loss = 0.949732,   acc = 0.959185
Epoch 56/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 21:39:09.667 | INFO     | model:train:251 - [epoch 56]: epoch loss = 0.943045,   acc = 0.958946
[epoch 56]: epoch loss = 0.943045,   acc = 0.958946
Epoch 57/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 21:39:17.995 | INFO     | model:train:251 - [epoch 57]: epoch loss = 0.933120,   acc = 0.959096
[epoch 57]: epoch loss = 0.933120,   acc = 0.959096
Epoch 58/160: 100%|██████████| 82/82 [00:08<00:00,  9.98it/s]
2025-07-02 21:39:26.250 | INFO     | model:train:251 - [epoch 58]: epoch loss = 0.914876,   acc = 0.960173
[epoch 58]: epoch loss = 0.914876,   acc = 0.960173
Epoch 59/160: 100%|██████████| 82/82 [00:08<00:00,  9.95it/s]
2025-07-02 21:39:34.558 | INFO     | model:train:251 - [epoch 59]: epoch loss = 0.896675,   acc = 0.961033
[epoch 59]: epoch loss = 0.896675,   acc = 0.961033
Epoch 60/160: 100%|██████████| 82/82 [00:08<00:00,  9.87it/s]
2025-07-02 21:39:42.932 | INFO     | model:train:251 - [epoch 60]: epoch loss = 0.885743,   acc = 0.961560
[epoch 60]: epoch loss = 0.885743,   acc = 0.961560
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([12953,  6974,  4510,  8059,  9430, 14711,  6128,  7111,   244,
        2652,  1166,  2654]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([10612,  6355,  5192,  6562, 11372, 14001,  7157,  6462,   929,
        2086,  1181,  4683]))
2025-07-02 21:39:43.242 | INFO     | model:train:267 - [epoch 60]: val loss = 6.057500,   val acc = 0.653358,   val balanced acc = 0.601991
[epoch 60]: val loss = 6.057500,   val acc = 0.653358,   val balanced acc = 0.601991
Epoch 61/160:  50%|█████     | 41/82 [00:03<00:04,  8.57it/s]wandb: WARNING Tried to log to step 60 that is less than the current step 61. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 61/160: 100%|██████████| 82/82 [00:08<00:00,  9.93it/s]
2025-07-02 21:39:51.608 | INFO     | model:train:251 - [epoch 61]: epoch loss = 0.895194,   acc = 0.959746
[epoch 61]: epoch loss = 0.895194,   acc = 0.959746
Epoch 62/160: 100%|██████████| 82/82 [00:08<00:00,  9.97it/s]
2025-07-02 21:39:59.900 | INFO     | model:train:251 - [epoch 62]: epoch loss = 0.922459,   acc = 0.957747
[epoch 62]: epoch loss = 0.922459,   acc = 0.957747
Epoch 63/160: 100%|██████████| 82/82 [00:08<00:00,  9.85it/s]
2025-07-02 21:40:08.290 | INFO     | model:train:251 - [epoch 63]: epoch loss = 0.884521,   acc = 0.960021
[epoch 63]: epoch loss = 0.884521,   acc = 0.960021
Epoch 64/160: 100%|██████████| 82/82 [00:08<00:00,  9.96it/s]
2025-07-02 21:40:16.589 | INFO     | model:train:251 - [epoch 64]: epoch loss = 0.866401,   acc = 0.961059
[epoch 64]: epoch loss = 0.866401,   acc = 0.961059
Epoch 65/160: 100%|██████████| 82/82 [00:08<00:00,  9.65it/s]
2025-07-02 21:40:25.149 | INFO     | model:train:251 - [epoch 65]: epoch loss = 0.838131,   acc = 0.963303
[epoch 65]: epoch loss = 0.838131,   acc = 0.963303
Epoch 66/160: 100%|██████████| 82/82 [00:08<00:00,  9.86it/s]
2025-07-02 21:40:33.537 | INFO     | model:train:251 - [epoch 66]: epoch loss = 0.820520,   acc = 0.964400
[epoch 66]: epoch loss = 0.820520,   acc = 0.964400
Epoch 67/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 21:40:41.899 | INFO     | model:train:251 - [epoch 67]: epoch loss = 0.802767,   acc = 0.965823
[epoch 67]: epoch loss = 0.802767,   acc = 0.965823
Epoch 68/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 21:40:50.313 | INFO     | model:train:251 - [epoch 68]: epoch loss = 0.795117,   acc = 0.965712
[epoch 68]: epoch loss = 0.795117,   acc = 0.965712
Epoch 69/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 21:40:58.778 | INFO     | model:train:251 - [epoch 69]: epoch loss = 0.801381,   acc = 0.964867
[epoch 69]: epoch loss = 0.801381,   acc = 0.964867
Epoch 70/160: 100%|██████████| 82/82 [00:08<00:00,  9.97it/s]
2025-07-02 21:41:07.069 | INFO     | model:train:251 - [epoch 70]: epoch loss = 0.796311,   acc = 0.964869
[epoch 70]: epoch loss = 0.796311,   acc = 0.964869
Epoch 71/160: 100%|██████████| 82/82 [00:08<00:00,  9.79it/s]
2025-07-02 21:41:15.514 | INFO     | model:train:251 - [epoch 71]: epoch loss = 0.778733,   acc = 0.966135
[epoch 71]: epoch loss = 0.778733,   acc = 0.966135
Epoch 72/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 21:41:23.864 | INFO     | model:train:251 - [epoch 72]: epoch loss = 0.750745,   acc = 0.968262
[epoch 72]: epoch loss = 0.750745,   acc = 0.968262
Epoch 73/160: 100%|██████████| 82/82 [00:08<00:00, 10.04it/s]
2025-07-02 21:41:32.091 | INFO     | model:train:251 - [epoch 73]: epoch loss = 0.762892,   acc = 0.966319
[epoch 73]: epoch loss = 0.762892,   acc = 0.966319
Epoch 74/160: 100%|██████████| 82/82 [00:08<00:00,  9.64it/s]
2025-07-02 21:41:40.669 | INFO     | model:train:251 - [epoch 74]: epoch loss = 0.777861,   acc = 0.965712
[epoch 74]: epoch loss = 0.777861,   acc = 0.965712
Epoch 75/160: 100%|██████████| 82/82 [00:08<00:00,  9.75it/s]
2025-07-02 21:41:49.305 | INFO     | model:train:251 - [epoch 75]: epoch loss = 0.761157,   acc = 0.966361
[epoch 75]: epoch loss = 0.761157,   acc = 0.966361
Epoch 76/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 21:41:57.655 | INFO     | model:train:251 - [epoch 76]: epoch loss = 0.730472,   acc = 0.968467
[epoch 76]: epoch loss = 0.730472,   acc = 0.968467
Epoch 77/160: 100%|██████████| 82/82 [00:08<00:00, 10.01it/s]
2025-07-02 21:42:05.916 | INFO     | model:train:251 - [epoch 77]: epoch loss = 0.720916,   acc = 0.968903
[epoch 77]: epoch loss = 0.720916,   acc = 0.968903
Epoch 78/160: 100%|██████████| 82/82 [00:08<00:00,  9.97it/s]
2025-07-02 21:42:14.215 | INFO     | model:train:251 - [epoch 78]: epoch loss = 0.704930,   acc = 0.970293
[epoch 78]: epoch loss = 0.704930,   acc = 0.970293
Epoch 79/160: 100%|██████████| 82/82 [00:08<00:00, 10.04it/s]
2025-07-02 21:42:22.442 | INFO     | model:train:251 - [epoch 79]: epoch loss = 0.692370,   acc = 0.970804
[epoch 79]: epoch loss = 0.692370,   acc = 0.970804
Epoch 80/160: 100%|██████████| 82/82 [00:08<00:00,  9.90it/s]
2025-07-02 21:42:30.783 | INFO     | model:train:251 - [epoch 80]: epoch loss = 0.682084,   acc = 0.971667
[epoch 80]: epoch loss = 0.682084,   acc = 0.971667
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([13077,  6337,  4714,  6051, 13019, 13119,  7333,  7062,   132,
        2140,   956,  2652]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([10612,  6355,  5192,  6562, 11372, 14001,  7157,  6462,   929,
        2086,  1181,  4683]))
2025-07-02 21:42:31.078 | INFO     | model:train:267 - [epoch 80]: val loss = 6.959554,   val acc = 0.631776,   val balanced acc = 0.560083
[epoch 80]: val loss = 6.959554,   val acc = 0.631776,   val balanced acc = 0.560083
2025-07-02 21:42:31.114 | INFO     | model:train:287 - EarlyStopping counter: 1 out of 3
EarlyStopping counter: 1 out of 3
Epoch 81/160:  76%|███████▌  | 62/82 [00:06<00:01, 10.76it/s]wandb: WARNING Tried to log to step 80 that is less than the current step 81. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 81/160: 100%|██████████| 82/82 [00:08<00:00,  9.98it/s]
2025-07-02 21:42:39.386 | INFO     | model:train:251 - [epoch 81]: epoch loss = 0.668168,   acc = 0.972183
[epoch 81]: epoch loss = 0.668168,   acc = 0.972183
Epoch 82/160: 100%|██████████| 82/82 [00:08<00:00,  9.84it/s]
2025-07-02 21:42:47.779 | INFO     | model:train:251 - [epoch 82]: epoch loss = 0.664288,   acc = 0.972758
[epoch 82]: epoch loss = 0.664288,   acc = 0.972758
Epoch 83/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 21:42:56.239 | INFO     | model:train:251 - [epoch 83]: epoch loss = 0.663311,   acc = 0.972245
[epoch 83]: epoch loss = 0.663311,   acc = 0.972245
Epoch 84/160: 100%|██████████| 82/82 [00:08<00:00,  9.84it/s]
2025-07-02 21:43:04.635 | INFO     | model:train:251 - [epoch 84]: epoch loss = 0.654175,   acc = 0.972857
[epoch 84]: epoch loss = 0.654175,   acc = 0.972857
Epoch 85/160: 100%|██████████| 82/82 [00:08<00:00,  9.80it/s]
2025-07-02 21:43:13.059 | INFO     | model:train:251 - [epoch 85]: epoch loss = 0.685464,   acc = 0.969945
[epoch 85]: epoch loss = 0.685464,   acc = 0.969945
Epoch 86/160: 100%|██████████| 82/82 [00:08<00:00, 10.01it/s]
2025-07-02 21:43:21.325 | INFO     | model:train:251 - [epoch 86]: epoch loss = 0.904157,   acc = 0.951787
[epoch 86]: epoch loss = 0.904157,   acc = 0.951787
Epoch 87/160: 100%|██████████| 82/82 [00:08<00:00,  9.75it/s]
2025-07-02 21:43:29.800 | INFO     | model:train:251 - [epoch 87]: epoch loss = 0.737954,   acc = 0.965025
[epoch 87]: epoch loss = 0.737954,   acc = 0.965025
Epoch 88/160: 100%|██████████| 82/82 [00:08<00:00,  9.98it/s]
2025-07-02 21:43:38.086 | INFO     | model:train:251 - [epoch 88]: epoch loss = 0.645991,   acc = 0.973548
[epoch 88]: epoch loss = 0.645991,   acc = 0.973548
Epoch 89/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 21:43:46.493 | INFO     | model:train:251 - [epoch 89]: epoch loss = 0.626655,   acc = 0.974736
[epoch 89]: epoch loss = 0.626655,   acc = 0.974736
Epoch 90/160: 100%|██████████| 82/82 [00:08<00:00,  9.87it/s]
2025-07-02 21:43:54.869 | INFO     | model:train:251 - [epoch 90]: epoch loss = 0.616283,   acc = 0.975460
[epoch 90]: epoch loss = 0.616283,   acc = 0.975460
Epoch 91/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 21:44:03.339 | INFO     | model:train:251 - [epoch 91]: epoch loss = 0.615151,   acc = 0.975171
[epoch 91]: epoch loss = 0.615151,   acc = 0.975171
Epoch 92/160: 100%|██████████| 82/82 [00:08<00:00,  9.96it/s]
2025-07-02 21:44:11.635 | INFO     | model:train:251 - [epoch 92]: epoch loss = 0.611285,   acc = 0.975082
[epoch 92]: epoch loss = 0.611285,   acc = 0.975082
Epoch 93/160: 100%|██████████| 82/82 [00:08<00:00,  9.98it/s]
2025-07-02 21:44:19.911 | INFO     | model:train:251 - [epoch 93]: epoch loss = 0.598286,   acc = 0.976127
[epoch 93]: epoch loss = 0.598286,   acc = 0.976127
Epoch 94/160: 100%|██████████| 82/82 [00:08<00:00,  9.85it/s]
2025-07-02 21:44:28.302 | INFO     | model:train:251 - [epoch 94]: epoch loss = 0.587391,   acc = 0.976958
[epoch 94]: epoch loss = 0.587391,   acc = 0.976958
Epoch 95/160: 100%|██████████| 82/82 [00:08<00:00,  9.96it/s]
2025-07-02 21:44:36.598 | INFO     | model:train:251 - [epoch 95]: epoch loss = 0.586811,   acc = 0.976606
[epoch 95]: epoch loss = 0.586811,   acc = 0.976606
Epoch 96/160: 100%|██████████| 82/82 [00:08<00:00,  9.91it/s]
2025-07-02 21:44:44.932 | INFO     | model:train:251 - [epoch 96]: epoch loss = 0.578469,   acc = 0.977306
[epoch 96]: epoch loss = 0.578469,   acc = 0.977306
Epoch 97/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 21:44:53.307 | INFO     | model:train:251 - [epoch 97]: epoch loss = 0.573959,   acc = 0.977802
[epoch 97]: epoch loss = 0.573959,   acc = 0.977802
Epoch 98/160: 100%|██████████| 82/82 [00:08<00:00,  9.91it/s]
2025-07-02 21:45:01.638 | INFO     | model:train:251 - [epoch 98]: epoch loss = 0.573680,   acc = 0.977252
[epoch 98]: epoch loss = 0.573680,   acc = 0.977252
Epoch 99/160: 100%|██████████| 82/82 [00:08<00:00,  9.69it/s]
2025-07-02 21:45:10.169 | INFO     | model:train:251 - [epoch 99]: epoch loss = 0.566739,   acc = 0.977508
[epoch 99]: epoch loss = 0.566739,   acc = 0.977508
Epoch 100/160: 100%|██████████| 82/82 [00:08<00:00,  9.39it/s]
2025-07-02 21:45:18.979 | INFO     | model:train:251 - [epoch 100]: epoch loss = 0.558450,   acc = 0.978003
[epoch 100]: epoch loss = 0.558450,   acc = 0.978003
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([15170,  6311,  4238,  5847, 11744, 13797,  6446,  7044,   176,
        2213,   879,  2727]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([10612,  6355,  5192,  6562, 11372, 14001,  7157,  6462,   929,
        2086,  1181,  4683]))
2025-07-02 21:45:19.303 | INFO     | model:train:267 - [epoch 100]: val loss = 7.225200,   val acc = 0.631241,   val balanced acc = 0.557618
[epoch 100]: val loss = 7.225200,   val acc = 0.631241,   val balanced acc = 0.557618
2025-07-02 21:45:19.338 | INFO     | model:train:287 - EarlyStopping counter: 2 out of 3
EarlyStopping counter: 2 out of 3
Epoch 101/160:  94%|█████████▍| 77/82 [00:07<00:00,  9.92it/s]wandb: WARNING Tried to log to step 100 that is less than the current step 101. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 101/160: 100%|██████████| 82/82 [00:08<00:00,  9.79it/s]
2025-07-02 21:45:27.887 | INFO     | model:train:251 - [epoch 101]: epoch loss = 0.557422,   acc = 0.978080
[epoch 101]: epoch loss = 0.557422,   acc = 0.978080
Epoch 102/160: 100%|██████████| 82/82 [00:08<00:00,  9.59it/s]
2025-07-02 21:45:36.515 | INFO     | model:train:251 - [epoch 102]: epoch loss = 0.549600,   acc = 0.978629
[epoch 102]: epoch loss = 0.549600,   acc = 0.978629
Epoch 103/160: 100%|██████████| 82/82 [00:08<00:00,  9.81it/s]
2025-07-02 21:45:44.942 | INFO     | model:train:251 - [epoch 103]: epoch loss = 0.545818,   acc = 0.978848
[epoch 103]: epoch loss = 0.545818,   acc = 0.978848
Epoch 104/160: 100%|██████████| 82/82 [00:08<00:00,  9.87it/s]
2025-07-02 21:45:53.337 | INFO     | model:train:251 - [epoch 104]: epoch loss = 0.539955,   acc = 0.979287
[epoch 104]: epoch loss = 0.539955,   acc = 0.979287
Epoch 105/160: 100%|██████████| 82/82 [00:08<00:00,  9.72it/s]
2025-07-02 21:46:01.850 | INFO     | model:train:251 - [epoch 105]: epoch loss = 0.540549,   acc = 0.978823
[epoch 105]: epoch loss = 0.540549,   acc = 0.978823
Epoch 106/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 21:46:10.536 | INFO     | model:train:251 - [epoch 106]: epoch loss = 0.535397,   acc = 0.979124
[epoch 106]: epoch loss = 0.535397,   acc = 0.979124
Epoch 107/160: 100%|██████████| 82/82 [00:08<00:00,  9.73it/s]
2025-07-02 21:46:19.056 | INFO     | model:train:251 - [epoch 107]: epoch loss = 0.537184,   acc = 0.978894
[epoch 107]: epoch loss = 0.537184,   acc = 0.978894
Epoch 108/160: 100%|██████████| 82/82 [00:08<00:00,  9.98it/s]
2025-07-02 21:46:27.385 | INFO     | model:train:251 - [epoch 108]: epoch loss = 0.526372,   acc = 0.979480
[epoch 108]: epoch loss = 0.526372,   acc = 0.979480
Epoch 109/160: 100%|██████████| 82/82 [00:08<00:00,  9.61it/s]
2025-07-02 21:46:36.009 | INFO     | model:train:251 - [epoch 109]: epoch loss = 0.524252,   acc = 0.979887
[epoch 109]: epoch loss = 0.524252,   acc = 0.979887
Epoch 110/160: 100%|██████████| 82/82 [00:08<00:00,  9.87it/s]
2025-07-02 21:46:45.227 | INFO     | model:train:251 - [epoch 110]: epoch loss = 0.521605,   acc = 0.979769
[epoch 110]: epoch loss = 0.521605,   acc = 0.979769
Epoch 111/160: 100%|██████████| 82/82 [00:08<00:00,  9.86it/s]
2025-07-02 21:46:53.622 | INFO     | model:train:251 - [epoch 111]: epoch loss = 0.518728,   acc = 0.979859
[epoch 111]: epoch loss = 0.518728,   acc = 0.979859
Epoch 112/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 21:47:02.029 | INFO     | model:train:251 - [epoch 112]: epoch loss = 0.517275,   acc = 0.979649
[epoch 112]: epoch loss = 0.517275,   acc = 0.979649
Epoch 113/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 21:47:10.516 | INFO     | model:train:251 - [epoch 113]: epoch loss = 0.513118,   acc = 0.979999
[epoch 113]: epoch loss = 0.513118,   acc = 0.979999
Epoch 114/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 21:47:18.994 | INFO     | model:train:251 - [epoch 114]: epoch loss = 0.506896,   acc = 0.980661
[epoch 114]: epoch loss = 0.506896,   acc = 0.980661
Epoch 115/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 21:47:27.416 | INFO     | model:train:251 - [epoch 115]: epoch loss = 0.506163,   acc = 0.980628
[epoch 115]: epoch loss = 0.506163,   acc = 0.980628
Epoch 116/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 21:47:35.919 | INFO     | model:train:251 - [epoch 116]: epoch loss = 0.509561,   acc = 0.980071
[epoch 116]: epoch loss = 0.509561,   acc = 0.980071
Epoch 117/160: 100%|██████████| 82/82 [00:08<00:00,  9.75it/s]
2025-07-02 21:47:44.410 | INFO     | model:train:251 - [epoch 117]: epoch loss = 0.502410,   acc = 0.980586
[epoch 117]: epoch loss = 0.502410,   acc = 0.980586
Epoch 118/160: 100%|██████████| 82/82 [00:08<00:00,  9.84it/s]
2025-07-02 21:47:52.818 | INFO     | model:train:251 - [epoch 118]: epoch loss = 0.501076,   acc = 0.980644
[epoch 118]: epoch loss = 0.501076,   acc = 0.980644
Epoch 119/160: 100%|██████████| 82/82 [00:08<00:00,  9.92it/s]
2025-07-02 21:48:01.160 | INFO     | model:train:251 - [epoch 119]: epoch loss = 0.498234,   acc = 0.980790
[epoch 119]: epoch loss = 0.498234,   acc = 0.980790
Epoch 120/160: 100%|██████████| 82/82 [00:08<00:00,  9.69it/s]
2025-07-02 21:48:09.698 | INFO     | model:train:251 - [epoch 120]: epoch loss = 0.493080,   acc = 0.981172
[epoch 120]: epoch loss = 0.493080,   acc = 0.981172
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([13852,  6020,  4935,  5971, 12093, 14147,  6891,  7094,   181,
        2106,   978,  2324]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([10612,  6355,  5192,  6562, 11372, 14001,  7157,  6462,   929,
        2086,  1181,  4683]))
2025-07-02 21:48:10.038 | INFO     | model:train:267 - [epoch 120]: val loss = 7.416486,   val acc = 0.636424,   val balanced acc = 0.566458
[epoch 120]: val loss = 7.416486,   val acc = 0.636424,   val balanced acc = 0.566458
2025-07-02 21:48:10.073 | INFO     | model:train:287 - EarlyStopping counter: 3 out of 3
EarlyStopping counter: 3 out of 3
2025-07-02 21:48:10.073 | INFO     | model:train:289 - Early stopping triggered.
Early stopping triggered.
