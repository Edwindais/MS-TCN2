Epoch 1/160: 100%|██████████| 82/82 [00:12<00:00,  6.75it/s]
2025-07-02 21:48:29.049 | INFO     | model:train:251 - [epoch 1]: epoch loss = 9.226954,   acc = 0.178607
[epoch 1]: epoch loss = 9.226954,   acc = 0.178607
Epoch 2/160: 100%|██████████| 82/82 [00:08<00:00,  9.50it/s]
2025-07-02 21:48:37.721 | INFO     | model:train:251 - [epoch 2]: epoch loss = 8.387094,   acc = 0.214899
[epoch 2]: epoch loss = 8.387094,   acc = 0.214899
Epoch 3/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 21:48:46.182 | INFO     | model:train:251 - [epoch 3]: epoch loss = 7.050625,   acc = 0.352605
[epoch 3]: epoch loss = 7.050625,   acc = 0.352605
Epoch 4/160: 100%|██████████| 82/82 [00:08<00:00,  9.49it/s]
2025-07-02 21:48:55.293 | INFO     | model:train:251 - [epoch 4]: epoch loss = 5.886983,   acc = 0.527771
[epoch 4]: epoch loss = 5.886983,   acc = 0.527771
Epoch 5/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 21:49:03.713 | INFO     | model:train:251 - [epoch 5]: epoch loss = 5.292231,   acc = 0.591316
[epoch 5]: epoch loss = 5.292231,   acc = 0.591316
Epoch 6/160: 100%|██████████| 82/82 [00:08<00:00,  9.40it/s]
2025-07-02 21:49:12.515 | INFO     | model:train:251 - [epoch 6]: epoch loss = 4.753823,   acc = 0.647011
[epoch 6]: epoch loss = 4.753823,   acc = 0.647011
Epoch 7/160: 100%|██████████| 82/82 [00:08<00:00,  9.64it/s]
2025-07-02 21:49:21.079 | INFO     | model:train:251 - [epoch 7]: epoch loss = 4.409953,   acc = 0.680457
[epoch 7]: epoch loss = 4.409953,   acc = 0.680457
Epoch 8/160: 100%|██████████| 82/82 [00:08<00:00,  9.48it/s]
2025-07-02 21:49:29.827 | INFO     | model:train:251 - [epoch 8]: epoch loss = 4.248533,   acc = 0.692674
[epoch 8]: epoch loss = 4.248533,   acc = 0.692674
Epoch 9/160: 100%|██████████| 82/82 [00:08<00:00,  9.92it/s]
2025-07-02 21:49:38.172 | INFO     | model:train:251 - [epoch 9]: epoch loss = 4.062911,   acc = 0.712199
[epoch 9]: epoch loss = 4.062911,   acc = 0.712199
Epoch 10/160: 100%|██████████| 82/82 [00:08<00:00,  9.94it/s]
2025-07-02 21:49:46.489 | INFO     | model:train:251 - [epoch 10]: epoch loss = 3.765240,   acc = 0.735020
[epoch 10]: epoch loss = 3.765240,   acc = 0.735020
Epoch 11/160: 100%|██████████| 82/82 [00:08<00:00,  9.93it/s]
2025-07-02 21:49:54.876 | INFO     | model:train:251 - [epoch 11]: epoch loss = 3.657592,   acc = 0.743302
[epoch 11]: epoch loss = 3.657592,   acc = 0.743302
Epoch 12/160: 100%|██████████| 82/82 [00:08<00:00,  9.90it/s]
2025-07-02 21:50:03.234 | INFO     | model:train:251 - [epoch 12]: epoch loss = 3.314489,   acc = 0.773943
[epoch 12]: epoch loss = 3.314489,   acc = 0.773943
Epoch 13/160: 100%|██████████| 82/82 [00:08<00:00,  9.94it/s]
2025-07-02 21:50:11.581 | INFO     | model:train:251 - [epoch 13]: epoch loss = 3.169913,   acc = 0.788896
[epoch 13]: epoch loss = 3.169913,   acc = 0.788896
Epoch 14/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 21:50:19.943 | INFO     | model:train:251 - [epoch 14]: epoch loss = 3.043998,   acc = 0.795324
[epoch 14]: epoch loss = 3.043998,   acc = 0.795324
Epoch 15/160: 100%|██████████| 82/82 [00:08<00:00,  9.92it/s]
2025-07-02 21:50:28.271 | INFO     | model:train:251 - [epoch 15]: epoch loss = 2.881785,   acc = 0.813752
[epoch 15]: epoch loss = 2.881785,   acc = 0.813752
Epoch 16/160: 100%|██████████| 82/82 [00:08<00:00,  9.97it/s]
2025-07-02 21:50:36.867 | INFO     | model:train:251 - [epoch 16]: epoch loss = 2.788706,   acc = 0.823801
[epoch 16]: epoch loss = 2.788706,   acc = 0.823801
Epoch 17/160: 100%|██████████| 82/82 [00:08<00:00,  9.81it/s]
2025-07-02 21:50:45.305 | INFO     | model:train:251 - [epoch 17]: epoch loss = 3.114827,   acc = 0.789346
[epoch 17]: epoch loss = 3.114827,   acc = 0.789346
Epoch 18/160: 100%|██████████| 82/82 [00:08<00:00,  9.61it/s]
2025-07-02 21:50:53.917 | INFO     | model:train:251 - [epoch 18]: epoch loss = 2.780429,   acc = 0.819822
[epoch 18]: epoch loss = 2.780429,   acc = 0.819822
Epoch 19/160: 100%|██████████| 82/82 [00:08<00:00,  9.66it/s]
2025-07-02 21:51:02.728 | INFO     | model:train:251 - [epoch 19]: epoch loss = 2.458243,   acc = 0.851193
[epoch 19]: epoch loss = 2.458243,   acc = 0.851193
Epoch 20/160: 100%|██████████| 82/82 [00:08<00:00,  9.66it/s]
2025-07-02 21:51:11.301 | INFO     | model:train:251 - [epoch 20]: epoch loss = 2.346397,   acc = 0.859950
[epoch 20]: epoch loss = 2.346397,   acc = 0.859950
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5062,  3865,  4983,  7446, 11828, 10137,  8218,  8470,   910,
        1254,  1474,  2989]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5317,  5480,  4998,  8104, 11654, 10208,  5049,  6591,  1572,
        1590,  2900,  3173]))
2025-07-02 21:51:11.601 | INFO     | model:train:267 - [epoch 20]: val loss = 5.986669,   val acc = 0.600066,   val balanced acc = 0.537551
[epoch 20]: val loss = 5.986669,   val acc = 0.600066,   val balanced acc = 0.537551
Epoch 21/160:  54%|█████▎    | 44/82 [00:04<00:03, 10.64it/s]wandb: WARNING Tried to log to step 20 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 21/160: 100%|██████████| 82/82 [00:08<00:00,  9.50it/s]
2025-07-02 21:51:20.373 | INFO     | model:train:251 - [epoch 21]: epoch loss = 2.249209,   acc = 0.867457
[epoch 21]: epoch loss = 2.249209,   acc = 0.867457
Epoch 22/160: 100%|██████████| 82/82 [00:08<00:00,  9.51it/s]
2025-07-02 21:51:29.064 | INFO     | model:train:251 - [epoch 22]: epoch loss = 2.136927,   acc = 0.875980
[epoch 22]: epoch loss = 2.136927,   acc = 0.875980
Epoch 23/160: 100%|██████████| 82/82 [00:08<00:00,  9.85it/s]
2025-07-02 21:51:37.458 | INFO     | model:train:251 - [epoch 23]: epoch loss = 1.981768,   acc = 0.891075
[epoch 23]: epoch loss = 1.981768,   acc = 0.891075
Epoch 24/160: 100%|██████████| 82/82 [00:08<00:00,  9.50it/s]
2025-07-02 21:51:46.185 | INFO     | model:train:251 - [epoch 24]: epoch loss = 1.990536,   acc = 0.888093
[epoch 24]: epoch loss = 1.990536,   acc = 0.888093
Epoch 25/160: 100%|██████████| 82/82 [00:08<00:00,  9.51it/s]
2025-07-02 21:51:54.871 | INFO     | model:train:251 - [epoch 25]: epoch loss = 1.960154,   acc = 0.889486
[epoch 25]: epoch loss = 1.960154,   acc = 0.889486
Epoch 26/160: 100%|██████████| 82/82 [00:08<00:00,  9.42it/s]
2025-07-02 21:52:04.469 | INFO     | model:train:251 - [epoch 26]: epoch loss = 1.943316,   acc = 0.890254
[epoch 26]: epoch loss = 1.943316,   acc = 0.890254
Epoch 27/160: 100%|██████████| 82/82 [00:08<00:00,  9.60it/s]
2025-07-02 21:52:13.089 | INFO     | model:train:251 - [epoch 27]: epoch loss = 1.877849,   acc = 0.894475
[epoch 27]: epoch loss = 1.877849,   acc = 0.894475
Epoch 28/160: 100%|██████████| 82/82 [00:08<00:00,  9.73it/s]
2025-07-02 21:52:21.587 | INFO     | model:train:251 - [epoch 28]: epoch loss = 1.786583,   acc = 0.901664
[epoch 28]: epoch loss = 1.786583,   acc = 0.901664
Epoch 29/160: 100%|██████████| 82/82 [00:08<00:00,  9.94it/s]
2025-07-02 21:52:29.902 | INFO     | model:train:251 - [epoch 29]: epoch loss = 2.034090,   acc = 0.877733
[epoch 29]: epoch loss = 2.034090,   acc = 0.877733
Epoch 30/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 21:52:38.388 | INFO     | model:train:251 - [epoch 30]: epoch loss = 2.225729,   acc = 0.862743
[epoch 30]: epoch loss = 2.225729,   acc = 0.862743
Epoch 31/160: 100%|██████████| 82/82 [00:08<00:00,  9.64it/s]
2025-07-02 21:52:46.961 | INFO     | model:train:251 - [epoch 31]: epoch loss = 1.762837,   acc = 0.902302
[epoch 31]: epoch loss = 1.762837,   acc = 0.902302
Epoch 32/160: 100%|██████████| 82/82 [00:08<00:00,  9.80it/s]
2025-07-02 21:52:55.392 | INFO     | model:train:251 - [epoch 32]: epoch loss = 1.563129,   acc = 0.920468
[epoch 32]: epoch loss = 1.563129,   acc = 0.920468
Epoch 33/160: 100%|██████████| 82/82 [00:08<00:00, 10.00it/s]
2025-07-02 21:53:03.662 | INFO     | model:train:251 - [epoch 33]: epoch loss = 1.515936,   acc = 0.922914
[epoch 33]: epoch loss = 1.515936,   acc = 0.922914
Epoch 34/160: 100%|██████████| 82/82 [00:08<00:00,  9.56it/s]
2025-07-02 21:53:12.318 | INFO     | model:train:251 - [epoch 34]: epoch loss = 1.481259,   acc = 0.925833
[epoch 34]: epoch loss = 1.481259,   acc = 0.925833
Epoch 35/160: 100%|██████████| 82/82 [00:08<00:00,  9.97it/s]
2025-07-02 21:53:20.613 | INFO     | model:train:251 - [epoch 35]: epoch loss = 1.483700,   acc = 0.923015
[epoch 35]: epoch loss = 1.483700,   acc = 0.923015
Epoch 36/160: 100%|██████████| 82/82 [00:08<00:00,  9.80it/s]
2025-07-02 21:53:29.039 | INFO     | model:train:251 - [epoch 36]: epoch loss = 1.463646,   acc = 0.924671
[epoch 36]: epoch loss = 1.463646,   acc = 0.924671
Epoch 37/160: 100%|██████████| 82/82 [00:08<00:00,  9.90it/s]
2025-07-02 21:53:37.385 | INFO     | model:train:251 - [epoch 37]: epoch loss = 1.379950,   acc = 0.932022
[epoch 37]: epoch loss = 1.379950,   acc = 0.932022
Epoch 38/160: 100%|██████████| 82/82 [00:08<00:00,  9.60it/s]
2025-07-02 21:53:46.023 | INFO     | model:train:251 - [epoch 38]: epoch loss = 1.344593,   acc = 0.933533
[epoch 38]: epoch loss = 1.344593,   acc = 0.933533
Epoch 39/160: 100%|██████████| 82/82 [00:08<00:00,  9.40it/s]
2025-07-02 21:53:54.814 | INFO     | model:train:251 - [epoch 39]: epoch loss = 1.330589,   acc = 0.933855
[epoch 39]: epoch loss = 1.330589,   acc = 0.933855
Epoch 40/160: 100%|██████████| 82/82 [00:08<00:00,  9.61it/s]
2025-07-02 21:54:03.413 | INFO     | model:train:251 - [epoch 40]: epoch loss = 1.282342,   acc = 0.936850
[epoch 40]: epoch loss = 1.282342,   acc = 0.936850
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 6680,  2757,  5142,  6231, 13697, 10592,  8958,  6051,   705,
        1889,   947,  2987]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5317,  5480,  4998,  8104, 11654, 10208,  5049,  6591,  1572,
        1590,  2900,  3173]))
2025-07-02 21:54:03.706 | INFO     | model:train:267 - [epoch 40]: val loss = 6.754773,   val acc = 0.595069,   val balanced acc = 0.534253
[epoch 40]: val loss = 6.754773,   val acc = 0.595069,   val balanced acc = 0.534253
2025-07-02 21:54:03.742 | INFO     | model:train:287 - EarlyStopping counter: 1 out of 3
EarlyStopping counter: 1 out of 3
Epoch 41/160:  35%|███▌      | 29/82 [00:02<00:05, 10.40it/s]wandb: WARNING Tried to log to step 40 that is less than the current step 41. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 41/160: 100%|██████████| 82/82 [00:08<00:00,  9.61it/s]
2025-07-02 21:54:12.338 | INFO     | model:train:251 - [epoch 41]: epoch loss = 1.257110,   acc = 0.938868
[epoch 41]: epoch loss = 1.257110,   acc = 0.938868
Epoch 42/160: 100%|██████████| 82/82 [00:08<00:00,  9.46it/s]
2025-07-02 21:54:21.114 | INFO     | model:train:251 - [epoch 42]: epoch loss = 1.249464,   acc = 0.938253
[epoch 42]: epoch loss = 1.249464,   acc = 0.938253
Epoch 43/160: 100%|██████████| 82/82 [00:08<00:00,  9.60it/s]
2025-07-02 21:54:29.723 | INFO     | model:train:251 - [epoch 43]: epoch loss = 1.204035,   acc = 0.942341
[epoch 43]: epoch loss = 1.204035,   acc = 0.942341
Epoch 44/160: 100%|██████████| 82/82 [00:08<00:00,  9.41it/s]
2025-07-02 21:54:38.532 | INFO     | model:train:251 - [epoch 44]: epoch loss = 1.234110,   acc = 0.938967
[epoch 44]: epoch loss = 1.234110,   acc = 0.938967
Epoch 45/160: 100%|██████████| 82/82 [00:08<00:00,  9.58it/s]
2025-07-02 21:54:47.149 | INFO     | model:train:251 - [epoch 45]: epoch loss = 1.277778,   acc = 0.934168
[epoch 45]: epoch loss = 1.277778,   acc = 0.934168
Epoch 46/160: 100%|██████████| 82/82 [00:08<00:00,  9.95it/s]
2025-07-02 21:54:55.467 | INFO     | model:train:251 - [epoch 46]: epoch loss = 1.156553,   acc = 0.943981
[epoch 46]: epoch loss = 1.156553,   acc = 0.943981
Epoch 47/160: 100%|██████████| 82/82 [00:08<00:00,  9.78it/s]
2025-07-02 21:55:03.906 | INFO     | model:train:251 - [epoch 47]: epoch loss = 1.105871,   acc = 0.947946
[epoch 47]: epoch loss = 1.105871,   acc = 0.947946
Epoch 48/160: 100%|██████████| 82/82 [00:08<00:00,  9.57it/s]
2025-07-02 21:55:12.532 | INFO     | model:train:251 - [epoch 48]: epoch loss = 1.068625,   acc = 0.950932
[epoch 48]: epoch loss = 1.068625,   acc = 0.950932
Epoch 49/160: 100%|██████████| 82/82 [00:08<00:00,  9.58it/s]
2025-07-02 21:55:21.305 | INFO     | model:train:251 - [epoch 49]: epoch loss = 1.048344,   acc = 0.952339
[epoch 49]: epoch loss = 1.048344,   acc = 0.952339
Epoch 50/160: 100%|██████████| 82/82 [00:08<00:00,  9.81it/s]
2025-07-02 21:55:29.739 | INFO     | model:train:251 - [epoch 50]: epoch loss = 1.083800,   acc = 0.947190
[epoch 50]: epoch loss = 1.083800,   acc = 0.947190
Epoch 51/160: 100%|██████████| 82/82 [00:08<00:00,  9.50it/s]
2025-07-02 21:55:38.457 | INFO     | model:train:251 - [epoch 51]: epoch loss = 1.179727,   acc = 0.938083
[epoch 51]: epoch loss = 1.179727,   acc = 0.938083
Epoch 52/160: 100%|██████████| 82/82 [00:08<00:00,  9.67it/s]
2025-07-02 21:55:47.905 | INFO     | model:train:251 - [epoch 52]: epoch loss = 1.079167,   acc = 0.946885
[epoch 52]: epoch loss = 1.079167,   acc = 0.946885
Epoch 53/160: 100%|██████████| 82/82 [00:08<00:00,  9.79it/s]
2025-07-02 21:55:56.355 | INFO     | model:train:251 - [epoch 53]: epoch loss = 1.349697,   acc = 0.924578
[epoch 53]: epoch loss = 1.349697,   acc = 0.924578
Epoch 54/160: 100%|██████████| 82/82 [00:08<00:00,  9.51it/s]
2025-07-02 21:56:05.052 | INFO     | model:train:251 - [epoch 54]: epoch loss = 1.770671,   acc = 0.890552
[epoch 54]: epoch loss = 1.770671,   acc = 0.890552
Epoch 55/160: 100%|██████████| 82/82 [00:08<00:00,  9.91it/s]
2025-07-02 21:56:13.414 | INFO     | model:train:251 - [epoch 55]: epoch loss = 1.211416,   acc = 0.937462
[epoch 55]: epoch loss = 1.211416,   acc = 0.937462
Epoch 56/160: 100%|██████████| 82/82 [00:08<00:00,  9.84it/s]
2025-07-02 21:56:21.812 | INFO     | model:train:251 - [epoch 56]: epoch loss = 1.023657,   acc = 0.953213
[epoch 56]: epoch loss = 1.023657,   acc = 0.953213
Epoch 57/160: 100%|██████████| 82/82 [00:08<00:00,  9.94it/s]
2025-07-02 21:56:30.138 | INFO     | model:train:251 - [epoch 57]: epoch loss = 0.955234,   acc = 0.958467
[epoch 57]: epoch loss = 0.955234,   acc = 0.958467
Epoch 58/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 21:56:38.551 | INFO     | model:train:251 - [epoch 58]: epoch loss = 0.923492,   acc = 0.960326
[epoch 58]: epoch loss = 0.923492,   acc = 0.960326
Epoch 59/160: 100%|██████████| 82/82 [00:08<00:00,  9.59it/s]
2025-07-02 21:56:47.242 | INFO     | model:train:251 - [epoch 59]: epoch loss = 0.906977,   acc = 0.960768
[epoch 59]: epoch loss = 0.906977,   acc = 0.960768
Epoch 60/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 21:56:55.720 | INFO     | model:train:251 - [epoch 60]: epoch loss = 0.883651,   acc = 0.962395
[epoch 60]: epoch loss = 0.883651,   acc = 0.962395
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 6659,  4100,  4730,  5968, 13280,  9899,  8995,  7279,   190,
        1767,   853,  2916]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5317,  5480,  4998,  8104, 11654, 10208,  5049,  6591,  1572,
        1590,  2900,  3173]))
2025-07-02 21:56:55.968 | INFO     | model:train:267 - [epoch 60]: val loss = 7.472898,   val acc = 0.598385,   val balanced acc = 0.539084
[epoch 60]: val loss = 7.472898,   val acc = 0.598385,   val balanced acc = 0.539084
Epoch 61/160:   6%|▌         | 5/82 [00:00<00:05, 13.68it/s]wandb: WARNING Tried to log to step 60 that is less than the current step 61. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 61/160: 100%|██████████| 82/82 [00:08<00:00,  9.72it/s]
2025-07-02 21:57:04.525 | INFO     | model:train:251 - [epoch 61]: epoch loss = 0.870155,   acc = 0.962803
[epoch 61]: epoch loss = 0.870155,   acc = 0.962803
Epoch 62/160: 100%|██████████| 82/82 [00:08<00:00,  9.67it/s]
2025-07-02 21:57:13.080 | INFO     | model:train:251 - [epoch 62]: epoch loss = 0.850738,   acc = 0.964151
[epoch 62]: epoch loss = 0.850738,   acc = 0.964151
Epoch 63/160: 100%|██████████| 82/82 [00:08<00:00,  9.59it/s]
2025-07-02 21:57:21.706 | INFO     | model:train:251 - [epoch 63]: epoch loss = 0.841055,   acc = 0.964562
[epoch 63]: epoch loss = 0.841055,   acc = 0.964562
Epoch 64/160: 100%|██████████| 82/82 [00:08<00:00,  9.71it/s]
2025-07-02 21:57:30.220 | INFO     | model:train:251 - [epoch 64]: epoch loss = 0.827996,   acc = 0.965098
[epoch 64]: epoch loss = 0.827996,   acc = 0.965098
Epoch 65/160: 100%|██████████| 82/82 [00:08<00:00,  9.48it/s]
2025-07-02 21:57:38.940 | INFO     | model:train:251 - [epoch 65]: epoch loss = 0.820311,   acc = 0.965113
[epoch 65]: epoch loss = 0.820311,   acc = 0.965113
Epoch 66/160: 100%|██████████| 82/82 [00:08<00:00,  9.66it/s]
2025-07-02 21:57:47.510 | INFO     | model:train:251 - [epoch 66]: epoch loss = 0.807538,   acc = 0.965949
[epoch 66]: epoch loss = 0.807538,   acc = 0.965949
Epoch 67/160: 100%|██████████| 82/82 [00:08<00:00,  9.52it/s]
2025-07-02 21:57:56.199 | INFO     | model:train:251 - [epoch 67]: epoch loss = 0.810041,   acc = 0.964858
[epoch 67]: epoch loss = 0.810041,   acc = 0.964858
Epoch 68/160: 100%|██████████| 82/82 [00:08<00:00,  9.49it/s]
2025-07-02 21:58:04.904 | INFO     | model:train:251 - [epoch 68]: epoch loss = 0.804241,   acc = 0.965250
[epoch 68]: epoch loss = 0.804241,   acc = 0.965250
Epoch 69/160: 100%|██████████| 82/82 [00:08<00:00,  9.61it/s]
2025-07-02 21:58:13.583 | INFO     | model:train:251 - [epoch 69]: epoch loss = 0.786918,   acc = 0.966280
[epoch 69]: epoch loss = 0.786918,   acc = 0.966280
Epoch 70/160: 100%|██████████| 82/82 [00:08<00:00,  9.65it/s]
2025-07-02 21:58:22.158 | INFO     | model:train:251 - [epoch 70]: epoch loss = 0.795447,   acc = 0.964308
[epoch 70]: epoch loss = 0.795447,   acc = 0.964308
Epoch 71/160: 100%|██████████| 82/82 [00:08<00:00,  9.36it/s]
2025-07-02 21:58:31.000 | INFO     | model:train:251 - [epoch 71]: epoch loss = 0.774754,   acc = 0.966200
[epoch 71]: epoch loss = 0.774754,   acc = 0.966200
Epoch 72/160: 100%|██████████| 82/82 [00:08<00:00,  9.58it/s]
2025-07-02 21:58:39.631 | INFO     | model:train:251 - [epoch 72]: epoch loss = 0.762126,   acc = 0.967021
[epoch 72]: epoch loss = 0.762126,   acc = 0.967021
Epoch 73/160: 100%|██████████| 82/82 [00:08<00:00,  9.42it/s]
2025-07-02 21:58:48.449 | INFO     | model:train:251 - [epoch 73]: epoch loss = 0.760258,   acc = 0.967172
[epoch 73]: epoch loss = 0.760258,   acc = 0.967172
Epoch 74/160: 100%|██████████| 82/82 [00:08<00:00,  9.47it/s]
2025-07-02 21:58:57.176 | INFO     | model:train:251 - [epoch 74]: epoch loss = 0.750974,   acc = 0.967100
[epoch 74]: epoch loss = 0.750974,   acc = 0.967100
Epoch 75/160: 100%|██████████| 82/82 [00:08<00:00,  9.54it/s]
2025-07-02 21:59:06.142 | INFO     | model:train:251 - [epoch 75]: epoch loss = 0.742845,   acc = 0.967465
[epoch 75]: epoch loss = 0.742845,   acc = 0.967465
Epoch 76/160: 100%|██████████| 82/82 [00:08<00:00,  9.80it/s]
2025-07-02 21:59:14.582 | INFO     | model:train:251 - [epoch 76]: epoch loss = 0.721610,   acc = 0.969148
[epoch 76]: epoch loss = 0.721610,   acc = 0.969148
Epoch 77/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 21:59:23.047 | INFO     | model:train:251 - [epoch 77]: epoch loss = 0.713270,   acc = 0.969777
[epoch 77]: epoch loss = 0.713270,   acc = 0.969777
Epoch 78/160: 100%|██████████| 82/82 [00:08<00:00,  9.52it/s]
2025-07-02 21:59:31.727 | INFO     | model:train:251 - [epoch 78]: epoch loss = 0.704534,   acc = 0.969988
[epoch 78]: epoch loss = 0.704534,   acc = 0.969988
Epoch 79/160: 100%|██████████| 82/82 [00:08<00:00,  9.28it/s]
2025-07-02 21:59:40.642 | INFO     | model:train:251 - [epoch 79]: epoch loss = 0.690236,   acc = 0.971271
[epoch 79]: epoch loss = 0.690236,   acc = 0.971271
Epoch 80/160: 100%|██████████| 82/82 [00:08<00:00,  9.32it/s]
2025-07-02 21:59:49.509 | INFO     | model:train:251 - [epoch 80]: epoch loss = 0.683810,   acc = 0.971520
[epoch 80]: epoch loss = 0.683810,   acc = 0.971520
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5102,  4369,  4621,  5936, 13342, 10713,  8559,  7082,   517,
        2091,  1228,  3076]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5317,  5480,  4998,  8104, 11654, 10208,  5049,  6591,  1572,
        1590,  2900,  3173]))
2025-07-02 21:59:49.779 | INFO     | model:train:267 - [epoch 80]: val loss = 7.871563,   val acc = 0.596765,   val balanced acc = 0.541273
[epoch 80]: val loss = 7.871563,   val acc = 0.596765,   val balanced acc = 0.541273
Epoch 81/160:  80%|████████  | 66/82 [00:06<00:01, 10.00it/s]wandb: WARNING Tried to log to step 80 that is less than the current step 81. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 81/160: 100%|██████████| 82/82 [00:08<00:00,  9.68it/s]
2025-07-02 21:59:58.370 | INFO     | model:train:251 - [epoch 81]: epoch loss = 0.683630,   acc = 0.971082
[epoch 81]: epoch loss = 0.683630,   acc = 0.971082
Epoch 82/160: 100%|██████████| 82/82 [00:08<00:00,  9.50it/s]
2025-07-02 22:00:07.647 | INFO     | model:train:251 - [epoch 82]: epoch loss = 0.688956,   acc = 0.969832
[epoch 82]: epoch loss = 0.688956,   acc = 0.969832
Epoch 83/160: 100%|██████████| 82/82 [00:08<00:00,  9.61it/s]
2025-07-02 22:00:16.253 | INFO     | model:train:251 - [epoch 83]: epoch loss = 0.704016,   acc = 0.968557
[epoch 83]: epoch loss = 0.704016,   acc = 0.968557
Epoch 84/160: 100%|██████████| 82/82 [00:08<00:00,  9.72it/s]
2025-07-02 22:00:24.756 | INFO     | model:train:251 - [epoch 84]: epoch loss = 0.662833,   acc = 0.972054
[epoch 84]: epoch loss = 0.662833,   acc = 0.972054
Epoch 85/160: 100%|██████████| 82/82 [00:08<00:00,  9.42it/s]
2025-07-02 22:00:33.497 | INFO     | model:train:251 - [epoch 85]: epoch loss = 0.662221,   acc = 0.971605
[epoch 85]: epoch loss = 0.662221,   acc = 0.971605
Epoch 86/160: 100%|██████████| 82/82 [00:08<00:00,  9.58it/s]
2025-07-02 22:00:42.120 | INFO     | model:train:251 - [epoch 86]: epoch loss = 0.652951,   acc = 0.972087
[epoch 86]: epoch loss = 0.652951,   acc = 0.972087
Epoch 87/160: 100%|██████████| 82/82 [00:08<00:00,  9.60it/s]
2025-07-02 22:00:50.733 | INFO     | model:train:251 - [epoch 87]: epoch loss = 0.630025,   acc = 0.974364
[epoch 87]: epoch loss = 0.630025,   acc = 0.974364
Epoch 88/160: 100%|██████████| 82/82 [00:08<00:00,  9.96it/s]
2025-07-02 22:00:59.046 | INFO     | model:train:251 - [epoch 88]: epoch loss = 0.619987,   acc = 0.975071
[epoch 88]: epoch loss = 0.619987,   acc = 0.975071
Epoch 89/160: 100%|██████████| 82/82 [00:08<00:00,  9.42it/s]
2025-07-02 22:01:07.816 | INFO     | model:train:251 - [epoch 89]: epoch loss = 0.624325,   acc = 0.974200
[epoch 89]: epoch loss = 0.624325,   acc = 0.974200
Epoch 90/160: 100%|██████████| 82/82 [00:08<00:00,  9.68it/s]
2025-07-02 22:01:16.350 | INFO     | model:train:251 - [epoch 90]: epoch loss = 0.620894,   acc = 0.974368
[epoch 90]: epoch loss = 0.620894,   acc = 0.974368
Epoch 91/160: 100%|██████████| 82/82 [00:08<00:00,  9.83it/s]
2025-07-02 22:01:24.731 | INFO     | model:train:251 - [epoch 91]: epoch loss = 0.620286,   acc = 0.973983
[epoch 91]: epoch loss = 0.620286,   acc = 0.973983
Epoch 92/160: 100%|██████████| 82/82 [00:08<00:00,  9.39it/s]
2025-07-02 22:01:33.534 | INFO     | model:train:251 - [epoch 92]: epoch loss = 0.619896,   acc = 0.973969
[epoch 92]: epoch loss = 0.619896,   acc = 0.973969
Epoch 93/160: 100%|██████████| 82/82 [00:08<00:00,  9.41it/s]
2025-07-02 22:01:42.334 | INFO     | model:train:251 - [epoch 93]: epoch loss = 0.602030,   acc = 0.975164
[epoch 93]: epoch loss = 0.602030,   acc = 0.975164
Epoch 94/160: 100%|██████████| 82/82 [00:08<00:00, 10.05it/s]
2025-07-02 22:01:50.555 | INFO     | model:train:251 - [epoch 94]: epoch loss = 0.599080,   acc = 0.975318
[epoch 94]: epoch loss = 0.599080,   acc = 0.975318
Epoch 95/160: 100%|██████████| 82/82 [00:08<00:00,  9.85it/s]
2025-07-02 22:01:58.955 | INFO     | model:train:251 - [epoch 95]: epoch loss = 0.594313,   acc = 0.975711
[epoch 95]: epoch loss = 0.594313,   acc = 0.975711
Epoch 96/160: 100%|██████████| 82/82 [00:08<00:00,  9.81it/s]
2025-07-02 22:02:07.372 | INFO     | model:train:251 - [epoch 96]: epoch loss = 0.582294,   acc = 0.976623
[epoch 96]: epoch loss = 0.582294,   acc = 0.976623
Epoch 97/160: 100%|██████████| 82/82 [00:08<00:00,  9.40it/s]
2025-07-02 22:02:16.156 | INFO     | model:train:251 - [epoch 97]: epoch loss = 0.570445,   acc = 0.977325
[epoch 97]: epoch loss = 0.570445,   acc = 0.977325
Epoch 98/160: 100%|██████████| 82/82 [00:08<00:00,  9.43it/s]
2025-07-02 22:02:24.939 | INFO     | model:train:251 - [epoch 98]: epoch loss = 0.573276,   acc = 0.976868
[epoch 98]: epoch loss = 0.573276,   acc = 0.976868
Epoch 99/160: 100%|██████████| 82/82 [00:08<00:00,  9.72it/s]
2025-07-02 22:02:33.461 | INFO     | model:train:251 - [epoch 99]: epoch loss = 0.571846,   acc = 0.976802
[epoch 99]: epoch loss = 0.571846,   acc = 0.976802
Epoch 100/160: 100%|██████████| 82/82 [00:08<00:00,  9.92it/s]
2025-07-02 22:02:41.810 | INFO     | model:train:251 - [epoch 100]: epoch loss = 0.574959,   acc = 0.976084
[epoch 100]: epoch loss = 0.574959,   acc = 0.976084
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5429,  4169,  5073,  6113, 14320, 11214,  7225,  7103,   321,
        1813,   832,  3024]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5317,  5480,  4998,  8104, 11654, 10208,  5049,  6591,  1572,
        1590,  2900,  3173]))
2025-07-02 22:02:42.071 | INFO     | model:train:267 - [epoch 100]: val loss = 8.225839,   val acc = 0.605754,   val balanced acc = 0.538733
[epoch 100]: val loss = 8.225839,   val acc = 0.605754,   val balanced acc = 0.538733
2025-07-02 22:02:42.107 | INFO     | model:train:287 - EarlyStopping counter: 1 out of 3
EarlyStopping counter: 1 out of 3
Epoch 101/160:  49%|████▉     | 40/82 [00:04<00:04,  9.01it/s]wandb: WARNING Tried to log to step 100 that is less than the current step 101. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 101/160: 100%|██████████| 82/82 [00:08<00:00,  9.48it/s]
2025-07-02 22:02:50.853 | INFO     | model:train:251 - [epoch 101]: epoch loss = 0.567522,   acc = 0.976850
[epoch 101]: epoch loss = 0.567522,   acc = 0.976850
Epoch 102/160: 100%|██████████| 82/82 [00:08<00:00,  9.56it/s]
2025-07-02 22:02:59.512 | INFO     | model:train:251 - [epoch 102]: epoch loss = 0.549580,   acc = 0.978508
[epoch 102]: epoch loss = 0.549580,   acc = 0.978508
Epoch 103/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 22:03:07.985 | INFO     | model:train:251 - [epoch 103]: epoch loss = 0.542895,   acc = 0.978771
[epoch 103]: epoch loss = 0.542895,   acc = 0.978771
Epoch 104/160: 100%|██████████| 82/82 [00:08<00:00,  9.52it/s]
2025-07-02 22:03:16.676 | INFO     | model:train:251 - [epoch 104]: epoch loss = 0.543052,   acc = 0.978640
[epoch 104]: epoch loss = 0.543052,   acc = 0.978640
Epoch 105/160: 100%|██████████| 82/82 [00:08<00:00,  9.78it/s]
2025-07-02 22:03:26.014 | INFO     | model:train:251 - [epoch 105]: epoch loss = 0.537568,   acc = 0.978717
[epoch 105]: epoch loss = 0.537568,   acc = 0.978717
Epoch 106/160: 100%|██████████| 82/82 [00:08<00:00,  9.57it/s]
2025-07-02 22:03:34.694 | INFO     | model:train:251 - [epoch 106]: epoch loss = 0.531259,   acc = 0.979219
[epoch 106]: epoch loss = 0.531259,   acc = 0.979219
Epoch 107/160: 100%|██████████| 82/82 [00:08<00:00,  9.81it/s]
2025-07-02 22:03:43.162 | INFO     | model:train:251 - [epoch 107]: epoch loss = 0.529422,   acc = 0.979206
[epoch 107]: epoch loss = 0.529422,   acc = 0.979206
Epoch 108/160: 100%|██████████| 82/82 [00:08<00:00,  9.35it/s]
2025-07-02 22:03:52.099 | INFO     | model:train:251 - [epoch 108]: epoch loss = 0.519835,   acc = 0.980042
[epoch 108]: epoch loss = 0.519835,   acc = 0.980042
Epoch 109/160: 100%|██████████| 82/82 [00:08<00:00,  9.49it/s]
2025-07-02 22:04:01.333 | INFO     | model:train:251 - [epoch 109]: epoch loss = 0.519087,   acc = 0.979917
[epoch 109]: epoch loss = 0.519087,   acc = 0.979917
Epoch 110/160: 100%|██████████| 82/82 [00:08<00:00,  9.84it/s]
2025-07-02 22:04:09.745 | INFO     | model:train:251 - [epoch 110]: epoch loss = 0.510438,   acc = 0.980707
[epoch 110]: epoch loss = 0.510438,   acc = 0.980707
Epoch 111/160: 100%|██████████| 82/82 [00:08<00:00,  9.42it/s]
2025-07-02 22:04:18.539 | INFO     | model:train:251 - [epoch 111]: epoch loss = 0.518178,   acc = 0.979423
[epoch 111]: epoch loss = 0.518178,   acc = 0.979423
Epoch 112/160: 100%|██████████| 82/82 [00:08<00:00,  9.42it/s]
2025-07-02 22:04:27.313 | INFO     | model:train:251 - [epoch 112]: epoch loss = 0.513990,   acc = 0.980148
[epoch 112]: epoch loss = 0.513990,   acc = 0.980148
Epoch 113/160: 100%|██████████| 82/82 [00:08<00:00,  9.46it/s]
2025-07-02 22:04:36.060 | INFO     | model:train:251 - [epoch 113]: epoch loss = 0.504749,   acc = 0.980860
[epoch 113]: epoch loss = 0.504749,   acc = 0.980860
Epoch 114/160: 100%|██████████| 82/82 [00:08<00:00,  9.56it/s]
2025-07-02 22:04:44.708 | INFO     | model:train:251 - [epoch 114]: epoch loss = 0.503632,   acc = 0.980669
[epoch 114]: epoch loss = 0.503632,   acc = 0.980669
Epoch 115/160: 100%|██████████| 82/82 [00:08<00:00,  9.57it/s]
2025-07-02 22:04:53.348 | INFO     | model:train:251 - [epoch 115]: epoch loss = 0.497846,   acc = 0.981067
[epoch 115]: epoch loss = 0.497846,   acc = 0.981067
Epoch 116/160: 100%|██████████| 82/82 [00:08<00:00,  9.52it/s]
2025-07-02 22:05:02.028 | INFO     | model:train:251 - [epoch 116]: epoch loss = 0.495370,   acc = 0.981352
[epoch 116]: epoch loss = 0.495370,   acc = 0.981352
Epoch 117/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 22:05:10.512 | INFO     | model:train:251 - [epoch 117]: epoch loss = 0.489680,   acc = 0.981551
[epoch 117]: epoch loss = 0.489680,   acc = 0.981551
Epoch 118/160: 100%|██████████| 82/82 [00:08<00:00,  9.66it/s]
2025-07-02 22:05:19.079 | INFO     | model:train:251 - [epoch 118]: epoch loss = 0.493077,   acc = 0.981070
[epoch 118]: epoch loss = 0.493077,   acc = 0.981070
Epoch 119/160: 100%|██████████| 82/82 [00:08<00:00,  9.40it/s]
2025-07-02 22:05:27.877 | INFO     | model:train:251 - [epoch 119]: epoch loss = 0.491052,   acc = 0.981406
[epoch 119]: epoch loss = 0.491052,   acc = 0.981406
Epoch 120/160: 100%|██████████| 82/82 [00:08<00:00,  9.56it/s]
2025-07-02 22:05:36.536 | INFO     | model:train:251 - [epoch 120]: epoch loss = 0.485821,   acc = 0.981548
[epoch 120]: epoch loss = 0.485821,   acc = 0.981548
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 6128,  3973,  5258,  6217, 12795, 11446,  7578,  7496,   185,
        1617,   892,  3051]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5317,  5480,  4998,  8104, 11654, 10208,  5049,  6591,  1572,
        1590,  2900,  3173]))
2025-07-02 22:05:36.827 | INFO     | model:train:267 - [epoch 120]: val loss = 8.565753,   val acc = 0.604463,   val balanced acc = 0.541281
[epoch 120]: val loss = 8.565753,   val acc = 0.604463,   val balanced acc = 0.541281
Epoch 121/160: 100%|██████████| 82/82 [00:08<00:00,  9.63it/s]
2025-07-02 22:05:45.461 | INFO     | model:train:251 - [epoch 121]: epoch loss = 0.485379,   acc = 0.981342
[epoch 121]: epoch loss = 0.485379,   acc = 0.981342
Epoch 122/160:  15%|█▍        | 12/82 [00:01<00:06, 10.32it/s]wandb: WARNING Tried to log to step 120 that is less than the current step 121. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 122/160: 100%|██████████| 82/82 [00:08<00:00,  9.60it/s]
2025-07-02 22:05:54.071 | INFO     | model:train:251 - [epoch 122]: epoch loss = 0.481589,   acc = 0.981732
[epoch 122]: epoch loss = 0.481589,   acc = 0.981732
Epoch 123/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 22:06:03.003 | INFO     | model:train:251 - [epoch 123]: epoch loss = 0.475800,   acc = 0.982192
[epoch 123]: epoch loss = 0.475800,   acc = 0.982192
Epoch 124/160: 100%|██████████| 82/82 [00:08<00:00,  9.64it/s]
2025-07-02 22:06:11.587 | INFO     | model:train:251 - [epoch 124]: epoch loss = 0.474966,   acc = 0.982260
[epoch 124]: epoch loss = 0.474966,   acc = 0.982260
Epoch 125/160: 100%|██████████| 82/82 [00:08<00:00, 10.00it/s]
2025-07-02 22:06:19.855 | INFO     | model:train:251 - [epoch 125]: epoch loss = 0.474960,   acc = 0.982192
[epoch 125]: epoch loss = 0.474960,   acc = 0.982192
Epoch 126/160: 100%|██████████| 82/82 [00:08<00:00,  9.94it/s]
2025-07-02 22:06:28.532 | INFO     | model:train:251 - [epoch 126]: epoch loss = 0.473614,   acc = 0.982076
[epoch 126]: epoch loss = 0.473614,   acc = 0.982076
Epoch 127/160: 100%|██████████| 82/82 [00:08<00:00,  9.72it/s]
2025-07-02 22:06:37.036 | INFO     | model:train:251 - [epoch 127]: epoch loss = 0.470118,   acc = 0.982410
[epoch 127]: epoch loss = 0.470118,   acc = 0.982410
Epoch 128/160: 100%|██████████| 82/82 [00:08<00:00,  9.84it/s]
2025-07-02 22:06:45.439 | INFO     | model:train:251 - [epoch 128]: epoch loss = 0.467636,   acc = 0.982503
[epoch 128]: epoch loss = 0.467636,   acc = 0.982503
Epoch 129/160: 100%|██████████| 82/82 [00:08<00:00,  9.60it/s]
2025-07-02 22:06:54.334 | INFO     | model:train:251 - [epoch 129]: epoch loss = 0.464272,   acc = 0.982820
[epoch 129]: epoch loss = 0.464272,   acc = 0.982820
Epoch 130/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 22:07:02.801 | INFO     | model:train:251 - [epoch 130]: epoch loss = 0.462686,   acc = 0.982904
[epoch 130]: epoch loss = 0.462686,   acc = 0.982904
Epoch 131/160: 100%|██████████| 82/82 [00:08<00:00,  9.80it/s]
2025-07-02 22:07:11.274 | INFO     | model:train:251 - [epoch 131]: epoch loss = 0.461536,   acc = 0.982989
[epoch 131]: epoch loss = 0.461536,   acc = 0.982989
Epoch 132/160: 100%|██████████| 82/82 [00:08<00:00,  9.84it/s]
2025-07-02 22:07:19.680 | INFO     | model:train:251 - [epoch 132]: epoch loss = 0.461176,   acc = 0.982924
[epoch 132]: epoch loss = 0.461176,   acc = 0.982924
Epoch 133/160: 100%|██████████| 82/82 [00:08<00:00,  9.57it/s]
2025-07-02 22:07:28.463 | INFO     | model:train:251 - [epoch 133]: epoch loss = 0.457169,   acc = 0.983268
[epoch 133]: epoch loss = 0.457169,   acc = 0.983268
Epoch 134/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 22:07:36.813 | INFO     | model:train:251 - [epoch 134]: epoch loss = 0.456965,   acc = 0.983355
[epoch 134]: epoch loss = 0.456965,   acc = 0.983355
Epoch 135/160: 100%|██████████| 82/82 [00:08<00:00,  9.85it/s]
2025-07-02 22:07:45.441 | INFO     | model:train:251 - [epoch 135]: epoch loss = 0.455224,   acc = 0.983333
[epoch 135]: epoch loss = 0.455224,   acc = 0.983333
Epoch 136/160: 100%|██████████| 82/82 [00:08<00:00,  9.75it/s]
2025-07-02 22:07:53.914 | INFO     | model:train:251 - [epoch 136]: epoch loss = 0.457463,   acc = 0.983138
[epoch 136]: epoch loss = 0.457463,   acc = 0.983138
Epoch 137/160: 100%|██████████| 82/82 [00:08<00:00,  9.79it/s]
2025-07-02 22:08:02.360 | INFO     | model:train:251 - [epoch 137]: epoch loss = 0.454097,   acc = 0.983340
[epoch 137]: epoch loss = 0.454097,   acc = 0.983340
Epoch 138/160: 100%|██████████| 82/82 [00:08<00:00,  9.43it/s]
2025-07-02 22:08:11.114 | INFO     | model:train:251 - [epoch 138]: epoch loss = 0.454936,   acc = 0.983170
[epoch 138]: epoch loss = 0.454936,   acc = 0.983170
Epoch 139/160: 100%|██████████| 82/82 [00:08<00:00,  9.77it/s]
2025-07-02 22:08:19.571 | INFO     | model:train:251 - [epoch 139]: epoch loss = 0.454631,   acc = 0.983294
[epoch 139]: epoch loss = 0.454631,   acc = 0.983294
Epoch 140/160: 100%|██████████| 82/82 [00:08<00:00,  9.63it/s]
2025-07-02 22:08:28.153 | INFO     | model:train:251 - [epoch 140]: epoch loss = 0.450389,   acc = 0.983555
[epoch 140]: epoch loss = 0.450389,   acc = 0.983555
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 6026,  4545,  4841,  6156, 13355, 12149,  6662,  7282,   221,
        1534,   839,  3026]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5317,  5480,  4998,  8104, 11654, 10208,  5049,  6591,  1572,
        1590,  2900,  3173]))
2025-07-02 22:08:28.454 | INFO     | model:train:267 - [epoch 140]: val loss = 8.716910,   val acc = 0.607855,   val balanced acc = 0.537417
[epoch 140]: val loss = 8.716910,   val acc = 0.607855,   val balanced acc = 0.537417
2025-07-02 22:08:28.497 | INFO     | model:train:287 - EarlyStopping counter: 1 out of 3
EarlyStopping counter: 1 out of 3
Epoch 141/160:  93%|█████████▎| 76/82 [00:07<00:00,  9.14it/s]wandb: WARNING Tried to log to step 140 that is less than the current step 141. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 141/160: 100%|██████████| 82/82 [00:08<00:00,  9.64it/s]
2025-07-02 22:08:37.073 | INFO     | model:train:251 - [epoch 141]: epoch loss = 0.450743,   acc = 0.983698
[epoch 141]: epoch loss = 0.450743,   acc = 0.983698
Epoch 142/160: 100%|██████████| 82/82 [00:08<00:00,  9.87it/s]
2025-07-02 22:08:45.454 | INFO     | model:train:251 - [epoch 142]: epoch loss = 0.448054,   acc = 0.983868
[epoch 142]: epoch loss = 0.448054,   acc = 0.983868
Epoch 143/160: 100%|██████████| 82/82 [00:08<00:00,  9.75it/s]
2025-07-02 22:08:53.936 | INFO     | model:train:251 - [epoch 143]: epoch loss = 0.448662,   acc = 0.983695
[epoch 143]: epoch loss = 0.448662,   acc = 0.983695
Epoch 144/160: 100%|██████████| 82/82 [00:08<00:00,  9.69it/s]
2025-07-02 22:09:02.491 | INFO     | model:train:251 - [epoch 144]: epoch loss = 0.449014,   acc = 0.983732
[epoch 144]: epoch loss = 0.449014,   acc = 0.983732
Epoch 145/160: 100%|██████████| 82/82 [00:08<00:00,  9.53it/s]
2025-07-02 22:09:11.202 | INFO     | model:train:251 - [epoch 145]: epoch loss = 0.446994,   acc = 0.983708
[epoch 145]: epoch loss = 0.446994,   acc = 0.983708
Epoch 146/160: 100%|██████████| 82/82 [00:08<00:00,  9.53it/s]
2025-07-02 22:09:19.868 | INFO     | model:train:251 - [epoch 146]: epoch loss = 0.447104,   acc = 0.983758
[epoch 146]: epoch loss = 0.447104,   acc = 0.983758
Epoch 147/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 22:09:28.333 | INFO     | model:train:251 - [epoch 147]: epoch loss = 0.445891,   acc = 0.983900
[epoch 147]: epoch loss = 0.445891,   acc = 0.983900
Epoch 148/160: 100%|██████████| 82/82 [00:08<00:00,  9.70it/s]
2025-07-02 22:09:36.849 | INFO     | model:train:251 - [epoch 148]: epoch loss = 0.444595,   acc = 0.984112
[epoch 148]: epoch loss = 0.444595,   acc = 0.984112
Epoch 149/160: 100%|██████████| 82/82 [00:08<00:00,  9.55it/s]
2025-07-02 22:09:45.501 | INFO     | model:train:251 - [epoch 149]: epoch loss = 0.444803,   acc = 0.984101
[epoch 149]: epoch loss = 0.444803,   acc = 0.984101
Epoch 150/160: 100%|██████████| 82/82 [00:08<00:00,  9.81it/s]
2025-07-02 22:09:53.927 | INFO     | model:train:251 - [epoch 150]: epoch loss = 0.444787,   acc = 0.984134
[epoch 150]: epoch loss = 0.444787,   acc = 0.984134
Epoch 151/160: 100%|██████████| 82/82 [00:08<00:00,  9.56it/s]
2025-07-02 22:10:02.741 | INFO     | model:train:251 - [epoch 151]: epoch loss = 0.442168,   acc = 0.984240
[epoch 151]: epoch loss = 0.442168,   acc = 0.984240
Epoch 152/160: 100%|██████████| 82/82 [00:08<00:00,  9.61it/s]
2025-07-02 22:10:11.342 | INFO     | model:train:251 - [epoch 152]: epoch loss = 0.444606,   acc = 0.984045
[epoch 152]: epoch loss = 0.444606,   acc = 0.984045
Epoch 153/160: 100%|██████████| 82/82 [00:08<00:00,  9.93it/s]
2025-07-02 22:10:19.895 | INFO     | model:train:251 - [epoch 153]: epoch loss = 0.444531,   acc = 0.984096
[epoch 153]: epoch loss = 0.444531,   acc = 0.984096
Epoch 154/160: 100%|██████████| 82/82 [00:08<00:00,  9.73it/s]
2025-07-02 22:10:28.407 | INFO     | model:train:251 - [epoch 154]: epoch loss = 0.443066,   acc = 0.984118
[epoch 154]: epoch loss = 0.443066,   acc = 0.984118
Epoch 155/160: 100%|██████████| 82/82 [00:08<00:00,  9.79it/s]
2025-07-02 22:10:36.965 | INFO     | model:train:251 - [epoch 155]: epoch loss = 0.443645,   acc = 0.984006
[epoch 155]: epoch loss = 0.443645,   acc = 0.984006
Epoch 156/160: 100%|██████████| 82/82 [00:08<00:00,  9.67it/s]
2025-07-02 22:10:45.525 | INFO     | model:train:251 - [epoch 156]: epoch loss = 0.442572,   acc = 0.984171
[epoch 156]: epoch loss = 0.442572,   acc = 0.984171
Epoch 157/160: 100%|██████████| 82/82 [00:08<00:00,  9.96it/s]
2025-07-02 22:10:53.834 | INFO     | model:train:251 - [epoch 157]: epoch loss = 0.443605,   acc = 0.984056
[epoch 157]: epoch loss = 0.443605,   acc = 0.984056
Epoch 158/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 22:11:02.420 | INFO     | model:train:251 - [epoch 158]: epoch loss = 0.441643,   acc = 0.984194
[epoch 158]: epoch loss = 0.441643,   acc = 0.984194
Epoch 159/160: 100%|██████████| 82/82 [00:08<00:00,  9.92it/s]
2025-07-02 22:11:10.771 | INFO     | model:train:251 - [epoch 159]: epoch loss = 0.443664,   acc = 0.984125
[epoch 159]: epoch loss = 0.443664,   acc = 0.984125
Epoch 160/160: 100%|██████████| 82/82 [00:08<00:00,  9.46it/s]
2025-07-02 22:11:19.516 | INFO     | model:train:251 - [epoch 160]: epoch loss = 0.441212,   acc = 0.984451
[epoch 160]: epoch loss = 0.441212,   acc = 0.984451
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 6018,  4611,  4779,  5996, 13457, 11737,  7108,  7294,   220,
        1571,   854,  2991]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 5317,  5480,  4998,  8104, 11654, 10208,  5049,  6591,  1572,
        1590,  2900,  3173]))
2025-07-02 22:11:19.810 | INFO     | model:train:267 - [epoch 160]: val loss = 8.781706,   val acc = 0.603202,   val balanced acc = 0.535352
[epoch 160]: val loss = 8.781706,   val acc = 0.603202,   val balanced acc = 0.535352
2025-07-02 22:11:19.847 | INFO     | model:train:287 - EarlyStopping counter: 2 out of 3
EarlyStopping counter: 2 out of 3
