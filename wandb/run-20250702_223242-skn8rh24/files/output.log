Epoch 1/160: 100%|██████████| 82/82 [00:08<00:00,  9.32it/s]
2025-07-02 22:32:53.407 | INFO     | model:train:251 - [epoch 1]: epoch loss = 9.287534,   acc = 0.173264
[epoch 1]: epoch loss = 9.287534,   acc = 0.173264
Epoch 2/160: 100%|██████████| 82/82 [00:08<00:00,  9.66it/s]
2025-07-02 22:33:01.929 | INFO     | model:train:251 - [epoch 2]: epoch loss = 8.386000,   acc = 0.208713
[epoch 2]: epoch loss = 8.386000,   acc = 0.208713
Epoch 3/160: 100%|██████████| 82/82 [00:08<00:00,  9.52it/s]
2025-07-02 22:33:10.601 | INFO     | model:train:251 - [epoch 3]: epoch loss = 7.074673,   acc = 0.346270
[epoch 3]: epoch loss = 7.074673,   acc = 0.346270
Epoch 4/160: 100%|██████████| 82/82 [00:08<00:00,  9.55it/s]
2025-07-02 22:33:19.246 | INFO     | model:train:251 - [epoch 4]: epoch loss = 5.835765,   acc = 0.537494
[epoch 4]: epoch loss = 5.835765,   acc = 0.537494
Epoch 5/160: 100%|██████████| 82/82 [00:08<00:00,  9.55it/s]
2025-07-02 22:33:27.970 | INFO     | model:train:251 - [epoch 5]: epoch loss = 5.213161,   acc = 0.608681
[epoch 5]: epoch loss = 5.213161,   acc = 0.608681
Epoch 6/160: 100%|██████████| 82/82 [00:08<00:00,  9.54it/s]
2025-07-02 22:33:36.643 | INFO     | model:train:251 - [epoch 6]: epoch loss = 4.835897,   acc = 0.641125
[epoch 6]: epoch loss = 4.835897,   acc = 0.641125
Epoch 7/160: 100%|██████████| 82/82 [00:08<00:00,  9.51it/s]
2025-07-02 22:33:45.330 | INFO     | model:train:251 - [epoch 7]: epoch loss = 4.432673,   acc = 0.677187
[epoch 7]: epoch loss = 4.432673,   acc = 0.677187
Epoch 8/160: 100%|██████████| 82/82 [00:08<00:00,  9.67it/s]
2025-07-02 22:33:53.971 | INFO     | model:train:251 - [epoch 8]: epoch loss = 4.152112,   acc = 0.703601
[epoch 8]: epoch loss = 4.152112,   acc = 0.703601
Epoch 9/160: 100%|██████████| 82/82 [00:08<00:00,  9.66it/s]
2025-07-02 22:34:02.531 | INFO     | model:train:251 - [epoch 9]: epoch loss = 4.018852,   acc = 0.713458
[epoch 9]: epoch loss = 4.018852,   acc = 0.713458
Epoch 10/160: 100%|██████████| 82/82 [00:08<00:00,  9.62it/s]
2025-07-02 22:34:11.136 | INFO     | model:train:251 - [epoch 10]: epoch loss = 3.726676,   acc = 0.737858
[epoch 10]: epoch loss = 3.726676,   acc = 0.737858
Epoch 11/160: 100%|██████████| 82/82 [00:08<00:00,  9.53it/s]
2025-07-02 22:34:19.817 | INFO     | model:train:251 - [epoch 11]: epoch loss = 3.443442,   acc = 0.764713
[epoch 11]: epoch loss = 3.443442,   acc = 0.764713
Epoch 12/160: 100%|██████████| 82/82 [00:08<00:00,  9.51it/s]
2025-07-02 22:34:28.506 | INFO     | model:train:251 - [epoch 12]: epoch loss = 3.383520,   acc = 0.769841
[epoch 12]: epoch loss = 3.383520,   acc = 0.769841
Epoch 13/160: 100%|██████████| 82/82 [00:08<00:00,  9.55it/s]
2025-07-02 22:34:37.147 | INFO     | model:train:251 - [epoch 13]: epoch loss = 3.218251,   acc = 0.785614
[epoch 13]: epoch loss = 3.218251,   acc = 0.785614
Epoch 14/160: 100%|██████████| 82/82 [00:08<00:00,  9.57it/s]
2025-07-02 22:34:45.774 | INFO     | model:train:251 - [epoch 14]: epoch loss = 2.939627,   acc = 0.810602
[epoch 14]: epoch loss = 2.939627,   acc = 0.810602
Epoch 15/160: 100%|██████████| 82/82 [00:08<00:00,  9.54it/s]
2025-07-02 22:34:54.475 | INFO     | model:train:251 - [epoch 15]: epoch loss = 2.947203,   acc = 0.809355
[epoch 15]: epoch loss = 2.947203,   acc = 0.809355
Epoch 16/160: 100%|██████████| 82/82 [00:08<00:00,  9.64it/s]
2025-07-02 22:35:03.067 | INFO     | model:train:251 - [epoch 16]: epoch loss = 2.729739,   acc = 0.829965
[epoch 16]: epoch loss = 2.729739,   acc = 0.829965
Epoch 17/160: 100%|██████████| 82/82 [00:08<00:00,  9.65it/s]
2025-07-02 22:35:11.642 | INFO     | model:train:251 - [epoch 17]: epoch loss = 2.646217,   acc = 0.832943
[epoch 17]: epoch loss = 2.646217,   acc = 0.832943
Epoch 18/160: 100%|██████████| 82/82 [00:08<00:00,  9.61it/s]
2025-07-02 22:35:20.624 | INFO     | model:train:251 - [epoch 18]: epoch loss = 2.652653,   acc = 0.834030
[epoch 18]: epoch loss = 2.652653,   acc = 0.834030
Epoch 19/160: 100%|██████████| 82/82 [00:08<00:00,  9.52it/s]
2025-07-02 22:35:29.311 | INFO     | model:train:251 - [epoch 19]: epoch loss = 2.540756,   acc = 0.843435
[epoch 19]: epoch loss = 2.540756,   acc = 0.843435
Epoch 20/160: 100%|██████████| 82/82 [00:08<00:00,  9.59it/s]
2025-07-02 22:35:37.936 | INFO     | model:train:251 - [epoch 20]: epoch loss = 2.465070,   acc = 0.847332
[epoch 20]: epoch loss = 2.465070,   acc = 0.847332
/home/djh/my_envs/dai_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2776: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 6933,  6228,  5564,  6249, 14609, 15296,  7104,  7216,   429,
         851,  1073,  3581]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([ 8428,  6639,  5421,  5982, 14716, 15255,  6793,  6137,  1236,
        1311,  3215]))
2025-07-02 22:35:38.291 | INFO     | model:train:267 - [epoch 20]: val loss = 5.151544,   val acc = 0.615309,   val balanced acc = 0.606859
[epoch 20]: val loss = 5.151544,   val acc = 0.615309,   val balanced acc = 0.606859
Epoch 21/160:  63%|██████▎   | 52/82 [00:05<00:02, 11.68it/s]wandb: WARNING Tried to log to step 20 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 21/160: 100%|██████████| 82/82 [00:08<00:00,  9.73it/s]
2025-07-02 22:35:46.844 | INFO     | model:train:251 - [epoch 21]: epoch loss = 2.193013,   acc = 0.873078
[epoch 21]: epoch loss = 2.193013,   acc = 0.873078
Epoch 22/160: 100%|██████████| 82/82 [00:08<00:00,  9.79it/s]
2025-07-02 22:35:55.285 | INFO     | model:train:251 - [epoch 22]: epoch loss = 2.148645,   acc = 0.875290
[epoch 22]: epoch loss = 2.148645,   acc = 0.875290
Epoch 23/160: 100%|██████████| 82/82 [00:08<00:00,  9.96it/s]
2025-07-02 22:36:03.579 | INFO     | model:train:251 - [epoch 23]: epoch loss = 2.128758,   acc = 0.875594
[epoch 23]: epoch loss = 2.128758,   acc = 0.875594
Epoch 24/160: 100%|██████████| 82/82 [00:08<00:00,  9.72it/s]
2025-07-02 22:36:12.083 | INFO     | model:train:251 - [epoch 24]: epoch loss = 1.963488,   acc = 0.889513
[epoch 24]: epoch loss = 1.963488,   acc = 0.889513
Epoch 25/160: 100%|██████████| 82/82 [00:08<00:00, 10.19it/s]
2025-07-02 22:36:20.210 | INFO     | model:train:251 - [epoch 25]: epoch loss = 1.854889,   acc = 0.898315
[epoch 25]: epoch loss = 1.854889,   acc = 0.898315
Epoch 26/160: 100%|██████████| 82/82 [00:08<00:00, 10.07it/s]
2025-07-02 22:36:28.431 | INFO     | model:train:251 - [epoch 26]: epoch loss = 1.768613,   acc = 0.904430
[epoch 26]: epoch loss = 1.768613,   acc = 0.904430
Epoch 27/160: 100%|██████████| 82/82 [00:08<00:00, 10.05it/s]
2025-07-02 22:36:36.662 | INFO     | model:train:251 - [epoch 27]: epoch loss = 1.788524,   acc = 0.901720
[epoch 27]: epoch loss = 1.788524,   acc = 0.901720
Epoch 28/160: 100%|██████████| 82/82 [00:08<00:00,  9.97it/s]
2025-07-02 22:36:44.957 | INFO     | model:train:251 - [epoch 28]: epoch loss = 2.020357,   acc = 0.877897
[epoch 28]: epoch loss = 2.020357,   acc = 0.877897
Epoch 29/160: 100%|██████████| 82/82 [00:08<00:00, 10.24it/s]
2025-07-02 22:36:53.039 | INFO     | model:train:251 - [epoch 29]: epoch loss = 1.984543,   acc = 0.883298
[epoch 29]: epoch loss = 1.984543,   acc = 0.883298
Epoch 30/160: 100%|██████████| 82/82 [00:08<00:00, 10.01it/s]
2025-07-02 22:37:01.310 | INFO     | model:train:251 - [epoch 30]: epoch loss = 1.725567,   acc = 0.906280
[epoch 30]: epoch loss = 1.725567,   acc = 0.906280
Epoch 31/160: 100%|██████████| 82/82 [00:08<00:00, 10.04it/s]
2025-07-02 22:37:09.594 | INFO     | model:train:251 - [epoch 31]: epoch loss = 1.651531,   acc = 0.910795
[epoch 31]: epoch loss = 1.651531,   acc = 0.910795
Epoch 32/160: 100%|██████████| 82/82 [00:08<00:00,  9.79it/s]
2025-07-02 22:37:18.051 | INFO     | model:train:251 - [epoch 32]: epoch loss = 1.529799,   acc = 0.921847
[epoch 32]: epoch loss = 1.529799,   acc = 0.921847
Epoch 33/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 22:37:26.592 | INFO     | model:train:251 - [epoch 33]: epoch loss = 1.486038,   acc = 0.924507
[epoch 33]: epoch loss = 1.486038,   acc = 0.924507
Epoch 34/160: 100%|██████████| 82/82 [00:08<00:00,  9.69it/s]
2025-07-02 22:37:35.128 | INFO     | model:train:251 - [epoch 34]: epoch loss = 1.463850,   acc = 0.924757
[epoch 34]: epoch loss = 1.463850,   acc = 0.924757
Epoch 35/160: 100%|██████████| 82/82 [00:08<00:00, 10.00it/s]
2025-07-02 22:37:43.419 | INFO     | model:train:251 - [epoch 35]: epoch loss = 1.392865,   acc = 0.929833
[epoch 35]: epoch loss = 1.392865,   acc = 0.929833
Epoch 36/160: 100%|██████████| 82/82 [00:08<00:00,  9.97it/s]
2025-07-02 22:37:51.865 | INFO     | model:train:251 - [epoch 36]: epoch loss = 1.391160,   acc = 0.930180
[epoch 36]: epoch loss = 1.391160,   acc = 0.930180
Epoch 37/160: 100%|██████████| 82/82 [00:08<00:00, 10.04it/s]
2025-07-02 22:38:00.104 | INFO     | model:train:251 - [epoch 37]: epoch loss = 1.325067,   acc = 0.935103
[epoch 37]: epoch loss = 1.325067,   acc = 0.935103
Epoch 38/160: 100%|██████████| 82/82 [00:08<00:00, 10.10it/s]
2025-07-02 22:38:08.472 | INFO     | model:train:251 - [epoch 38]: epoch loss = 1.308387,   acc = 0.934446
[epoch 38]: epoch loss = 1.308387,   acc = 0.934446
Epoch 39/160: 100%|██████████| 82/82 [00:08<00:00,  9.92it/s]
2025-07-02 22:38:16.815 | INFO     | model:train:251 - [epoch 39]: epoch loss = 1.680260,   acc = 0.904955
[epoch 39]: epoch loss = 1.680260,   acc = 0.904955
Epoch 40/160: 100%|██████████| 82/82 [00:08<00:00, 10.17it/s]
2025-07-02 22:38:24.989 | INFO     | model:train:251 - [epoch 40]: epoch loss = 2.010295,   acc = 0.877595
[epoch 40]: epoch loss = 2.010295,   acc = 0.877595
/home/djh/my_envs/dai_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2776: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 7486,  6083,  5777,  7851, 12984, 15223,  6572,  7778,   658,
         676,  1129,  2916]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([ 8428,  6639,  5421,  5982, 14716, 15255,  6793,  6137,  1236,
        1311,  3215]))
2025-07-02 22:38:25.321 | INFO     | model:train:267 - [epoch 40]: val loss = 5.808761,   val acc = 0.601799,   val balanced acc = 0.595071
[epoch 40]: val loss = 5.808761,   val acc = 0.601799,   val balanced acc = 0.595071
2025-07-02 22:38:25.356 | INFO     | model:train:287 - EarlyStopping counter: 1 out of 3
EarlyStopping counter: 1 out of 3
Epoch 41/160: 100%|██████████| 82/82 [00:08<00:00, 10.12it/s]
2025-07-02 22:38:33.542 | INFO     | model:train:251 - [epoch 41]: epoch loss = 1.384404,   acc = 0.928224
[epoch 41]: epoch loss = 1.384404,   acc = 0.928224
Epoch 42/160:   0%|          | 0/82 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 40 that is less than the current step 41. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 42/160: 100%|██████████| 82/82 [00:08<00:00, 10.12it/s]
2025-07-02 22:38:41.730 | INFO     | model:train:251 - [epoch 42]: epoch loss = 1.258636,   acc = 0.938886
[epoch 42]: epoch loss = 1.258636,   acc = 0.938886
Epoch 43/160: 100%|██████████| 82/82 [00:08<00:00, 10.05it/s]
2025-07-02 22:38:49.973 | INFO     | model:train:251 - [epoch 43]: epoch loss = 1.200356,   acc = 0.943259
[epoch 43]: epoch loss = 1.200356,   acc = 0.943259
Epoch 44/160: 100%|██████████| 82/82 [00:08<00:00,  9.73it/s]
2025-07-02 22:38:58.472 | INFO     | model:train:251 - [epoch 44]: epoch loss = 1.152696,   acc = 0.946419
[epoch 44]: epoch loss = 1.152696,   acc = 0.946419
Epoch 45/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 22:39:07.039 | INFO     | model:train:251 - [epoch 45]: epoch loss = 1.147220,   acc = 0.946087
[epoch 45]: epoch loss = 1.147220,   acc = 0.946087
Epoch 46/160: 100%|██████████| 82/82 [00:08<00:00,  9.90it/s]
2025-07-02 22:39:15.424 | INFO     | model:train:251 - [epoch 46]: epoch loss = 1.151803,   acc = 0.944436
[epoch 46]: epoch loss = 1.151803,   acc = 0.944436
Epoch 47/160: 100%|██████████| 82/82 [00:08<00:00,  9.98it/s]
2025-07-02 22:39:23.714 | INFO     | model:train:251 - [epoch 47]: epoch loss = 1.091713,   acc = 0.949182
[epoch 47]: epoch loss = 1.091713,   acc = 0.949182
Epoch 48/160: 100%|██████████| 82/82 [00:08<00:00,  9.99it/s]
2025-07-02 22:39:32.003 | INFO     | model:train:251 - [epoch 48]: epoch loss = 1.081651,   acc = 0.949836
[epoch 48]: epoch loss = 1.081651,   acc = 0.949836
Epoch 49/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 22:39:40.735 | INFO     | model:train:251 - [epoch 49]: epoch loss = 1.044632,   acc = 0.952638
[epoch 49]: epoch loss = 1.044632,   acc = 0.952638
Epoch 50/160: 100%|██████████| 82/82 [00:08<00:00,  9.58it/s]
2025-07-02 22:39:49.375 | INFO     | model:train:251 - [epoch 50]: epoch loss = 1.010167,   acc = 0.954576
[epoch 50]: epoch loss = 1.010167,   acc = 0.954576
Epoch 51/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 22:39:57.859 | INFO     | model:train:251 - [epoch 51]: epoch loss = 1.007332,   acc = 0.954628
[epoch 51]: epoch loss = 1.007332,   acc = 0.954628
Epoch 52/160: 100%|██████████| 82/82 [00:08<00:00,  9.95it/s]
2025-07-02 22:40:06.172 | INFO     | model:train:251 - [epoch 52]: epoch loss = 1.009159,   acc = 0.953935
[epoch 52]: epoch loss = 1.009159,   acc = 0.953935
Epoch 53/160: 100%|██████████| 82/82 [00:08<00:00,  9.90it/s]
2025-07-02 22:40:14.577 | INFO     | model:train:251 - [epoch 53]: epoch loss = 0.972281,   acc = 0.956018
[epoch 53]: epoch loss = 0.972281,   acc = 0.956018
Epoch 54/160: 100%|██████████| 82/82 [00:08<00:00,  9.94it/s]
2025-07-02 22:40:22.904 | INFO     | model:train:251 - [epoch 54]: epoch loss = 0.987303,   acc = 0.954419
[epoch 54]: epoch loss = 0.987303,   acc = 0.954419
Epoch 55/160: 100%|██████████| 82/82 [00:08<00:00,  9.87it/s]
2025-07-02 22:40:31.271 | INFO     | model:train:251 - [epoch 55]: epoch loss = 1.072983,   acc = 0.947420
[epoch 55]: epoch loss = 1.072983,   acc = 0.947420
Epoch 56/160: 100%|██████████| 82/82 [00:08<00:00,  9.91it/s]
2025-07-02 22:40:39.652 | INFO     | model:train:251 - [epoch 56]: epoch loss = 1.022193,   acc = 0.950056
[epoch 56]: epoch loss = 1.022193,   acc = 0.950056
Epoch 57/160: 100%|██████████| 82/82 [00:08<00:00,  9.90it/s]
2025-07-02 22:40:48.027 | INFO     | model:train:251 - [epoch 57]: epoch loss = 0.931487,   acc = 0.957321
[epoch 57]: epoch loss = 0.931487,   acc = 0.957321
Epoch 58/160: 100%|██████████| 82/82 [00:08<00:00,  9.78it/s]
2025-07-02 22:40:56.490 | INFO     | model:train:251 - [epoch 58]: epoch loss = 0.897306,   acc = 0.960866
[epoch 58]: epoch loss = 0.897306,   acc = 0.960866
Epoch 59/160: 100%|██████████| 82/82 [00:08<00:00,  9.96it/s]
2025-07-02 22:41:04.804 | INFO     | model:train:251 - [epoch 59]: epoch loss = 0.871906,   acc = 0.962077
[epoch 59]: epoch loss = 0.871906,   acc = 0.962077
Epoch 60/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 22:41:13.287 | INFO     | model:train:251 - [epoch 60]: epoch loss = 0.859190,   acc = 0.962724
[epoch 60]: epoch loss = 0.859190,   acc = 0.962724
/home/djh/my_envs/dai_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2776: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 9502,  5528,  6746,  7169, 13411, 13504,  6490,  7295,    80,
         609,   983,  3816]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([ 8428,  6639,  5421,  5982, 14716, 15255,  6793,  6137,  1236,
        1311,  3215]))
2025-07-02 22:41:13.592 | INFO     | model:train:267 - [epoch 60]: val loss = 6.722522,   val acc = 0.606498,   val balanced acc = 0.603130
[epoch 60]: val loss = 6.722522,   val acc = 0.606498,   val balanced acc = 0.603130
2025-07-02 22:41:13.628 | INFO     | model:train:287 - EarlyStopping counter: 2 out of 3
EarlyStopping counter: 2 out of 3
Epoch 61/160:   0%|          | 0/82 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 60 that is less than the current step 61. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Epoch 61/160: 100%|██████████| 82/82 [00:08<00:00,  9.75it/s]
2025-07-02 22:41:22.140 | INFO     | model:train:251 - [epoch 61]: epoch loss = 0.864289,   acc = 0.961089
[epoch 61]: epoch loss = 0.864289,   acc = 0.961089
Epoch 62/160: 100%|██████████| 82/82 [00:08<00:00,  9.82it/s]
2025-07-02 22:41:30.591 | INFO     | model:train:251 - [epoch 62]: epoch loss = 0.898226,   acc = 0.958346
[epoch 62]: epoch loss = 0.898226,   acc = 0.958346
Epoch 63/160: 100%|██████████| 82/82 [00:08<00:00,  9.95it/s]
2025-07-02 22:41:38.915 | INFO     | model:train:251 - [epoch 63]: epoch loss = 0.922889,   acc = 0.956276
[epoch 63]: epoch loss = 0.922889,   acc = 0.956276
Epoch 64/160: 100%|██████████| 82/82 [00:08<00:00,  9.88it/s]
2025-07-02 22:41:47.289 | INFO     | model:train:251 - [epoch 64]: epoch loss = 0.849316,   acc = 0.961963
[epoch 64]: epoch loss = 0.849316,   acc = 0.961963
Epoch 65/160: 100%|██████████| 82/82 [00:08<00:00,  9.82it/s]
2025-07-02 22:41:55.712 | INFO     | model:train:251 - [epoch 65]: epoch loss = 0.797607,   acc = 0.966077
[epoch 65]: epoch loss = 0.797607,   acc = 0.966077
Epoch 66/160: 100%|██████████| 82/82 [00:08<00:00,  9.74it/s]
2025-07-02 22:42:04.314 | INFO     | model:train:251 - [epoch 66]: epoch loss = 0.775388,   acc = 0.967697
[epoch 66]: epoch loss = 0.775388,   acc = 0.967697
Epoch 67/160: 100%|██████████| 82/82 [00:08<00:00,  9.82it/s]
2025-07-02 22:42:12.861 | INFO     | model:train:251 - [epoch 67]: epoch loss = 0.775243,   acc = 0.966871
[epoch 67]: epoch loss = 0.775243,   acc = 0.966871
Epoch 68/160: 100%|██████████| 82/82 [00:08<00:00,  9.96it/s]
2025-07-02 22:42:21.170 | INFO     | model:train:251 - [epoch 68]: epoch loss = 0.840639,   acc = 0.962046
[epoch 68]: epoch loss = 0.840639,   acc = 0.962046
Epoch 69/160: 100%|██████████| 82/82 [00:08<00:00,  9.95it/s]
2025-07-02 22:42:29.484 | INFO     | model:train:251 - [epoch 69]: epoch loss = 1.545205,   acc = 0.903422
[epoch 69]: epoch loss = 1.545205,   acc = 0.903422
Epoch 70/160: 100%|██████████| 82/82 [00:08<00:00,  9.99it/s]
2025-07-02 22:42:37.770 | INFO     | model:train:251 - [epoch 70]: epoch loss = 1.413162,   acc = 0.914546
[epoch 70]: epoch loss = 1.413162,   acc = 0.914546
Epoch 71/160: 100%|██████████| 82/82 [00:08<00:00,  9.80it/s]
2025-07-02 22:42:46.217 | INFO     | model:train:251 - [epoch 71]: epoch loss = 0.887593,   acc = 0.959286
[epoch 71]: epoch loss = 0.887593,   acc = 0.959286
Epoch 72/160: 100%|██████████| 82/82 [00:08<00:00,  9.85it/s]
2025-07-02 22:42:54.604 | INFO     | model:train:251 - [epoch 72]: epoch loss = 0.797954,   acc = 0.965976
[epoch 72]: epoch loss = 0.797954,   acc = 0.965976
Epoch 73/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 22:43:03.699 | INFO     | model:train:251 - [epoch 73]: epoch loss = 0.762463,   acc = 0.968742
[epoch 73]: epoch loss = 0.762463,   acc = 0.968742
Epoch 74/160: 100%|██████████| 82/82 [00:08<00:00,  9.76it/s]
2025-07-02 22:43:12.346 | INFO     | model:train:251 - [epoch 74]: epoch loss = 0.738066,   acc = 0.970413
[epoch 74]: epoch loss = 0.738066,   acc = 0.970413
Epoch 75/160: 100%|██████████| 82/82 [00:08<00:00,  9.89it/s]
2025-07-02 22:43:20.720 | INFO     | model:train:251 - [epoch 75]: epoch loss = 0.723949,   acc = 0.971202
[epoch 75]: epoch loss = 0.723949,   acc = 0.971202
Epoch 76/160: 100%|██████████| 82/82 [00:08<00:00,  9.86it/s]
2025-07-02 22:43:29.114 | INFO     | model:train:251 - [epoch 76]: epoch loss = 0.714947,   acc = 0.971443
[epoch 76]: epoch loss = 0.714947,   acc = 0.971443
Epoch 77/160: 100%|██████████| 82/82 [00:08<00:00,  9.93it/s]
2025-07-02 22:43:37.517 | INFO     | model:train:251 - [epoch 77]: epoch loss = 0.710259,   acc = 0.971015
[epoch 77]: epoch loss = 0.710259,   acc = 0.971015
Epoch 78/160: 100%|██████████| 82/82 [00:08<00:00,  9.73it/s]
2025-07-02 22:43:46.087 | INFO     | model:train:251 - [epoch 78]: epoch loss = 0.695428,   acc = 0.972150
[epoch 78]: epoch loss = 0.695428,   acc = 0.972150
Epoch 79/160: 100%|██████████| 82/82 [00:08<00:00,  9.82it/s]
2025-07-02 22:43:54.506 | INFO     | model:train:251 - [epoch 79]: epoch loss = 0.679564,   acc = 0.973515
[epoch 79]: epoch loss = 0.679564,   acc = 0.973515
Epoch 80/160: 100%|██████████| 82/82 [00:08<00:00,  9.79it/s]
2025-07-02 22:44:02.970 | INFO     | model:train:251 - [epoch 80]: epoch loss = 0.675980,   acc = 0.973288
[epoch 80]: epoch loss = 0.675980,   acc = 0.973288
Predicted labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([12235,  6075,  5177,  6619, 13218, 15271,  5229,  6448,   555,
        1005,  3301]))
True labels distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11]), array([ 8428,  6639,  5421,  5982, 14716, 15255,  6793,  6137,  1236,
        1311,  3215]))
2025-07-02 22:44:03.293 | INFO     | model:train:267 - [epoch 80]: val loss = 7.390699,   val acc = 0.600136,   val balanced acc = 0.590579
[epoch 80]: val loss = 7.390699,   val acc = 0.600136,   val balanced acc = 0.590579
2025-07-02 22:44:03.330 | INFO     | model:train:287 - EarlyStopping counter: 3 out of 3
EarlyStopping counter: 3 out of 3
2025-07-02 22:44:03.330 | INFO     | model:train:289 - Early stopping triggered.
Early stopping triggered.
